---
title: HPC集群运维总结1-配置与职责
date: 2017-12-12 20:20:01
categories: 集群管理
tags: [集群管理,运维,Linux,HPC]
---

[TOC]


### 序言
2016年5月至2018年3月之间，我与另外一位同学担任了某集群的运行维护工作。期间主要负责了机房维护、空调维护、配电房维护、安全防护、分布式系统管理、高性能软件管理、用户管理、计算资源管理、服务器系统管理、服务器硬件维护等工作，内容繁多，工作复杂，但是总体来说，在过去一年的集群管理工作中还是有不少收获的。

下面将自己在管理过程中遇到的问题和解决方法分享给大家，尽管里面涉及到的技术和理论还不够深入，甚至不足以解决一些很棘手的问题，但是可以方便后来的管理人员快速入手，同时也是一个交流的机会，非常欢迎大家批评指正和指导。
<!--more-->

### 集群配置概况
#### 服务器配置信息
集群包含4种类型的服务器
* TC4600刀箱中的CB60-G15计算刀片
* I640-G10普通2U机架式服务器
* I640-G15存储服务器
* W580I-G10GPU节点

各机型配置如下：

* TC4600刀箱
```
标准19英寸5U机架式刀片机箱、可以支持14个计算刀片；
1*管理模块，集成远程KVM和远程虚拟媒体；
1*万兆上联模块，对外2个sfp+万兆，对内14个千兆端口；
4*冗余热插拔散热模块；
4*2000W电源（3+1冗余热拔插）；
```


* 刀片CB60-G15
```
2*Intel Xeon E5-2670 八核处器 (2.6GHz)；
8*8GB DDR3 1600MHz；
1*300G 2.5寸10000转SAS硬盘；
1*56Gb Infiniband 接口；
2*1000M以太网接口；        

```

* 管理I620-G10
```
2U机架式
2*Intel Xeon E5-2670 八核处器 (2.6GHz)；
8*16GB DDR3 1600MHz；
1*300G 2.5寸10000转SAS硬盘；
2*1000M以太网接口；     
1+1冗余电源；  
```


* 存储I640-G15
```
4U机架式
1*Intel Xeon E5-2603 CPU 四核处器 (1.8GHz)；
4*4GB DDR3 ECC；
1*300G 2.5寸10000转SAS硬盘；
36* 900G SAS硬盘；
1*56Gb Infiniband 接口；
2*1000M以太网接口；
冗余电源；  
```


* 存储I620-G10
```
2U机架式
2*Intel Xeon E5-2603 CPU 四核处器 (1.8GHz)；
8*4GB DDR3 ECC；
1*300G 2.5寸10000转SAS硬盘；
1*56Gb Infiniband 接口；
2*1000M以太网接口；     
1+1冗余电源；

```

其中node1-node306是CB60-G15计算刀片，每14个刀片位于同一个刀箱，最后一个刀箱只有10个刀片。
node307-node308是I640-G10普通2U机架服务器，主要用来登录集群和集群的管理工作，其中node307主要用于用户登录和集群管理工作，node308部署了zabbix和集群温度监控脚本，以及Crane主页等内容。

node309-node310是Lustre的MDS服务器，即分布式文件系统的索引服务器。
node311-node323是Lustre的对象存储服务器。
node324目前是一台NFS服务器，主要用于某些不能使用Lustre的节点进行文件传输。
node325-node326是两台空闲的存储服务器，可以用来做备用机，一般情况不使用，主要用于应急，当然后期也可以考虑将该节点加入到Lustre的对象存储服务器中。

node327-node336是10台GPU节点
node337-node348共12台刀片，在北面从西至东的第三个机柜的最下面一个刀箱，也是计算刀片。 

#### 网络配置信息
所有计算刀片、存储服务器、存储IO节点通过Mellaonx FDR324口（18个18+18页版）Infiniband交换机连接，16U，模块化交换机。

集群入口节点没有Infniband网络，只有以太网络，即千兆以太网。

集群的所有刀箱、机架服务器都通过一个H3C的48口千兆以太网交换机连接在一起了。

刀箱后面一个H3C的以太网交换模块，这个模块提供了14个内置的交换端口和4个对外的交换端口以及2个万兆光口。

所有刀箱的交换模块通过一个H3C 24口 SFP+万兆交换机连接，也就是说，跨刀箱的节点通信通过万兆网，刀箱内部通信是通过交换模块内置的网络进行的。

另外，集群的所有服务器都有一个管理口，即远程控制地址。该套网络也是在H3C 48口以太网交换机、万兆交换机、以及刀箱上面的交换模块结合来构建的，具体方案现在还没有特别确定。具体布局情况最好请教网络方面的同学或工程师。

##### 通信流程
###### 同刀箱刀片通信
同一个刀片机箱的所有刀片节点之间的通信是通过刀箱后面的以太网交换模块进行的，相当与一个交换机。

###### 跨刀箱刀片通信
刀片A将数据转发到刀箱的交换模块，交换模块将数据通过万兆光模块转发到万兆交换机，万兆交换机转发到刀片B所在刀箱的交换模块，交换模块将数据转发给刀片B.

###### 管理节点与刀片通信

###### 管理节点与存储通信

###### 刀片节点与存储通信

#### 文件系统配置信息
集群采用并行分布式文件系统Lustre来提供/public和/home目录，即所有节点都挂载/public/home作为节点的/home目录，提供统一的用户信息。
Lustre文件系统构成如下，node309和node310为MDS服务器，node311-node318为/home对应的OST，node319-node323为/public目录对应的OST，由于之前Lustre文件系统出现过故障，后来node311-node318就不再提供存储服务了，node319-node323提供/public目录，并且提供/public/home为/home目录。
其次，在不能使用Lustre的节点，可以挂载NFS文件系统进行文件的传递。

### 日常职责
#### 机房巡检
该工作原则上是每天至少1次但是考虑到时间有限，可以宽限为每周2-3次，周一、周五都必须检查一次，主要检查空调运行情况、机房温度、机房湿度、是否存在火宅隐患、检查火宅报警器是否正常开机运行等。
总结如下：
* 主机房空调和配电房空调检查。
* 市电和UPS配电柜状态检查。
* UPS系统状态检查。
* 火灾报警器工作状态检查，包括主机房和配电房。
* 硬盘和服务器指示灯检查。
* 未罗列事项。

#### 集群硬件维护
该项职责主要包括集群存储服务器硬盘状态查看、计算刀片故障处理、硬盘更换、内存条更换、网络故障处理等。
集群软件系统维护，主要包括集群分布式文件系统故障处理、集群计算刀片操作系统管理、集群并行计算软件环境维护等。
计算资源分配与管理，主要将空闲的计算资源分配给有需要的用户，为其添加集群账号，分配节点。用户计算完后，主动回收节点，并清理不用的用户资料。
总结而言主要包括如下内容：
* 计算节点主板更换。
* 计算节点内存更换。
* 刀片机箱电源模块更换。
* 存储服务器硬盘更换。
* Infiniband网络故障解决。
* 其他未罗列事项。

#### 服务器软件维护
该项职责主要包括如下内容：
* 计算节点的操作系统维护，升级内核、还原系统等。
* 计算节点内置软件环境的配置和管理。
* 计算节点的环境变量配置。
* 并行计算环境在各节点的Client端或者代理端配置。
* PBS队列管理系统和Zabbix监控系统的客户端daemon配置与维护。
* 其他未罗列的计算节点软件。

#### HPC软件环境维护
HPC软件环境主要包括如下内容：
* Intel编译器、gcc编译器、gcc-c++编译器、gfortran编译器等编译支持软件。
* 各类库MKL、acml、fftw2-float、fftw2-double、scalapack等。
* 各类MPI版本，openmpi-gpu、openmpi-intel、mvapich2-gnu、mvapich2-intel、mpich3-gnu等。
* 各类具体的HPC软件，Matlab、Gaussian、Octopus、Lammps、CellSys、Ansys fluent、BLAST、Ansys CFX、WRF、WAVEWATCH 3、OpenFOAM等具体领域的软件。
* 监控平台如Zabbix。
* 分布式文件系统Lustre。
* 其他未罗列的HPC软件环境。

#### 集群资源分配与管理
* PBS队列配置与管理
* PBS队列使用权限分配
* HPC集群分区和资源利用最大化
* GPGPU资源的分配和管理
* 资源使用的监控与统计
* 资源的回收再分配

#### 用户管理与技术支持
* 为用户添加账号和重置密码
* 添加IP地址许可和网络转发
* 安装公用开源软件
* 协助用户解决遇到的问题

#### 小结
概括主要有如下几件事要做：
* 主机房两个空调是否正常运行，主要看报警数据还有温度信息，主机房正常温度在22-24摄氏度，湿度40%-60%范围内，空调控制面板报警选项卡中没有报警信息，并且查看最近一周温度变化情况，是否存在温度异常的情况。
* 主机房火灾报警器是否正常运行，主要看是否开机，面板提示是否正常。
* 主机房所有机柜绕一圈，检查是否有刀片服务器电源模块亮黄灯，黄灯表示异常，绿灯表示正常运行。同时，检查存储服务器后置硬盘阵列是否存在亮红灯的情况，存在亮红灯的硬盘，请及时拔下硬盘，有手机或者相机拍下服务器上序列号，并进行报修。最后还要检查刀片机箱的交换模块是否正常运行，即网络接口是否有灯在闪。
* 配电房UPS电池组是否存在异常，主要看是否有异味，UPS主机柜和配电柜是否有异常显示信息，异常时会有红灯亮，注意不是面板显示的红色信息。
* 配电房空调温度是否正常，东北角的柜式空调，正常温度设定在18-20摄氏度，存在异常请及时报修。

### 机房巡检
#### 概览
机房环境的维护是一件非常重要的事情，良好的机房环境是集群正常运行的基础，机房环境出现问题，直接集群瘫痪，无法正常运行，更严重的，还有可能导致火灾，因此机房环境需要重点关注。

#### 温度
机房环境主要指房间的温度、湿度，这两点特别重要，如果温度过高、过低或者温度不稳定，集群很容易出现问题，集群本身产热非常大，如果温度过高，很容易出现服务器报警、主板老化的情况。本集群之前空调出现问题，导致主板节点温度过高，服务器过早坏掉的情况。温度过高最容易导致主板电容爆掉的情况，因此需要引起注意，另外用户在进行计算过程中，要特别注意提醒用户不要提交过量的计算任务，避免导致主板温度过高提前老化的情况。任何时候，如果出现机房空调温度不正常，需要及时给实验室相关负责老师打电话并联系空调维保公司技术人员尽快进行空调维护，并关闭一部分机器，避免温度过高。

#### 湿度
机房的湿度控制，正常情况下，机房湿度都在标准允许的范围内，即不超过50%。除非是武汉地区长期下雨，可能会存在湿度超标的情况，一般湿度都会在可控的范围内。另外，之前机房顶部靠近北边的地方出现过漏水的情况，具体原因没有查清楚，因此进行机房巡视时，一定要注意看天花板，检查是否存在房顶漏水的情况，如果存在这样的情况，首先及时汇报情况给机房负责老师，并及时关闭集群，并切断电源（具体方法见后面集群断电方式），避免进水导致机器烧毁。

#### 火灾
机房火灾报警器检查，机房火灾一般会由于温度过高，机器老化导致，即很可能是因为电路老化导致火灾。因此每次去机房都要检查火灾报警器是否正常运行了。之前机房出现过火灾报警器误报的情况，即实际上不存在火灾，但是报警器一直响，出现过多次，后来发现原因是夏季时机房湿度比较大，报警器上面有水凝结，导致误报。之后稍微修理了一下，暂时没有出现误报的情况。如果出现了报警器报警的情况，检查温度是否过高，是否存在烟雾，是不是误报，然后及时汇报情况给负责老师。如果存在温度过高或者存在烟雾或有异味，赶紧关闭集群，断电方法请参考集群关机或启动流程章节。

#### 供电
集群供电是从机械大楼迁过来的，E5楼停电对集群无任何影响。
UPS仅仅提供IT设备的供电，断电后，应该在10分钟内关闭整个集群，保证服务器设备的安全和机房的安全。
因此巡检需要到配电房检查配电柜、UPS信息等，保证集群电力供应正常。

