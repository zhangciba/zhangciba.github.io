---
title: HPC集群运维总结7-故障处理
date: 2017-12-12 20:20:27
categories: 集群管理
tags: [集群管理,运维,Linux,HPC]
---

[TOC]

### 常见故障
#### 用户无法登录到集群
有如下几种原因会导致用户无法登录到集群：
* 用户使用的是校园无线网络，校园无线网络不允许登录到集群。
* 用户在校园网外部登录集群，集群IP地址未暴露给外网，校园网外部无法直接访问。
* 用户使用校园内部有线网络，无法登录到集群，需要在集群管理服务器添加IP地址许可。
* 用户不存在，则无法登录集群。
* 用户长期不登录或者已归还集群，账户密码被锁定，则无法登录集群。
<!-- more -->
解决方法：
* 检查IP地址是否允许登录，若不允许登录，则添加IP地址到iptables。
* 检查是否存在该用户，若不存在，则使用clusconf命令添加用户。
* 若用户存在，则执行`passwd -u username`解锁用户密码。

#### 用户无法登录节点
这种情况可能的原因如下：
* 1.节点死机或者被关机。
* 2.节点负载过高。
* 3.该机器限制了用户从某些主机的登录行为。
* 4.SSH服务默认端口被恶意更改。

解决方法依次如下：
* 1.使用IPMI重启机器。
* 2.待负载降下来之后登录。
* 3.从其他机器跳转到节点后，编辑/etc/hosts.deny解除登录节点限制。
* 4.询问上一批次使用的用户具体端口，或者直接去机房本地登录后进行修改。

#### 用户登录节点时需要密码
出现需要输入密码，可能原因如下：
* Lustre文件系统没有正确挂载，导致用户主目录没有，需要输入密码。
* 该节点没有分配给该用户。
* 该用户主目录中的密钥文件不存在或者权限不正确。

解决方法依次如下：
* 1.登录到节点解决Lustre文件系统挂载的问题，参考[集群分布式文件系统](#Lustre)章节进行解决。
* 2.考虑分配节点给用户或禁止用户登录该节点。
* 3.提示用户生成密钥文件或修改密钥文件的权限。

#### Lustre文件系统无法挂载
导致无法挂载的原因较多，较常见的如下：
* Infiniband网路故障。
* Lustre相关模块未加载。
* 计算节点内核被升级或者更换。
* Lustre文件系统本身故障。

解决方法依次如下：
* 1.参考[Infiniband网络故障解决](#Infiniband)部分解决Infiniband网络故障。
* 2.加载Lustre相关模块，若是由于Infiniband网络故障导致的模块无法加载，请先解决该网络故障，或者屏蔽Infiniband网络模块。
* 3.将内核修改为编译过Lustre的内核。
* 4.寻到具体故障，并进行解决，如果无法解决，首先在所有节点将/public或/home目录卸载，然后重启Lustre文件系统。

#### 无法获取zabbix监控信息
可能原因如下：
* zabbix配置信息不正确。
* zabbix相关文件夹权限问题。
* zabbix配置文件与服务端不一致。
* 防火墙未开放相关端口。

解决方法依次如下：
* 1.重新配置zabbix配置信息，编辑/etc/zabbix/zabbix_agent.conf文件中的Hostname，Server，ServerActive信息。
* 2.将/var/log/zabbix和/var/run/zabbix文件夹的权限分配给zabbix用户，并检查用户是否存在，若不存在，则不添加。
* 3.查看服务器配置信息，并修改配置文件。这个一般是Server或ServerActive信息不一致导致。
* 4.关闭防火墙或者添加端口允许。

#### IPMI无法获取服务器信息
可能有如下原因导致：
* IPMI相关模块未加载。
* IPMI软件被恶意卸载，特别是有root权限的用户。
* 集群长时间运行导致IPMI模块无法正常工作，可能需要卸下并插上刀片。

解决方法依次如下：
* 1.执行如下命令加载IPMI模块，然后再次执行IPMI命令，查看是否正常。
```
modprobe ipmi_watchdog
modprobe ipmi_poweroff
modprobe ipmi_devintf
modprobe ipmi_si
modprobe ipmi_msghandler
```
* 2.重新安装freeipmi软件包，具体网上有，集群的公共软件目录中也有。
* 3.刀片关机，然后拔掉后面的Infiniband线缆，拔出刀片，等待2分钟，插入刀片，插上Infiniband线缆，启动刀片，一般而言会好。如果实在不行，可以考虑关闭刀箱，并切断电源，但是这种做法不太推荐，除非这个刀箱的大多数刀片都无法进行IPMI控制，这些节点又经常需要进行重启，且无法使用操作系统进行开关机命令重启的情况下，才能使用。


### 售后报修
集群建设日期是2012年12月，合同约定保修日期是5年，报修到期日期是2017年12月，在此期间，任何硬件故障、软件故障都可以打电话给厂家全国客服，报修之前一定要把服务器的序列号记下来，然后进行报修。报修之后，厂家湖北分公司会有技术支持联系，之后会进行故障处理。


### 故障历史
#### 挖矿病毒
在每个节点上运行有很多的[watchdog]进程，这些进程原本是操作系统上进行不同CPU之间进程迁移的进程，挖矿病毒伪装为[watchdog]进程，占用全部的CPU进行计算，每天晚上22:00开始自动执行/etc/systemc命令，然后产生多个[watchdog]进程，在次日的7:00自动执行killall -9 [watchdog]杀死该进程。这些自动操作是添加到crontab中自动执行的。

另外一个特点就是，ssh登录到感染该病毒的节点之后，[watchdog]进程就不见了，但是使用top命令查看系统过去1、5、15分钟的系统负载都是很高的，原因是在/etc/bashrc中包含如下命令killall -9 [watchdog]，每次用户登录时自动执行该脚本文件杀死进程。

病毒感染规模很大，大约有1/3的节点感染了该病毒。每天22:00至7:00运行，导致机房温度曲线在这个时段偏高，这个可以通过空调控制面板中的温度曲线看到。

根据文件的修改日期，大概是2014年12月发生的。如今严格控制了节点的网络访问，也限制了登录到集群的外网IP地址，现象比较少出现了。

#### RAID磁盘阵列崩溃
某一台Lustre文件系统的对象存储服务器后面的磁盘阵列上面的所有磁盘亮红灯，/home目录无法正常读取或写入文件，由于/home目录提供的是用户文件，还有用户的软件，用户科研数据，为了防止情况恶化，直接停止了集群的使用，集群准备维修。

售后过来之后，读取了该服务器的RAID配置信息，发现是在一块磁盘出现问题，RAID进行Rebuild的过程中，又坏掉了一块磁盘，导致RAID直接崩溃了。他们尝试修复阵列信息，但是修复后，阵列一直处于Rebuild的过程中，Rebuid完成之后，又开始Rebuild过程，里面的文件被一次又一次的被破坏了，最后Lustre文件系统的/home目录压根就无法挂载了。因此该阵列上面的所有用户信息都丢失了。

由于这个阵列出现故障，直接将该服务器从Lustre的对象管理服务器中踢出，Lustre文件系统还是无法工作。售后没有办法解决这个问题了，而直接采用外部公司来恢复数据，价格估计很贵，加上集群本身是免费提供给用户使用的，因此也没有义务修复数据，因此集群就开始暂停服务，等待解决方案了。

最后经过几个月的等待，解决方案是重新安装Lustre文件系统。

#### Infiniband网络在Initializing和Active间切换
在集群重启时，集群所有服务器、以太网交换机、Infiniband网络交换机、BMC模块下电后，重新上电，当所有机器启动后，计算节点的IB网络状态正常（通过ibstat中的State字段查看），但是Lustre文件系统所在服务器，node309、node310、node317-node323的IB网络状态一直在切换。导致Lustre文件系统不能正常服务。即，计算节点无法正常挂载Lustre文件系统，即使挂载成功，也会由于IB网络不断在Initializing和Active之间切换，导致非常卡顿，无法正常使用。

```
发现问题
紧急处理措施
恢复挂载
```
##### 终极解决方案
[opensmd服务导致IB网络状态不断切换](http://blog.zhangchi.xyz/opensmd服务导致IB网络状态不断切换.html)
或者参考上面的内容[IB网络故障](#ibpro)
**教训：一定要每周检查磁盘的状态，特别是Lustre文件系统的存储服务器的磁盘，及时找到有故障的磁盘信息，并将磁盘报修，如果出现亮红灯的情况，一定要及时更换，请参考[服务器磁盘故障发现与处理](#disk)部分进行处理**

#### 异常停电
说到停电，一般指的是市电。集群的配电房后面有一个箱变，这个变压器的交流电是从**其他大楼**连接过来，不是从集群所在楼栋连接的线路。因此需要密切关注**其他大楼**的停电信息。
如果有相关停电通知，最好提前一天禁止用户提交作业和使用集群，停电之前的3-5小时按照[集群关机流程](#poweroff)来进行集群关机。然后断开配电柜和UPS相关的输出开关，避免突然来电导致的其他故障。
如果没有任何通知就突然停电了，事情就非常麻烦了。市电断开以后，UPS会给服务器继续供电，服务器会继续工作，而主机房空调，照明，新风等所有非IT设备都会断电，也就是说，机房的温度会陡然升高，直至UPS电量耗尽。因此在夏天，温度可能会达到60-80摄氏度，冬天可能达到50-70度左右，很可能导致火灾。不敢想象。所以这种情况一旦出现，必须第一时间感到配电房，将UPS输出的总开关断掉，停止服务器，防止继续产热。

在本人任职期间，机房出现过一次异常停电的情况，当时未接到任何通知，当时机房的火灾报警器响起，刚开始以为是误报，因为夏天下大雨时出现过报警器湿度过大导致误报的情况（不止一次），而且出现报警时，也第一时间登录集群，发现没啥问题，所以更加坚信应该没问题。但是报警器不会无缘无故响起，于是还是第一时间赶到机房，来到门口的那一刻，还是被吓到了，配电房和主机房的门都因为门禁没电而自动打开了，而且配电柜和UPS柜的输出电压显示都是熄灭的，所有开关都没有亮，感觉是出问题了，机房都没电，除了UPS主机和控制一直在显示之外。然后迅速跑到主机房，一打开，一阵火气扑面而来，还夹杂着一股塑料的味道，被吓到了，不敢进去，害怕里面已经烧坏了。于是赶紧把门打开，散热，但是过了好久，门口还是能够感到一大股热量。后来借了一把手电筒还是进去看了下，所有机柜后面后非常热，机柜门也是非常热，很浓的塑料味，不是烧焦的味道，但是第一次看到这样的场景，还是吓到了。刚开始以为机房烧了，价值不菲的机器废掉了，心里还是很担心的。搞不懂到底是啥问题，刚开始以为空调设备老化，短路，导致机房配电柜或者是箱变烧毁或者跳闸了。但是后来又仔细想了下，如果空调出现故障，服务器一直运行，那么会导致产热一直不会停止，导致温度达到近100度，机房会直接起火烧毁，那么就不会是这样子了。机房目前的状况来看，感觉应该没有损坏太多东西，应该不会有啥大问题。计算维修，也不需要太多费用。由于是周五，而且很晚了，一时间难以解决问题，于是将市电配电柜和UPS输出的所有开关都断开了，等待第二天的处理。

第二天一大早上来到机房，发现配电柜的三相电指示灯都亮了，我瞬间明白了，昨天的问题可能是机房停电了。这是，主机房的空调维保公司已经到位，并且检查了精密空调的状况，发现没有故障。于是经过确认后，我将UPS电池开关、主输出、旁路输出、主输入开关都打开了，然后把照明、门禁的空气开关也打开了，照明和门禁都正常工作了。于是我把新风、ADU地板也打开了，目前打开的设备一切OK。空调维修师傅确认后，我把空调的空气开关也打开了，空调开始正常工作了。经过这个过程，我大致可以确定，服务器应该也没有故障，因此抱着尝试的态度打开了2个机柜的UPS输出开关，进行供电。到主机房后，发现，机器可以正常工作，于是将所有机柜的UPS输出打开，PDM和服务器风扇和电源模块基本上都正常上电了。OK，说明机房基本没啥问题。于是按照高[集群开机流程](#poweron)进行了集群开机操作。经过30分钟的耐心等待，集群基本上正常工作。

通过排查服务器系统日志、精密空调日志、火灾报警器日志等发现，均是在前一天的下午18:40左右出现的断电，服务器断开时间稍晚一些。
这次故障出现真的是听折腾人的，担心了一晚上，没休息好，居然是停电导致的。之后经过确认，确实是**其他大楼**停电能导致的。希望后来的管理员遇到这种情况也可以从容面对了。哈哈。

