---
title: PBS作业提交后，输出错误MPI_Aobrt
date: 2017-05-26 08:49:28
categories: 高性能计算
tags: [MPI,PBS,Torque]
---

先看具体内容的截图。
![朋友圈](http://cdn.zhangchi.xyz//pic/Memoents0525.PNG)

<!--more-->
在/home/sce目录下面，有一个PBS的脚本，脚本里面的内容如下：
```
#!/bin/bash
mpirun -np 16 /home/sce/software/dl_poly_4.05/execute/DLPOLY.Z
```
这个脚本文件非常简单，什么东西都没有指定。如果把脚本中的命令直接在集群的某一节点进行输出时，可以正常运行。

但是通过Torque提交任务
```
qsub job.sub
```

然后执行
```
qstat
```
查看作业执行状况时，发现作业很快由运行状态R变成状态完成C了。
查看作业的输出文件和错误文件。发现输出文件中没有任何信息，错误文件中报如下错误。
![MPI_Abort](http://cdn.zhangchi.xyz//pic/MPI_Abort_error.jpeg)
由于作业脚本中指定的CPU核数是16，所以有16个MPI_Abort问题，如果修改np参数的值，MPI_Abort的数量也跟着变为一致。

最开始一致以为是MPI运行环境或者是Torque运行环境没有配置好。一直将精力放在这些环境的配置上面，重新安装了多次环境。环境配置成功之后，我又运行了MPI Hello World程序和计算PI值的程序，发现这两个程序无论是通过MPI直接运行，还是通过Torque多机器运行，都可以正常运行，虽然有一些报错，是XRC，也就是Infiniband驱动版本过旧的问题外，结果是正确的。

由于手工执行job.sub中的mpirun命令，可以正常运行，所以认为作业本身没问题，认为是PBS调度系统的问题，或者MPI和PBS结合配置的地方有问题。

就这样一直折腾着，后来发现运行Hello World和CPI程序可以的时候，隐约感觉是jos.sub有些问题。之后多次通过qsub job.sub提交作业，偶然间发现了在/home/sce下面有一个OUTPUT文件，发现这个文件的修改时间和作业提交时间很接近，而且这个文件原来是没有的，然后在提交脚本运行后，就出现了。于是查看了一下里面的内容，内容如下：
![CONFIG_FILE](http://cdn.zhangchi.xyz//pic/CONFIG_FILE_ERROR.jpeg)
上面提示CONFIG文件不存在，于是我想会不会是工作路径的问题，本身job.sub脚本坐在目录/home/sce/
app/DL_POLY/1/文件夹下面都有，但是通过Torque提交后，默认的工作目录是用户的主目录，因此找不到CONFIG文件，提示报错，于是我将脚本所在路径下面的所有文件都拷贝到了用户主目录/home/sec下面来，发现通过作业管理系统可以正常运行了。而且多机器情况下也可以正常运行。

第二天。我发现脚本又出现问题了，而且是通过mpi直接运行或者是作业脚本直接运行，都提示MPI_Abort错误，弄了半天，发现OUTPUT文件一直有输出，而且提示是CONFIG文件不存在，然后拷贝一个CONFIG文件过来，就恢复正常了。

经过一周的折腾，发现，导致脚本不能运行的根本原因是路径问题，也就是说脚本所在的目录并不是PBS的工作目录，PBS工作目录是在用户主目录下面，所以程序的相关文件也需要拷贝到用户的主目录中。

反思自己解决问题的过程，一直在配置MPI环境和Torque环境，却没有认真查看程序的结构和输出文件，不过这个程序确实没有将结果输出到job.sub.o122文件中，而是输出到OUTPUT文件中，这也是导致一直没有查看该文件中的输出信息进行调试的原因。

从这次的调试过程中，总结出了一个非常重要的经验，解决此类问题，应该遵循自底向上的解决问题的方法，首先从小的问题开始排查，如果确实没有问题，再排查MPI环境，最后是主机问题或者调度系统的问题。否则会浪费时间，走冤枉路，我遵循的方式是自顶向下，所以花了很多功夫，最后问题出在最下面，程序的使用方式不正确或者说程序的路径问题。
