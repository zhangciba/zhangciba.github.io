---
title: MPI基本语法笔记
date: 2017-04-27 23:52:05
categories: 高性能计算
tags: MPI
---

`MPI_SEND(buf,count,datatype,dest,tag,comm)`

buf,count,datatype是消息数据
dest,tag,comm是消息信封

`MPI_RECV(buf,count,datatype,source,tag,comm,status)`
buf,count,datatype是数据消息
source,tag,comm是消息信息
<!--more-->

消息信封的组成<源地址/目的地址，标识，通信域>
消息数据的组成<数据缓冲区起始地址，数据个数，数据类型>

tag标识的用途，当发送者发送两个相同的数据给同一个接受者时，如果没有消息标识，接受者将无法区分这两个消息。

MPI_ANY_SOURCE 任意源
MPI_ANY_TAG 任意标识
接收操作可以接收任意发送者的消息，但是对于发送操作则必须致命单独的接收者。

MPI通信域包括进程组和通信上下文。
编号0至N-1
不同的消息在不同的上下文中传递

MPI_Initialized(int *flag)判断MPI_INIT是否执行了。
MPI_Abort(MPI_Comm comm,int errorcode)出现异常时，退出

MPI_ANY_SOURCE  任意源
MPI_ANY_TAG  任意标识

#### 编写安全的MPI程序
主要的想法是为了避免死锁。
例如说进程0有A和C两个操作，进程1有B和D两个操作。
A从进程1接收消息，C向进程1发送消息
B从进程0接收消息，D向进程0发送消息

因此A需要等待D，D需要等待B，B需要等待C，C需要等待A，导致了一个环形的等待，造成了死锁。

另外一种情形，也是进程0有A和C两个操作，进程1有B和D两个操作
A向进程1发送消息，C从进程1接收消息
B向进程0发送消息，D从进程0接收消息。

由于C需要等待A操作完成才能继续操作，因此C依赖于A，D需要接收从进程0发送过来的消息，因此D依赖于A操作完成，同时又必须等待B操作完成，因此D依赖于B，C需要接收从进程1发送过来的消息，因此必须等待B完成，C依赖于B，总结起来就是C依赖于A，D依赖于A，C依赖于B，D依赖于B，而A和B两个操作都要向对方发送消息，因此需要占用缓冲区，如果缓冲区的大小不够，则A操作和B操作都无法完成，导致后面的操作也就无法进行，导致进程僵死。

##一种消息安全传递的通信调用秩序
当两个进程需要相互发送消息进行数据交换时，需要将它们的发送和接收操作按照一定的次序进行匹配，即一个进程的发送操作在前，接收操作在后，那么另一个进程的接收操作在前，发送操作在后，前后两个发送和接收操作需要相互匹配。


