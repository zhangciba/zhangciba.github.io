<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DIYER糍粑的博客</title>
  
  <subtitle>快乐学习、快乐工作、快乐生活</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.zhangchi.xyz/"/>
  <updated>2018-12-20T02:52:20.121Z</updated>
  <id>http://blog.zhangchi.xyz/</id>
  
  <author>
    <name>ZHANGCHI</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>grep匹配TAB的写法</title>
    <link href="http://blog.zhangchi.xyz/grep%E5%8C%B9%E9%85%8DTAB%E7%9A%84%E5%86%99%E6%B3%95.html"/>
    <id>http://blog.zhangchi.xyz/grep匹配TAB的写法.html</id>
    <published>2018-12-20T02:51:53.000Z</published>
    <updated>2018-12-20T02:52:20.121Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1、grep -P ‘t’</span><br><span class="line"></span><br><span class="line">2、grep [[:space:]] // 所有空白字符</span><br><span class="line"></span><br><span class="line">3、直接grep tab字符 // 命令行下用”ESC TAB”输入</span><br><span class="line"></span><br><span class="line">4、grep $’t’</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>grub2修改启动顺序</title>
    <link href="http://blog.zhangchi.xyz/grub2%E4%BF%AE%E6%94%B9%E5%90%AF%E5%8A%A8%E9%A1%BA%E5%BA%8F.html"/>
    <id>http://blog.zhangchi.xyz/grub2修改启动顺序.html</id>
    <published>2018-08-28T07:56:47.000Z</published>
    <updated>2018-08-28T07:58:43.981Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>查看所有的entry</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@dpdk grub2]# awk -F \&apos; &apos;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&apos; /etc/grub2.cfg</span><br><span class="line">0 : CentOS Linux (3.10.0-693.11.1.el7.x86_64) 7 (Core)</span><br><span class="line">1 : CentOS Linux (3.10.0-693.5.2.el7.x86_64) 7 (Core)</span><br><span class="line">2 : CentOS Linux (3.10.0-693.2.2.el7.x86_64) 7 (Core)</span><br><span class="line">3 : CentOS Linux (0-rescue-37138ca794604b28bca5b6394f5cd3c2) 7 (Core)</span><br></pre></td></tr></table></figure></li><li><p>查看当前default的entry</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dpdk grub2]# grub2-editenv list</span><br><span class="line">saved_entry=CentOS Linux (3.10.0-693.11.1.el7.x86_64) 7 (Core)</span><br><span class="line">[root@dpdk grub2]#</span><br></pre></td></tr></table></figure></li><li><p>修改为指定的entry</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@dpdk grub2]# grub2-set-default 2</span><br><span class="line">[root@dpdk grub2]# grub2-editenv list</span><br><span class="line">saved_entry=2</span><br><span class="line">[root@dpdk grub2]#</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看所有的entry&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;li
      
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="grub2" scheme="http://blog.zhangchi.xyz/tags/grub2/"/>
    
  </entry>
  
  <entry>
    <title>CentOS设置root用户邮箱</title>
    <link href="http://blog.zhangchi.xyz/CentOS%E8%AE%BE%E7%BD%AEroot%E7%94%A8%E6%88%B7%E9%82%AE%E7%AE%B1.html"/>
    <id>http://blog.zhangchi.xyz/CentOS设置root用户邮箱.html</id>
    <published>2018-05-18T11:48:04.000Z</published>
    <updated>2018-05-18T11:51:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>设置root邮箱<br>   在系统出现错误或有重要通知发送邮件给root的时候，让系统自动转送到通常使用的邮箱中，这样方便查阅相关报告和日志。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node999 root]# vi /etc/aliases</span><br><span class="line">root: bestmylover@hotmail.com　← 加入自己的邮箱地址</span><br><span class="line">[root@node999 root]# newaliases　← 重建aliasesdb</span><br><span class="line">/etc/aliases: 79 aliases, longest 23 bytes, 829 bytes total</span><br><span class="line">[root@node999 root]# echo test | mail root　← 发送测试邮件给root</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      CentOS设置root用户邮箱
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="root邮箱" scheme="http://blog.zhangchi.xyz/tags/root%E9%82%AE%E7%AE%B1/"/>
    
  </entry>
  
  <entry>
    <title>红黑树源码</title>
    <link href="http://blog.zhangchi.xyz/%E7%BA%A2%E9%BB%91%E6%A0%91%E6%BA%90%E7%A0%81.html"/>
    <id>http://blog.zhangchi.xyz/红黑树源码.html</id>
    <published>2018-03-28T12:57:01.000Z</published>
    <updated>2018-05-17T09:13:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>红黑树源码<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">  Red Black Trees</span><br><span class="line">  (C) 1999  Andrea Arcangeli &lt;andrea@suse.de&gt;</span><br><span class="line">  (C) 2002  David Woodhouse &lt;dwmw2@infradead.org&gt;</span><br><span class="line">  (C) 2012  Michel Lespinasse &lt;walken@google.com&gt;</span><br><span class="line"></span><br><span class="line">  This program is free software; you can redistribute it and/or modify</span><br><span class="line">  it under the terms of the GNU General Public License as published by</span><br><span class="line">  the Free Software Foundation; either version 2 of the License, or</span><br><span class="line">  (at your option) any later version.</span><br><span class="line"></span><br><span class="line">  This program is distributed in the hope that it will be useful,</span><br><span class="line">  but WITHOUT ANY WARRANTY; without even the implied warranty of</span><br><span class="line">  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span><br><span class="line">  GNU General Public License for more details.</span><br><span class="line"></span><br><span class="line">  You should have received a copy of the GNU General Public License</span><br><span class="line">  along with this program; if not, write to the Free Software</span><br><span class="line">  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA</span><br><span class="line"></span><br><span class="line">  linux/lib/rbtree.c</span><br><span class="line">*/</span><br><span class="line">#include &lt;linux/rbtree_augmented.h&gt;</span><br><span class="line">#include &lt;linux/export.h&gt;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * red-black trees properties:  http://en.wikipedia.org/wiki/Rbtree</span><br><span class="line"> *</span><br><span class="line"> *  1) A node is either red or black</span><br><span class="line"> *  2) The root is black</span><br><span class="line"> *  3) All leaves (NULL) are black</span><br><span class="line"> *  4) Both children of every red node are black</span><br><span class="line"> *  5) Every simple path from root to leaves contains the same number</span><br><span class="line"> *     of black nodes.</span><br><span class="line"> *</span><br><span class="line"> *  4 and 5 give the O(log n) guarantee, since 4 implies you cannot have two</span><br><span class="line"> *  consecutive red nodes in a path and every red node is therefore followed by</span><br><span class="line"> *  a black. So if B is the number of black nodes on every simple path (as per</span><br><span class="line"> *  5), then the longest possible path due to 4 is 2B.</span><br><span class="line"> *</span><br><span class="line"> *  We shall indicate color with case, where black nodes are uppercase and red</span><br><span class="line"> *  nodes will be lowercase. Unknown color nodes shall be drawn as red within</span><br><span class="line"> *  parentheses and have some accompanying text comment.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">static inline void rb_set_black(struct rb_node *rb)</span><br><span class="line">&#123;</span><br><span class="line">rb-&gt;__rb_parent_color |= RB_BLACK;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline struct rb_node *rb_red_parent(struct rb_node *red)</span><br><span class="line">&#123;</span><br><span class="line">return (struct rb_node *)red-&gt;__rb_parent_color;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Helper function for rotations:</span><br><span class="line"> * - old&apos;s parent and color get assigned to new</span><br><span class="line"> * - old gets assigned new as a parent and &apos;color&apos; as a color.</span><br><span class="line"> */</span><br><span class="line">static inline void</span><br><span class="line">__rb_rotate_set_parents(struct rb_node *old, struct rb_node *new,</span><br><span class="line">struct rb_root *root, int color)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *parent = rb_parent(old);</span><br><span class="line">new-&gt;__rb_parent_color = old-&gt;__rb_parent_color;</span><br><span class="line">rb_set_parent_color(old, new, color);</span><br><span class="line">__rb_change_child(old, new, parent, root);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static __always_inline void</span><br><span class="line">__rb_insert(struct rb_node *node, struct rb_root *root,</span><br><span class="line">    void (*augment_rotate)(struct rb_node *old, struct rb_node *new))</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *parent = rb_red_parent(node), *gparent, *tmp;</span><br><span class="line"></span><br><span class="line">while (true) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Loop invariant: node is red</span><br><span class="line"> *</span><br><span class="line"> * If there is a black parent, we are done.</span><br><span class="line"> * Otherwise, take some corrective action as we don&apos;t</span><br><span class="line"> * want a red root or two consecutive red nodes.</span><br><span class="line"> */</span><br><span class="line">if (!parent) &#123;</span><br><span class="line">rb_set_parent_color(node, NULL, RB_BLACK);</span><br><span class="line">break;</span><br><span class="line">&#125; else if (rb_is_black(parent))</span><br><span class="line">break;</span><br><span class="line"></span><br><span class="line">gparent = rb_red_parent(parent);</span><br><span class="line"></span><br><span class="line">tmp = gparent-&gt;rb_right;</span><br><span class="line">if (parent != tmp) &#123;/* parent == gparent-&gt;rb_left */</span><br><span class="line">if (tmp &amp;&amp; rb_is_red(tmp)) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Case 1 - color flips</span><br><span class="line"> *</span><br><span class="line"> *       G            g</span><br><span class="line"> *      / \          / \</span><br><span class="line"> *     p   u  --&gt;   P   U</span><br><span class="line"> *    /            /</span><br><span class="line"> *   n            N</span><br><span class="line"> *</span><br><span class="line"> * However, since g&apos;s parent might be red, and</span><br><span class="line"> * 4) does not allow this, we need to recurse</span><br><span class="line"> * at g.</span><br><span class="line"> */</span><br><span class="line">rb_set_parent_color(tmp, gparent, RB_BLACK);</span><br><span class="line">rb_set_parent_color(parent, gparent, RB_BLACK);</span><br><span class="line">node = gparent;</span><br><span class="line">parent = rb_parent(node);</span><br><span class="line">rb_set_parent_color(node, parent, RB_RED);</span><br><span class="line">continue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tmp = parent-&gt;rb_right;</span><br><span class="line">if (node == tmp) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Case 2 - left rotate at parent</span><br><span class="line"> *</span><br><span class="line"> *      G             G</span><br><span class="line"> *     / \           / \</span><br><span class="line"> *    p   U  --&gt;    n   U</span><br><span class="line"> *     \           /</span><br><span class="line"> *      n         p</span><br><span class="line"> *</span><br><span class="line"> * This still leaves us in violation of 4), the</span><br><span class="line"> * continuation into Case 3 will fix that.</span><br><span class="line"> */</span><br><span class="line">parent-&gt;rb_right = tmp = node-&gt;rb_left;</span><br><span class="line">node-&gt;rb_left = parent;</span><br><span class="line">if (tmp)</span><br><span class="line">rb_set_parent_color(tmp, parent,</span><br><span class="line">    RB_BLACK);</span><br><span class="line">rb_set_parent_color(parent, node, RB_RED);</span><br><span class="line">augment_rotate(parent, node);</span><br><span class="line">parent = node;</span><br><span class="line">tmp = node-&gt;rb_right;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Case 3 - right rotate at gparent</span><br><span class="line"> *</span><br><span class="line"> *        G           P</span><br><span class="line"> *       / \         / \</span><br><span class="line"> *      p   U  --&gt;  n   g</span><br><span class="line"> *     /                 \</span><br><span class="line"> *    n                   U</span><br><span class="line"> */</span><br><span class="line">gparent-&gt;rb_left = tmp;  /* == parent-&gt;rb_right */</span><br><span class="line">parent-&gt;rb_right = gparent;</span><br><span class="line">if (tmp)</span><br><span class="line">rb_set_parent_color(tmp, gparent, RB_BLACK);</span><br><span class="line">__rb_rotate_set_parents(gparent, parent, root, RB_RED);</span><br><span class="line">augment_rotate(gparent, parent);</span><br><span class="line">break;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">tmp = gparent-&gt;rb_left;</span><br><span class="line">if (tmp &amp;&amp; rb_is_red(tmp)) &#123;</span><br><span class="line">/* Case 1 - color flips */</span><br><span class="line">rb_set_parent_color(tmp, gparent, RB_BLACK);</span><br><span class="line">rb_set_parent_color(parent, gparent, RB_BLACK);</span><br><span class="line">node = gparent;</span><br><span class="line">parent = rb_parent(node);</span><br><span class="line">rb_set_parent_color(node, parent, RB_RED);</span><br><span class="line">continue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tmp = parent-&gt;rb_left;</span><br><span class="line">if (node == tmp) &#123;</span><br><span class="line">/* Case 2 - right rotate at parent */</span><br><span class="line">parent-&gt;rb_left = tmp = node-&gt;rb_right;</span><br><span class="line">node-&gt;rb_right = parent;</span><br><span class="line">if (tmp)</span><br><span class="line">rb_set_parent_color(tmp, parent,</span><br><span class="line">    RB_BLACK);</span><br><span class="line">rb_set_parent_color(parent, node, RB_RED);</span><br><span class="line">augment_rotate(parent, node);</span><br><span class="line">parent = node;</span><br><span class="line">tmp = node-&gt;rb_left;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* Case 3 - left rotate at gparent */</span><br><span class="line">gparent-&gt;rb_right = tmp;  /* == parent-&gt;rb_left */</span><br><span class="line">parent-&gt;rb_left = gparent;</span><br><span class="line">if (tmp)</span><br><span class="line">rb_set_parent_color(tmp, gparent, RB_BLACK);</span><br><span class="line">__rb_rotate_set_parents(gparent, parent, root, RB_RED);</span><br><span class="line">augment_rotate(gparent, parent);</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Inline version for rb_erase() use - we want to be able to inline</span><br><span class="line"> * and eliminate the dummy_rotate callback there</span><br><span class="line"> */</span><br><span class="line">static __always_inline void</span><br><span class="line">____rb_erase_color(struct rb_node *parent, struct rb_root *root,</span><br><span class="line">void (*augment_rotate)(struct rb_node *old, struct rb_node *new))</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *node = NULL, *sibling, *tmp1, *tmp2;</span><br><span class="line"></span><br><span class="line">while (true) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Loop invariants:</span><br><span class="line"> * - node is black (or NULL on first iteration)</span><br><span class="line"> * - node is not the root (parent is not NULL)</span><br><span class="line"> * - All leaf paths going through parent and node have a</span><br><span class="line"> *   black node count that is 1 lower than other leaf paths.</span><br><span class="line"> */</span><br><span class="line">sibling = parent-&gt;rb_right;</span><br><span class="line">if (node != sibling) &#123;/* node == parent-&gt;rb_left */</span><br><span class="line">if (rb_is_red(sibling)) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Case 1 - left rotate at parent</span><br><span class="line"> *</span><br><span class="line"> *     P               S</span><br><span class="line"> *    / \             / \</span><br><span class="line"> *   N   s    --&gt;    p   Sr</span><br><span class="line"> *      / \         / \</span><br><span class="line"> *     Sl  Sr      N   Sl</span><br><span class="line"> */</span><br><span class="line">parent-&gt;rb_right = tmp1 = sibling-&gt;rb_left;</span><br><span class="line">sibling-&gt;rb_left = parent;</span><br><span class="line">rb_set_parent_color(tmp1, parent, RB_BLACK);</span><br><span class="line">__rb_rotate_set_parents(parent, sibling, root,</span><br><span class="line">RB_RED);</span><br><span class="line">augment_rotate(parent, sibling);</span><br><span class="line">sibling = tmp1;</span><br><span class="line">&#125;</span><br><span class="line">tmp1 = sibling-&gt;rb_right;</span><br><span class="line">if (!tmp1 || rb_is_black(tmp1)) &#123;</span><br><span class="line">tmp2 = sibling-&gt;rb_left;</span><br><span class="line">if (!tmp2 || rb_is_black(tmp2)) &#123;</span><br><span class="line">/*</span><br><span class="line"> * Case 2 - sibling color flip</span><br><span class="line"> * (p could be either color here)</span><br><span class="line"> *</span><br><span class="line"> *    (p)           (p)</span><br><span class="line"> *    / \           / \</span><br><span class="line"> *   N   S    --&gt;  N   s</span><br><span class="line"> *      / \           / \</span><br><span class="line"> *     Sl  Sr        Sl  Sr</span><br><span class="line"> *</span><br><span class="line"> * This leaves us violating 5) which</span><br><span class="line"> * can be fixed by flipping p to black</span><br><span class="line"> * if it was red, or by recursing at p.</span><br><span class="line"> * p is red when coming from Case 1.</span><br><span class="line"> */</span><br><span class="line">rb_set_parent_color(sibling, parent,</span><br><span class="line">    RB_RED);</span><br><span class="line">if (rb_is_red(parent))</span><br><span class="line">rb_set_black(parent);</span><br><span class="line">else &#123;</span><br><span class="line">node = parent;</span><br><span class="line">parent = rb_parent(node);</span><br><span class="line">if (parent)</span><br><span class="line">continue;</span><br><span class="line">&#125;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">/*</span><br><span class="line"> * Case 3 - right rotate at sibling</span><br><span class="line"> * (p could be either color here)</span><br><span class="line"> *</span><br><span class="line"> *   (p)           (p)</span><br><span class="line"> *   / \           / \</span><br><span class="line"> *  N   S    --&gt;  N   Sl</span><br><span class="line"> *     / \             \</span><br><span class="line"> *    sl  Sr            s</span><br><span class="line"> *                       \</span><br><span class="line"> *                        Sr</span><br><span class="line"> */</span><br><span class="line">sibling-&gt;rb_left = tmp1 = tmp2-&gt;rb_right;</span><br><span class="line">tmp2-&gt;rb_right = sibling;</span><br><span class="line">parent-&gt;rb_right = tmp2;</span><br><span class="line">if (tmp1)</span><br><span class="line">rb_set_parent_color(tmp1, sibling,</span><br><span class="line">    RB_BLACK);</span><br><span class="line">augment_rotate(sibling, tmp2);</span><br><span class="line">tmp1 = sibling;</span><br><span class="line">sibling = tmp2;</span><br><span class="line">&#125;</span><br><span class="line">/*</span><br><span class="line"> * Case 4 - left rotate at parent + color flips</span><br><span class="line"> * (p and sl could be either color here.</span><br><span class="line"> *  After rotation, p becomes black, s acquires</span><br><span class="line"> *  p&apos;s color, and sl keeps its color)</span><br><span class="line"> *</span><br><span class="line"> *      (p)             (s)</span><br><span class="line"> *      / \             / \</span><br><span class="line"> *     N   S     --&gt;   P   Sr</span><br><span class="line"> *        / \         / \</span><br><span class="line"> *      (sl) sr      N  (sl)</span><br><span class="line"> */</span><br><span class="line">parent-&gt;rb_right = tmp2 = sibling-&gt;rb_left;</span><br><span class="line">sibling-&gt;rb_left = parent;</span><br><span class="line">rb_set_parent_color(tmp1, sibling, RB_BLACK);</span><br><span class="line">if (tmp2)</span><br><span class="line">rb_set_parent(tmp2, parent);</span><br><span class="line">__rb_rotate_set_parents(parent, sibling, root,</span><br><span class="line">RB_BLACK);</span><br><span class="line">augment_rotate(parent, sibling);</span><br><span class="line">break;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">sibling = parent-&gt;rb_left;</span><br><span class="line">if (rb_is_red(sibling)) &#123;</span><br><span class="line">/* Case 1 - right rotate at parent */</span><br><span class="line">parent-&gt;rb_left = tmp1 = sibling-&gt;rb_right;</span><br><span class="line">sibling-&gt;rb_right = parent;</span><br><span class="line">rb_set_parent_color(tmp1, parent, RB_BLACK);</span><br><span class="line">__rb_rotate_set_parents(parent, sibling, root,</span><br><span class="line">RB_RED);</span><br><span class="line">augment_rotate(parent, sibling);</span><br><span class="line">sibling = tmp1;</span><br><span class="line">&#125;</span><br><span class="line">tmp1 = sibling-&gt;rb_left;</span><br><span class="line">if (!tmp1 || rb_is_black(tmp1)) &#123;</span><br><span class="line">tmp2 = sibling-&gt;rb_right;</span><br><span class="line">if (!tmp2 || rb_is_black(tmp2)) &#123;</span><br><span class="line">/* Case 2 - sibling color flip */</span><br><span class="line">rb_set_parent_color(sibling, parent,</span><br><span class="line">    RB_RED);</span><br><span class="line">if (rb_is_red(parent))</span><br><span class="line">rb_set_black(parent);</span><br><span class="line">else &#123;</span><br><span class="line">node = parent;</span><br><span class="line">parent = rb_parent(node);</span><br><span class="line">if (parent)</span><br><span class="line">continue;</span><br><span class="line">&#125;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">/* Case 3 - right rotate at sibling */</span><br><span class="line">sibling-&gt;rb_right = tmp1 = tmp2-&gt;rb_left;</span><br><span class="line">tmp2-&gt;rb_left = sibling;</span><br><span class="line">parent-&gt;rb_left = tmp2;</span><br><span class="line">if (tmp1)</span><br><span class="line">rb_set_parent_color(tmp1, sibling,</span><br><span class="line">    RB_BLACK);</span><br><span class="line">augment_rotate(sibling, tmp2);</span><br><span class="line">tmp1 = sibling;</span><br><span class="line">sibling = tmp2;</span><br><span class="line">&#125;</span><br><span class="line">/* Case 4 - left rotate at parent + color flips */</span><br><span class="line">parent-&gt;rb_left = tmp2 = sibling-&gt;rb_right;</span><br><span class="line">sibling-&gt;rb_right = parent;</span><br><span class="line">rb_set_parent_color(tmp1, sibling, RB_BLACK);</span><br><span class="line">if (tmp2)</span><br><span class="line">rb_set_parent(tmp2, parent);</span><br><span class="line">__rb_rotate_set_parents(parent, sibling, root,</span><br><span class="line">RB_BLACK);</span><br><span class="line">augment_rotate(parent, sibling);</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* Non-inline version for rb_erase_augmented() use */</span><br><span class="line">void __rb_erase_color(struct rb_node *parent, struct rb_root *root,</span><br><span class="line">void (*augment_rotate)(struct rb_node *old, struct rb_node *new))</span><br><span class="line">&#123;</span><br><span class="line">____rb_erase_color(parent, root, augment_rotate);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(__rb_erase_color);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Non-augmented rbtree manipulation functions.</span><br><span class="line"> *</span><br><span class="line"> * We use dummy augmented callbacks here, and have the compiler optimize them</span><br><span class="line"> * out of the rb_insert_color() and rb_erase() function definitions.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">static inline void dummy_propagate(struct rb_node *node, struct rb_node *stop) &#123;&#125;</span><br><span class="line">static inline void dummy_copy(struct rb_node *old, struct rb_node *new) &#123;&#125;</span><br><span class="line">static inline void dummy_rotate(struct rb_node *old, struct rb_node *new) &#123;&#125;</span><br><span class="line"></span><br><span class="line">static const struct rb_augment_callbacks dummy_callbacks = &#123;</span><br><span class="line">dummy_propagate, dummy_copy, dummy_rotate</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">void rb_insert_color(struct rb_node *node, struct rb_root *root)</span><br><span class="line">&#123;</span><br><span class="line">__rb_insert(node, root, dummy_rotate);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_insert_color);</span><br><span class="line"></span><br><span class="line">void rb_erase(struct rb_node *node, struct rb_root *root)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *rebalance;</span><br><span class="line">rebalance = __rb_erase_augmented(node, root, &amp;dummy_callbacks);</span><br><span class="line">if (rebalance)</span><br><span class="line">____rb_erase_color(rebalance, root, dummy_rotate);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_erase);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * Augmented rbtree manipulation functions.</span><br><span class="line"> *</span><br><span class="line"> * This instantiates the same __always_inline functions as in the non-augmented</span><br><span class="line"> * case, but this time with user-defined callbacks.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,</span><br><span class="line">void (*augment_rotate)(struct rb_node *old, struct rb_node *new))</span><br><span class="line">&#123;</span><br><span class="line">__rb_insert(node, root, augment_rotate);</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(__rb_insert_augmented);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * This function returns the first node (in sort order) of the tree.</span><br><span class="line"> */</span><br><span class="line">struct rb_node *rb_first(const struct rb_root *root)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node*n;</span><br><span class="line"></span><br><span class="line">n = root-&gt;rb_node;</span><br><span class="line">if (!n)</span><br><span class="line">return NULL;</span><br><span class="line">while (n-&gt;rb_left)</span><br><span class="line">n = n-&gt;rb_left;</span><br><span class="line">return n;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_first);</span><br><span class="line"></span><br><span class="line">struct rb_node *rb_last(const struct rb_root *root)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node*n;</span><br><span class="line"></span><br><span class="line">n = root-&gt;rb_node;</span><br><span class="line">if (!n)</span><br><span class="line">return NULL;</span><br><span class="line">while (n-&gt;rb_right)</span><br><span class="line">n = n-&gt;rb_right;</span><br><span class="line">return n;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_last);</span><br><span class="line"></span><br><span class="line">struct rb_node *rb_next(const struct rb_node *node)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *parent;</span><br><span class="line"></span><br><span class="line">if (RB_EMPTY_NODE(node))</span><br><span class="line">return NULL;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * If we have a right-hand child, go down and then left as far</span><br><span class="line"> * as we can.</span><br><span class="line"> */</span><br><span class="line">if (node-&gt;rb_right) &#123;</span><br><span class="line">node = node-&gt;rb_right; </span><br><span class="line">while (node-&gt;rb_left)</span><br><span class="line">node=node-&gt;rb_left;</span><br><span class="line">return (struct rb_node *)node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * No right-hand children. Everything down and left is smaller than us,</span><br><span class="line"> * so any &apos;next&apos; node must be in the general direction of our parent.</span><br><span class="line"> * Go up the tree; any time the ancestor is a right-hand child of its</span><br><span class="line"> * parent, keep going up. First time it&apos;s a left-hand child of its</span><br><span class="line"> * parent, said parent is our &apos;next&apos; node.</span><br><span class="line"> */</span><br><span class="line">while ((parent = rb_parent(node)) &amp;&amp; node == parent-&gt;rb_right)</span><br><span class="line">node = parent;</span><br><span class="line"></span><br><span class="line">return parent;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_next);</span><br><span class="line"></span><br><span class="line">struct rb_node *rb_prev(const struct rb_node *node)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *parent;</span><br><span class="line"></span><br><span class="line">if (RB_EMPTY_NODE(node))</span><br><span class="line">return NULL;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * If we have a left-hand child, go down and then right as far</span><br><span class="line"> * as we can.</span><br><span class="line"> */</span><br><span class="line">if (node-&gt;rb_left) &#123;</span><br><span class="line">node = node-&gt;rb_left; </span><br><span class="line">while (node-&gt;rb_right)</span><br><span class="line">node=node-&gt;rb_right;</span><br><span class="line">return (struct rb_node *)node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * No left-hand children. Go up till we find an ancestor which</span><br><span class="line"> * is a right-hand child of its parent.</span><br><span class="line"> */</span><br><span class="line">while ((parent = rb_parent(node)) &amp;&amp; node == parent-&gt;rb_left)</span><br><span class="line">node = parent;</span><br><span class="line"></span><br><span class="line">return parent;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_prev);</span><br><span class="line"></span><br><span class="line">void rb_replace_node(struct rb_node *victim, struct rb_node *new,</span><br><span class="line">     struct rb_root *root)</span><br><span class="line">&#123;</span><br><span class="line">struct rb_node *parent = rb_parent(victim);</span><br><span class="line"></span><br><span class="line">/* Set the surrounding nodes to point to the replacement */</span><br><span class="line">__rb_change_child(victim, new, parent, root);</span><br><span class="line">if (victim-&gt;rb_left)</span><br><span class="line">rb_set_parent(victim-&gt;rb_left, new);</span><br><span class="line">if (victim-&gt;rb_right)</span><br><span class="line">rb_set_parent(victim-&gt;rb_right, new);</span><br><span class="line"></span><br><span class="line">/* Copy the pointers/colour from the victim to the replacement */</span><br><span class="line">*new = *victim;</span><br><span class="line">&#125;</span><br><span class="line">EXPORT_SYMBOL(rb_replace_node);</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;红黑树源码&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="红黑树" scheme="http://blog.zhangchi.xyz/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/"/>
    
      <category term="RBTree" scheme="http://blog.zhangchi.xyz/tags/RBTree/"/>
    
  </entry>
  
  <entry>
    <title>Are cache-line-ping-pong and false sharing the same?</title>
    <link href="http://blog.zhangchi.xyz/Are-cache-line-ping-pong-and-false-sharing-the-same.html"/>
    <id>http://blog.zhangchi.xyz/Are-cache-line-ping-pong-and-false-sharing-the-same.html</id>
    <published>2018-02-25T01:14:46.000Z</published>
    <updated>2018-02-25T01:16:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>Summary:</p><p>False sharing and cache-line ping-ponging are related but not the same thing. False sharing can cause cache-line ping-ponging, but it is not the only possible cause since cache-line ping-ponging can also be caused by true sharing.</p><p>Details:</p><p>False sharing</p><p>False sharing occurs when different threads have data that is not shared in the program, but this data gets mapped to a cache line that is shared. For example imagine a program that had an array of integers where one thread performed reads and writes to all of the array entries with an even index, and the other thread performed reads and writes to entries with an odd index. In this case the threads would not actually be sharing data, but they would share cache lines since each cache line would contain both odd and even indexed values (assuming the cache line was bigger than an integer, which is typically true).</p><p>Cache-line ping-ponging</p><p>Cache line ping-ponging is the effect where a cache line is transferred between multiple CPUs (or cores) in rapid succession. This can be cause by either false or true sharing. Essentially if multiple CPUs are trying to read and write data in the same cache line then that cache line might have to be transferred between the two threads in rapid succession, and this can cause a significant performance degradation (possibly even worse performance than if a single thread were executing). False sharing can make this problem particularly difficult to detect, because a programmer might have tried to write an application so that the threads weren’t sharing data, without realizing that the data was mapped to the same cache line. But false sharing is not the only possible cause of cache-line ping-ponging. This could also be caused by true sharing where multiple threads are trying to read and write the same data.</p><p>From <a href="https://stackoverflow.com/questions/30684974/are-cache-line-ping-pong-and-false-sharing-the-same" target="_blank" rel="noopener">StackOverFlow</a></p>]]></content>
    
    <summary type="html">
    
      Are cache-line-ping-pong and false sharing the same
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="cache ping-pong" scheme="http://blog.zhangchi.xyz/tags/cache-ping-pong/"/>
    
      <category term="false sharing" scheme="http://blog.zhangchi.xyz/tags/false-sharing/"/>
    
  </entry>
  
  <entry>
    <title>mpich3.2.1-release</title>
    <link href="http://blog.zhangchi.xyz/mpich3-2-1-release.html"/>
    <id>http://blog.zhangchi.xyz/mpich3-2-1-release.html</id>
    <published>2018-02-24T03:12:59.000Z</published>
    <updated>2018-02-24T03:19:25.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br></pre></td><td class="code"><pre><span class="line">MPICH Release 3.2.1</span><br><span class="line"></span><br><span class="line">MPICH is a high-performance and widely portable implementation of the</span><br><span class="line">MPI-3.1 standard from the Argonne National Laboratory.  This release</span><br><span class="line">has all MPI 3.1 functions and features required by the standard with</span><br><span class="line">the exception of support for the &quot;external32&quot; portable I/O format and</span><br><span class="line">user-defined data representations for I/O.</span><br><span class="line"></span><br><span class="line">This README file should contain enough information to get you started</span><br><span class="line">with MPICH. More extensive installation and user guides can be found</span><br><span class="line">in the doc/installguide/install.pdf and doc/userguide/user.pdf files</span><br><span class="line">respectively. Additional information regarding the contents of the</span><br><span class="line">release can be found in the CHANGES file in the top-level directory,</span><br><span class="line">and in the RELEASE_NOTES file, where certain restrictions are</span><br><span class="line">detailed. Finally, the MPICH web site, http://www.mpich.org, contains</span><br><span class="line">information on bug fixes and new releases.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1.  Getting Started</span><br><span class="line">2.  Reporting Installation or Usage Problems</span><br><span class="line">3.  Compiler Flags</span><br><span class="line">4.  Alternate Channels and Devices</span><br><span class="line">5.  Alternate Process Managers</span><br><span class="line">6.  Alternate Configure Options</span><br><span class="line">7.  Testing the MPICH installation</span><br><span class="line">8.  Fault Tolerance</span><br><span class="line">9.  Developer Builds</span><br><span class="line">10. Multiple Fortran compiler support</span><br><span class="line">11. ABI Compatibility</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">1. Getting Started</span><br><span class="line">==================</span><br><span class="line"></span><br><span class="line">The following instructions take you through a sequence of steps to get</span><br><span class="line">the default configuration (ch3 device, nemesis channel (with TCP and</span><br><span class="line">shared memory), Hydra process management) of MPICH up and running.</span><br><span class="line"></span><br><span class="line">(a) You will need the following prerequisites.</span><br><span class="line"></span><br><span class="line">    - REQUIRED: This tar file mpich-3.2.1.tar.gz</span><br><span class="line"></span><br><span class="line">    - REQUIRED: A C compiler (gcc is sufficient)</span><br><span class="line"></span><br><span class="line">    - OPTIONAL: A C++ compiler, if C++ applications are to be used</span><br><span class="line">      (g++, etc.). If you do not require support for C++ applications,</span><br><span class="line">      you can disable this support using the configure option</span><br><span class="line">      --disable-cxx (configuring MPICH is described in step 1(d)</span><br><span class="line">      below).</span><br><span class="line"></span><br><span class="line">    - OPTIONAL: A Fortran compiler, if Fortran applications are to be</span><br><span class="line">      used (gfortran, ifort, etc.). If you do not require support for</span><br><span class="line">      Fortran applications, you can disable this support using</span><br><span class="line">      --disable-fortran (configuring MPICH is described in step 1(d)</span><br><span class="line">      below).</span><br><span class="line"></span><br><span class="line">    Also, you need to know what shell you are using since different shell</span><br><span class="line">    has different command syntax. Command &quot;echo $SHELL&quot; prints out the</span><br><span class="line">    current shell used by your terminal program.</span><br><span class="line"></span><br><span class="line">(b) Unpack the tar file and go to the top level directory:</span><br><span class="line"></span><br><span class="line">      tar xzf mpich-3.2.1.tar.gz</span><br><span class="line">      cd mpich-3.2.1</span><br><span class="line"></span><br><span class="line">    If your tar doesn&apos;t accept the z option, use</span><br><span class="line"></span><br><span class="line">      gunzip mpich-3.2.1.tar.gz</span><br><span class="line">      tar xf mpich-3.2.1.tar</span><br><span class="line">      cd mpich-3.2.1</span><br><span class="line"></span><br><span class="line">(c) Choose an installation directory, say</span><br><span class="line">    /home/&lt;USERNAME&gt;/mpich-install, which is assumed to non-existent</span><br><span class="line">    or empty. It will be most convenient if this directory is shared</span><br><span class="line">    by all of the machines where you intend to run processes. If not,</span><br><span class="line">    you will have to duplicate it on the other machines after</span><br><span class="line">    installation.</span><br><span class="line"></span><br><span class="line">(d) Configure MPICH specifying the installation directory:</span><br><span class="line"></span><br><span class="line">    for csh and tcsh:</span><br><span class="line"></span><br><span class="line">      ./configure --prefix=/home/&lt;USERNAME&gt;/mpich-install |&amp; tee c.txt</span><br><span class="line"></span><br><span class="line">    for bash and sh:</span><br><span class="line"></span><br><span class="line">      ./configure --prefix=/home/&lt;USERNAME&gt;/mpich-install 2&gt;&amp;1 | tee c.txt</span><br><span class="line"></span><br><span class="line">    Bourne-like shells, sh and bash, accept &quot;2&gt;&amp;1 |&quot;.  Csh-like shell,</span><br><span class="line">    csh and tcsh, accept &quot;|&amp;&quot;. If a failure occurs, the configure</span><br><span class="line">    command will display the error. Most errors are straight-forward</span><br><span class="line">    to follow. For example, if the configure command fails with:</span><br><span class="line"></span><br><span class="line">       &quot;No Fortran compiler found. If you don&apos;t need to build any</span><br><span class="line">        Fortran programs, you can disable Fortran support using</span><br><span class="line">        --disable-fortran. If you do want to build Fortran programs,</span><br><span class="line">        you need to install a Fortran compiler such as gfortran or</span><br><span class="line">        ifort before you can proceed.&quot;</span><br><span class="line"></span><br><span class="line">    ... it means that you don&apos;t have a Fortran compiler :-). You will</span><br><span class="line">    need to either install one, or disable Fortran support in MPICH.</span><br><span class="line"></span><br><span class="line">    If you are unable to understand what went wrong, please go to step</span><br><span class="line">    (2) below, for reporting the issue to the MPICH developers and</span><br><span class="line">    other users.</span><br><span class="line"></span><br><span class="line">(e) Build MPICH:</span><br><span class="line"></span><br><span class="line">    for csh and tcsh:</span><br><span class="line"></span><br><span class="line">      make |&amp; tee m.txt</span><br><span class="line"></span><br><span class="line">    for bash and sh:</span><br><span class="line"></span><br><span class="line">      make 2&gt;&amp;1 | tee m.txt</span><br><span class="line"></span><br><span class="line">    This step should succeed if there were no problems with the</span><br><span class="line">    preceding step. Check file m.txt. If there were problems, do a</span><br><span class="line">    &quot;make clean&quot; and then run make again with V=1.</span><br><span class="line"></span><br><span class="line">      make V=1 |&amp; tee m.txt       (for csh and tcsh)</span><br><span class="line"></span><br><span class="line">      OR</span><br><span class="line"></span><br><span class="line">      make V=1 2&gt;&amp;1 | tee m.txt   (for bash and sh)</span><br><span class="line"></span><br><span class="line">    Then go to step (2) below, for reporting the issue to the MPICH</span><br><span class="line">    developers and other users.</span><br><span class="line"></span><br><span class="line">(f) Install the MPICH commands:</span><br><span class="line"></span><br><span class="line">    for csh and tcsh:</span><br><span class="line"></span><br><span class="line">      make install |&amp; tee mi.txt</span><br><span class="line"></span><br><span class="line">    for bash and sh:</span><br><span class="line"></span><br><span class="line">      make install 2&gt;&amp;1 | tee mi.txt</span><br><span class="line"></span><br><span class="line">    This step collects all required executables and scripts in the bin</span><br><span class="line">    subdirectory of the directory specified by the prefix argument to</span><br><span class="line">    configure.</span><br><span class="line"></span><br><span class="line">(g) Add the bin subdirectory of the installation directory to your</span><br><span class="line">    path in your startup script (.bashrc for bash, .cshrc for csh,</span><br><span class="line">    etc.):</span><br><span class="line"></span><br><span class="line">    for csh and tcsh:</span><br><span class="line"></span><br><span class="line">      setenv PATH /home/&lt;USERNAME&gt;/mpich-install/bin:$PATH</span><br><span class="line"></span><br><span class="line">    for bash and sh:</span><br><span class="line"></span><br><span class="line">      PATH=/home/&lt;USERNAME&gt;/mpich-install/bin:$PATH ; export PATH</span><br><span class="line"></span><br><span class="line">    Check that everything is in order at this point by doing:</span><br><span class="line"></span><br><span class="line">      which mpicc</span><br><span class="line">      which mpiexec</span><br><span class="line"></span><br><span class="line">    These commands should display the path to your bin subdirectory of</span><br><span class="line">    your install directory.</span><br><span class="line"></span><br><span class="line">    IMPORTANT NOTE: The install directory has to be visible at exactly</span><br><span class="line">    the same path on all machines you want to run your applications</span><br><span class="line">    on. This is typically achieved by installing MPICH on a shared</span><br><span class="line">    NFS file-system. If you do not have a shared NFS directory, you</span><br><span class="line">    will need to manually copy the install directory to all machines</span><br><span class="line">    at exactly the same location.</span><br><span class="line"></span><br><span class="line">(h) MPICH uses a process manager for starting MPI applications. The</span><br><span class="line">    process manager provides the &quot;mpiexec&quot; executable, together with</span><br><span class="line">    other utility executables. MPICH comes packaged with multiple</span><br><span class="line">    process managers; the default is called Hydra.</span><br><span class="line"></span><br><span class="line">    Now we will run an MPI job, using the mpiexec command as specified</span><br><span class="line">    in the MPI standard. There are some examples in the install</span><br><span class="line">    directory, which you have already put in your path, as well as in</span><br><span class="line">    the directory mpich-3.2.1/examples. One of them is the classic</span><br><span class="line">    CPI example, which computes the value of pi by numerical</span><br><span class="line">    integration in parallel.</span><br><span class="line"></span><br><span class="line">    To run the CPI example with &apos;n&apos; processes on your local machine,</span><br><span class="line">    you can use:</span><br><span class="line"></span><br><span class="line">      mpiexec -n &lt;number&gt; ./examples/cpi</span><br><span class="line"></span><br><span class="line">    Test that you can run an &apos;n&apos; process CPI job on multiple nodes:</span><br><span class="line"></span><br><span class="line">      mpiexec -f machinefile -n &lt;number&gt; ./examples/cpi</span><br><span class="line"></span><br><span class="line">    The &apos;machinefile&apos; is of the form:</span><br><span class="line"></span><br><span class="line">      host1</span><br><span class="line">      host2:2</span><br><span class="line">      host3:4   # Random comments</span><br><span class="line">      host4:1</span><br><span class="line"></span><br><span class="line">    &apos;host1&apos;, &apos;host2&apos;, &apos;host3&apos; and &apos;host4&apos; are the hostnames of the</span><br><span class="line">    machines you want to run the job on. The &apos;:2&apos;, &apos;:4&apos;, &apos;:1&apos; segments</span><br><span class="line">    depict the number of processes you want to run on each node. If</span><br><span class="line">    nothing is specified, &apos;:1&apos; is assumed.</span><br><span class="line"></span><br><span class="line">    More details on interacting with Hydra can be found at</span><br><span class="line">    http://wiki.mpich.org/mpich/index.php/Using_the_Hydra_Process_Manager</span><br><span class="line"></span><br><span class="line">If you have completed all of the above steps, you have successfully</span><br><span class="line">installed MPICH and run an MPI example.</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">2. Reporting Installation or Usage Problems</span><br><span class="line">===========================================</span><br><span class="line"></span><br><span class="line">[VERY IMPORTANT: PLEASE COMPRESS ALL FILES BEFORE SENDING THEM TO</span><br><span class="line">US. DO NOT SPAM THE MAILING LIST WITH LARGE ATTACHMENTS.]</span><br><span class="line"></span><br><span class="line">The distribution has been tested by us on a variety of machines in our</span><br><span class="line">environments as well as our partner institutes. If you have problems</span><br><span class="line">with the installation or usage of MPICH, please follow these steps:</span><br><span class="line"></span><br><span class="line">1. First see the Frequently Asked Questions (FAQ) page at</span><br><span class="line">http://wiki.mpich.org/mpich/index.php/Frequently_Asked_Questions to</span><br><span class="line">see if the problem you are facing has a simple solution. Many common</span><br><span class="line">problems and their solutions are listed here.</span><br><span class="line"></span><br><span class="line">2. If you cannot find an answer on the FAQ page, look through previous</span><br><span class="line">email threads on the discuss@mpich.org mailing list archive</span><br><span class="line">(https://lists.mpich.org/mailman/listinfo/discuss). It is likely</span><br><span class="line">someone else had a similar problem, which has already been resolved</span><br><span class="line">before.</span><br><span class="line"></span><br><span class="line">3. If neither of the above steps work, please send an email to</span><br><span class="line">discuss@mpich.org. You need to subscribe to this list</span><br><span class="line">(https://lists.mpich.org/mailman/listinfo/discuss) before sending an</span><br><span class="line">email.</span><br><span class="line"></span><br><span class="line">Your email should contain the following files.  ONCE AGAIN, PLEASE</span><br><span class="line">COMPRESS BEFORE SENDING, AS THE FILES CAN BE LARGE.  Note that,</span><br><span class="line">depending on which step the build failed, some of the files might not</span><br><span class="line">exist.</span><br><span class="line"></span><br><span class="line">    mpich-3.2.1/c.txt (generated in step 1(d) above)</span><br><span class="line">    mpich-3.2.1/m.txt (generated in step 1(e) above)</span><br><span class="line">    mpich-3.2.1/mi.txt (generated in step 1(f) above)</span><br><span class="line">    mpich-3.2.1/config.log (generated in step 1(d) above)</span><br><span class="line">    mpich-3.2.1/src/openpa/config.log (generated in step 1(d) above)</span><br><span class="line">    mpich-3.2.1/src/mpl/config.log (generated in step 1(d) above)</span><br><span class="line">    mpich-3.2.1/src/pm/hydra/config.log (generated in step 1(d) above)</span><br><span class="line">    mpich-3.2.1/src/pm/hydra/tools/topo/hwloc/hwloc/config.log (generated in step 1(d) above)</span><br><span class="line"></span><br><span class="line">    DID WE MENTION? DO NOT FORGET TO COMPRESS THESE FILES!</span><br><span class="line"></span><br><span class="line">If you have compiled MPICH and are having trouble running an</span><br><span class="line">application, please provide the output of the following command in</span><br><span class="line">your email.</span><br><span class="line"></span><br><span class="line">    mpiexec -info</span><br><span class="line"></span><br><span class="line">Finally, please include the actual error you are seeing when running</span><br><span class="line">the application, including the mpiexec command used, and the host</span><br><span class="line">file. If possible, please try to reproduce the error with a smaller</span><br><span class="line">application or benchmark and send that along in your bug report.</span><br><span class="line"></span><br><span class="line">4. If you have found a bug in MPICH, we request that you report it at</span><br><span class="line">our bug tracking system:</span><br><span class="line">(https://trac.mpich.org/projects/mpich/newticket). Even if you believe</span><br><span class="line">you have found a bug, we recommend you sending an email to</span><br><span class="line">discuss@mpich.org first.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">3. Compiler Flags</span><br><span class="line">=================</span><br><span class="line"></span><br><span class="line">MPICH allows several sets of compiler flags to be used. The first</span><br><span class="line">three sets are configure-time options for MPICH, while the fourth is</span><br><span class="line">only relevant when compiling applications with mpicc and friends.</span><br><span class="line"></span><br><span class="line">(a) CFLAGS, CPPFLAGS, CXXFLAGS, FFLAGS, FCFLAGS, LDFLAGS and LIBS</span><br><span class="line">(abbreviated as xFLAGS): Setting these flags would result in the</span><br><span class="line">MPICH library being compiled/linked with these flags and the flags</span><br><span class="line">internally being used in mpicc and friends.</span><br><span class="line"></span><br><span class="line">(b) MPICHLIB_CFLAGS, MPICHLIB_CPPFLAGS, MPICHLIB_CXXFLAGS,</span><br><span class="line">MPICHLIB_FFLAGS, and MPICHLIB_FCFLAGS (abbreviated as</span><br><span class="line">MPICHLIB_xFLAGS): Setting these flags would result in the MPICH</span><br><span class="line">library being compiled with these flags.  However, these flags will</span><br><span class="line">*not* be used by mpicc and friends.</span><br><span class="line"></span><br><span class="line">(c) MPICH_MAKE_CFLAGS: Setting these flags would result in MPICH&apos;s</span><br><span class="line">configure tests to not use these flags, but the makefile&apos;s to use</span><br><span class="line">them. This is a temporary hack for certain cases that advanced</span><br><span class="line">developers might be interested in, but which break existing configure</span><br><span class="line">tests (e.g., -Werror). These are NOT recommended for regular users.</span><br><span class="line"></span><br><span class="line">(d) MPICH_MPICC_CFLAGS, MPICH_MPICC_CPPFLAGS, MPICH_MPICC_LDFLAGS,</span><br><span class="line">MPICH_MPICC_LIBS, and so on for MPICXX, MPIF77 and MPIFORT</span><br><span class="line">(abbreviated as MPICH_MPIX_FLAGS): These flags do *not* affect the</span><br><span class="line">compilation of the MPICH library itself, but will be internally used</span><br><span class="line">by mpicc and friends.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  +--------------------------------------------------------------------+</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  |                    |    MPICH library     |    mpicc and friends   |</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  +--------------------+----------------------+------------------------+</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  |     xFLAGS         |         Yes          |           Yes          |</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  +--------------------+----------------------+------------------------+</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  |  MPICHLIB_xFLAGS   |         Yes          |           No           |</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  +--------------------+----------------------+------------------------+</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  | MPICH_MAKE_xFLAGS  |         Yes          |           No           |</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  +--------------------+----------------------+------------------------+</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  | MPICH_MPIX_FLAGS   |         No           |           Yes          |</span><br><span class="line">  |                    |                      |                        |</span><br><span class="line">  +--------------------+----------------------+------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All these flags can be set as part of configure command or through</span><br><span class="line">environment variables.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Default flags</span><br><span class="line">--------------</span><br><span class="line">By default, MPICH automatically adds certain compiler optimizations</span><br><span class="line">to MPICHLIB_CFLAGS. The currently used optimization level is -O2.</span><br><span class="line"></span><br><span class="line">** IMPORTANT NOTE: Remember that this only affects the compilation of</span><br><span class="line">the MPICH library and is not used in the wrappers (mpicc and friends)</span><br><span class="line">that are used to compile your applications or other libraries.</span><br><span class="line"></span><br><span class="line">This optimization level can be changed with the --enable-fast option</span><br><span class="line">passed to configure. For example, to build an MPICH environment with</span><br><span class="line">-O3 for all language bindings, one can simply do:</span><br><span class="line"></span><br><span class="line">  ./configure --enable-fast=O3</span><br><span class="line"></span><br><span class="line">Or to disable all compiler optimizations, one can do:</span><br><span class="line"></span><br><span class="line">  ./configure --disable-fast</span><br><span class="line"></span><br><span class="line">For more details of --enable-fast, see the output of &quot;configure</span><br><span class="line">--help&quot;.</span><br><span class="line"></span><br><span class="line">For performance testing, we recommend the following flags:</span><br><span class="line"></span><br><span class="line">  ./configure --enable-fast=O3,ndebug --disable-error-checking --without-timing \</span><br><span class="line">              --without-mpit-pvars</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Examples</span><br><span class="line">--------</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">  ./configure --disable-fast MPICHLIB_CFLAGS=-O3 MPICHLIB_FFLAGS=-O3 \</span><br><span class="line">        MPICHLIB_CXXFLAGS=-O3 MPICHLIB_FCFLAGS=-O3</span><br><span class="line"></span><br><span class="line">This will cause the MPICH libraries to be built with -O3, and -O3</span><br><span class="line">will *not* be included in the mpicc and other MPI wrapper script.</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">  ./configure --disable-fast CFLAGS=-O3 FFLAGS=-O3 CXXFLAGS=-O3 FCFLAGS=-O3</span><br><span class="line"></span><br><span class="line">This will cause the MPICH libraries to be built with -O3, and -O3</span><br><span class="line">will be included in the mpicc and other MPI wrapper script.</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">There are certain compiler flags that should not be used with MPICH&apos;s</span><br><span class="line">configure, e.g. gcc&apos;s -Werror, which would confuse configure and cause</span><br><span class="line">certain configure tests to fail to detect the correct system features.</span><br><span class="line">To use -Werror in building MPICH libraries, you can pass the compiler</span><br><span class="line">flags during the make step through the Makefile variable</span><br><span class="line">MPICH_MAKE_CFLAGS as follows:</span><br><span class="line"></span><br><span class="line">  make MPICH_MAKE_CFLAGS=&quot;-Wall -Werror&quot;</span><br><span class="line"></span><br><span class="line">The content of MPICH_MAKE_CFLAGS is appended to the CFLAGS in all</span><br><span class="line">relevant Makefiles.</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">4. Alternate Channels and Devices</span><br><span class="line">=================================</span><br><span class="line"></span><br><span class="line">The communication mechanisms in MPICH are called &quot;devices&quot;. MPICH</span><br><span class="line">supports ch3 (default), as well as many third-party devices that are</span><br><span class="line">released and maintained by other institutes.</span><br><span class="line"></span><br><span class="line">                   *************************************</span><br><span class="line"></span><br><span class="line">ch3 device</span><br><span class="line">**********</span><br><span class="line">The ch3 device contains different internal communication options</span><br><span class="line">called &quot;channels&quot;. We currently support nemesis (default) and sock</span><br><span class="line">channels.</span><br><span class="line"></span><br><span class="line">nemesis channel</span><br><span class="line">---------------</span><br><span class="line">Nemesis provides communication using different networks (tcp, mx) as</span><br><span class="line">well as various shared-memory optimizations. To configure MPICH with</span><br><span class="line">nemesis, you can use the following configure option:</span><br><span class="line"></span><br><span class="line">  --with-device=ch3:nemesis</span><br><span class="line"></span><br><span class="line">The TCP network module gets configured in by default.</span><br><span class="line"></span><br><span class="line">Shared-memory optimizations are enabled by default to improve</span><br><span class="line">performance for multi-processor/multi-core platforms. They can be</span><br><span class="line">disabled (at the cost of performance) either by setting the</span><br><span class="line">environment variable MPICH_NO_LOCAL to 1, or using the following</span><br><span class="line">configure option:</span><br><span class="line"></span><br><span class="line">  --enable-nemesis-dbg-nolocal</span><br><span class="line"></span><br><span class="line">The --with-shared-memory= configure option allows you to choose how</span><br><span class="line">Nemesis allocates shared memory.  The options are &quot;auto&quot;, &quot;sysv&quot;, and</span><br><span class="line">&quot;mmap&quot;.  Using &quot;sysv&quot; will allocate shared memory using the System V</span><br><span class="line">shmget(), shmat(), etc. functions.  Using &quot;mmap&quot; will allocate shared</span><br><span class="line">memory by creating a file (in /dev/shm if it exists, otherwise /tmp),</span><br><span class="line">then mmap() the file.  The default is &quot;auto&quot;. Note that System V</span><br><span class="line">shared memory has limits on the size of shared memory segments so</span><br><span class="line">using this for Nemesis may limit the number of processes that can be</span><br><span class="line">started on a single node.</span><br><span class="line"></span><br><span class="line">mxm network module</span><br><span class="line">```````````````</span><br></pre></td></tr></table></figure><p>The mxm netmod provides support for Mellanox InfiniBand adapters.  It<br>can be built with the following configure option:</p><p>  –with-device=ch3:nemesis:mxm</p><p>If your MXM library is installed in a non-standard location, you might<br>need to help configure find it using the following configure option<br>(assuming the libraries are present in /path/to/mxm/lib and the<br>include headers are present in /path/to/mxm/include):</p><p>  –with-mxm=/path/to/mxm</p><p>(or)</p><p>  –with-mxm-lib=/path/to/mxm/lib<br>  –with-mxm-include=/path/to/mxm/include</p><p>By default, the mxm library throws warnings when the system does not<br>enable certain features that might hurt performance.  These are<br>important warnings that might cause performance degradation on your<br>system.  But you might need root privileges to fix some of them.  If<br>you would like to disable such warnings, you can set the MXM log level<br>to “error” instead of the default “warn” by using:</p><p>  MXM_LOG_LEVEL=error<br>  export MXM_LOG_LEVEL</p><p>portals4 network module<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">The portals4 netmod provides support for the Portals 4 network</span><br><span class="line">programming interface. To enable, configure with the following option:</span><br><span class="line"></span><br><span class="line">  --with-device=ch3:nemesis:portals4</span><br><span class="line"></span><br><span class="line">If the Portals 4 include files and libraries are not in the normal</span><br><span class="line">search paths, you can specify them with the following options:</span><br><span class="line"></span><br><span class="line">  --with-portals4-include= and --with-portals4-lib=</span><br><span class="line"></span><br><span class="line">... or the if lib/ and include/ are in the same directory, you can use</span><br><span class="line">the following option:</span><br><span class="line"></span><br><span class="line">  --with-portals4=</span><br><span class="line"></span><br><span class="line">If the Portals libraries are shared libraries, they need to be in the</span><br><span class="line">shared library search path. This can be done by adding the path to</span><br><span class="line">/etc/ld.so.conf, or by setting the LD_LIBRARY_PATH variable in your</span><br><span class="line">environment. It&apos;s also possible to set the shared library search path</span><br><span class="line">in the binary. If you&apos;re using gcc, you can do this by adding</span><br><span class="line"></span><br><span class="line">  LD_LIBRARY_PATH=/path/to/lib</span><br><span class="line"></span><br><span class="line">  (and)</span><br><span class="line"></span><br><span class="line">  LDFLAGS=&quot;-Wl,-rpath -Wl,/path/to/lib&quot;</span><br><span class="line"></span><br><span class="line">... as arguments to configure.</span><br><span class="line"></span><br><span class="line">Currently, use of MPI_ANY_SOURCE and MPI dynamic processes are unsupported</span><br><span class="line">with the portals4 netmod.</span><br><span class="line"></span><br><span class="line">ofi network module</span><br></pre></td></tr></table></figure></p><p>The ofi netmod provides support for the OFI network programming interface.<br>To enable, configure with the following option:</p><p>  –with-device=ch3:nemesis:ofi</p><p>If the OFI include files and libraries are not in the normal search paths,<br>you can specify them with the following options:</p><p>  –with-ofi-include= and –with-ofi-lib=</p><p>… or the if lib/ and include/ are in the same directory, you can use<br>the following option:</p><p>  –with-ofi=</p><p>If the OFI libraries are shared libraries, they need to be in the<br>shared library search path. This can be done by adding the path to<br>/etc/ld.so.conf, or by setting the LD_LIBRARY_PATH variable in your<br>environment. It’s also possible to set the shared library search path<br>in the binary. If you’re using gcc, you can do this by adding</p><p>  LD_LIBRARY_PATH=/path/to/lib</p><p>  (and)</p><p>  LDFLAGS=”-Wl,-rpath -Wl,/path/to/lib”</p><p>… as arguments to configure.</p><h2 id="sock-channel"><a href="#sock-channel" class="headerlink" title="sock channel"></a>sock channel</h2><p>sock is the traditional TCP sockets based communication channel. It<br>uses TCP/IP sockets for all communication including intra-node<br>communication. So, though the performance of this channel is worse<br>than that of nemesis, it should work on almost every platform. This<br>channel can be configured using the following option:</p><p>  –with-device=ch3:sock</p><p>pamid device</p><hr><p>This is the device used on the IBM Blue Gene/Q system.  The following<br>configure options can be used:</p><p>  ./configure –host=powerpc64-bgq-linux                                \<br>              –with-device=pamid:BGQ                                   \<br>              –with-file-system=bg+bglockless</p><p>The Blue Gene/Q cross compilers must either be in the $PATH, or<br>explicitly specified using environment variables, before configure.<br>For example:</p><p>  PATH=$PATH:/bgsys/drivers/ppcfloor/gnu-linux/bin</p><p>or</p><p>  CC=/bgsys/drivers/ppcfloor/gnu-linux/bin/powerpc64-bgq-linux-gcc<br>  CXX=…<br>  …</p><p>There are several other configure options that are specific to building<br>on a Blue Gene/Q system.  See the wiki page for more information:</p><p>  <a href="https://wiki.mpich.org/mpich/index.php/BGQ" target="_blank" rel="noopener">https://wiki.mpich.org/mpich/index.php/BGQ</a></p><hr><h1 id="5-Alternate-Process-Managers"><a href="#5-Alternate-Process-Managers" class="headerlink" title="5. Alternate Process Managers"></a>5. Alternate Process Managers</h1><h2 id="hydra"><a href="#hydra" class="headerlink" title="hydra"></a>hydra</h2><p>Hydra is the default process management framework that uses existing<br>daemons on nodes (e.g., ssh, pbs, slurm, sge) to start MPI<br>processes. More information on Hydra can be found at<br><a href="http://wiki.mpich.org/mpich/index.php/Using_the_Hydra_Process_Manager" target="_blank" rel="noopener">http://wiki.mpich.org/mpich/index.php/Using_the_Hydra_Process_Manager</a></p><h2 id="gforker"><a href="#gforker" class="headerlink" title="gforker"></a>gforker</h2><p>gforker is a process manager that creates processes on a single<br>machine, by having mpiexec directly fork and exec them. gforker is<br>mostly meant as a research platform and for debugging purposes, as it<br>is only meant for single-node systems.</p><h2 id="slurm"><a href="#slurm" class="headerlink" title="slurm"></a>slurm</h2><p>SLURM is an external process manager not distributed with<br>MPICH. MPICH’s default process manager, hydra, has native support<br>for slurm and you can directly use it in slurm environments (it will<br>automatically detect slurm and use slurm capabilities). However, if<br>you want to use the slurm provided “srun” process manager, you can use<br>the “–with-pmi=slurm –with-pm=no” option with configure. Note that<br>the “srun” process manager that comes with slurm uses an older PMI<br>standard which does not have some of the performance enhancements that<br>hydra provides in slurm environments.</p><hr><h1 id="6-Alternate-Configure-Options"><a href="#6-Alternate-Configure-Options" class="headerlink" title="6. Alternate Configure Options"></a>6. Alternate Configure Options</h1><p>MPICH has a number of other features. If you are exploring MPICH as<br>part of a development project, you might want to tweak the MPICH<br>build with the following configure options. A complete list of<br>configuration options can be found using:</p><p>   ./configure –help</p><hr><h1 id="7-Testing-the-MPICH-installation"><a href="#7-Testing-the-MPICH-installation" class="headerlink" title="7. Testing the MPICH installation"></a>7. Testing the MPICH installation</h1><p>To test MPICH, we package the MPICH test suite in the MPICH<br>distribution. You can run the test suite using:</p><pre><code>make testing</code></pre><p>The results summary will be placed in test/summary.xml</p><hr><h1 id="8-Fault-Tolerance"><a href="#8-Fault-Tolerance" class="headerlink" title="8. Fault Tolerance"></a>8. Fault Tolerance</h1><p>MPICH has some tolerance to process failures, and supports<br>checkpointing and restart.</p><h2 id="Tolerance-to-Process-Failures"><a href="#Tolerance-to-Process-Failures" class="headerlink" title="Tolerance to Process Failures"></a>Tolerance to Process Failures</h2><p>The features described in this section should be considered<br>experimental.  Which means that they have not been fully tested, and<br>the behavior may change in future releases. The below notes are some<br>guidelines on what can be expected in this feature:</p><ul><li><p>ERROR RETURNS: Communication failures in MPICH are not fatal<br>errors.  This means that if the user sets the error handler to<br>MPI_ERRORS_RETURN, MPICH will return an appropriate error code in<br>the event of a communication failure.  When a process detects a<br>failure when communicating with another process, it will consider<br>the other process as having failed and will no longer attempt to<br>communicate with that process.  The user can, however, continue<br>making communication calls to other processes.  Any outstanding<br>send or receive operations to a failed process, or wildcard<br>receives (i.e., with MPI_ANY_SOURCE) posted to communicators with a<br>failed process, will be immediately completed with an appropriate<br>error code.</p></li><li><p>COLLECTIVES: For collective operations performed on communicators<br>with a failed process, the collective would return an error on<br>some, but not necessarily all processes. A collective call<br>returning MPI_SUCCESS on a given process means that the part of the<br>collective performed by that process has been successful.</p></li><li><p>PROCESS MANAGER: If used with the hydra process manager, hydra will<br>detect failed processes and notify the MPICH library.  Users can<br>query the list of failed processes using MPIX_Comm_group_failed().<br>This functions returns a group consisting of the failed processes<br>in the communicator.  The function MPIX_Comm_remote_group_failed()<br>is provided for querying failed processes in the remote processes<br>of an intercommunicator.</p><p>Note that hydra by default will abort the entire application when<br>any process terminates before calling MPI_Finalize.  In order to<br>allow an application to continue running despite failed processes,<br>you will need to pass the -disable-auto-cleanup option to mpiexec.</p></li><li><p>FAILURE NOTIFICATION: THIS IS AN UNSUPPORTED FEATURE AND WILL<br>ALMOST CERTAINLY CHANGE IN THE FUTURE!</p><p>In the current release, hydra notifies the MPICH library of failed<br>processes by sending a SIGUSR1 signal.  The application can catch<br>this signal to be notified of failed processes.  If the application<br>replaces the library’s signal handler with its own, the application<br>must be sure to call the library’s handler from it’s own<br>handler.  Note that you cannot call any MPI function from inside a<br>signal handler.</p></li></ul><h2 id="Checkpoint-and-Restart"><a href="#Checkpoint-and-Restart" class="headerlink" title="Checkpoint and Restart"></a>Checkpoint and Restart</h2><p>MPICH supports checkpointing and restart fault-tolerance using BLCR.</p><p>CONFIGURATION</p><p>First, you need to have BLCR version 0.8.2 or later installed on your<br>machine.  If it’s installed in the default system location, you don’t<br>need to do anything.</p><p>If BLCR is not installed in the default system location, you’ll need<br>to tell MPICH’s configure where to find it. You might also need to<br>set the LD_LIBRARY_PATH environment variable so that BLCR’s shared<br>libraries can be found.  In this case add the following options to<br>your configure command:</p><p>  –with-blcr=<blcr_install_dir><br>  LD_LIBRARY_PATH=<blcr_install_dir>/lib</blcr_install_dir></blcr_install_dir></p><p>where <blcr_install_dir> is the directory where BLCR has been<br>installed (whatever was specified in –prefix when BLCR was<br>configured).</blcr_install_dir></p><p>After it’s configured compile as usual (e.g., make; make install).</p><p>Note, checkpointing is only supported with the Hydra process manager.</p><p>VERIFYING CHECKPOINTING SUPPORT</p><p>Make sure MPICH is correctly configured with BLCR. You can do this<br>using:</p><p>  mpiexec -info</p><p>This should display ‘BLCR’ under ‘Checkpointing libraries available’.</p><p>CHECKPOINTING THE APPLICATION</p><p>There are two ways to cause the application to checkpoint. You can ask<br>mpiexec to periodically checkpoint the application using the mpiexec<br>option -ckpoint-interval (seconds):</p><p>  mpiexec -ckpointlib blcr -ckpoint-prefix /tmp/app.ckpoint \<br>      -ckpoint-interval 3600 -f hosts -n 4 ./app</p><p>Alternatively, you can also manually force checkpointing by sending a<br>SIGUSR1 signal to mpiexec.</p><p>The checkpoint/restart parameters can also be controlled with the<br>environment variables HYDRA_CKPOINTLIB, HYDRA_CKPOINT_PREFIX and<br>HYDRA_CKPOINT_INTERVAL.</p><p>To restart a process:</p><p>  mpiexec -ckpointlib blcr -ckpoint-prefix /tmp/app.ckpoint -f hosts -n 4 -ckpoint-num <n></n></p><p>where <n> is the checkpoint number you want to restart from.</n></p><p>These instructions can also be found on the MPICH wiki:</p><p>  <a href="http://wiki.mpich.org/mpich/index.php/Checkpointing" target="_blank" rel="noopener">http://wiki.mpich.org/mpich/index.php/Checkpointing</a></p><hr><h1 id="9-Developer-Builds"><a href="#9-Developer-Builds" class="headerlink" title="9. Developer Builds"></a>9. Developer Builds</h1><p>For MPICH developers who want to directly work on the primary version<br>control system, there are a few additional steps involved (people<br>using the release tarballs do not have to follow these steps). Details<br>about these steps can be found here:<br><a href="http://wiki.mpich.org/mpich/index.php/Getting_And_Building_MPICH" target="_blank" rel="noopener">http://wiki.mpich.org/mpich/index.php/Getting_And_Building_MPICH</a></p><hr><h1 id="10-Multiple-Fortran-compiler-support"><a href="#10-Multiple-Fortran-compiler-support" class="headerlink" title="10. Multiple Fortran compiler support"></a>10. Multiple Fortran compiler support</h1><p>If the C compiler that is used to build MPICH libraries supports both<br>multiple weak symbols and multiple aliases of common symbols, the<br>Fortran binding can support multiple Fortran compilers. The<br>multiple weak symbols support allow MPICH to provide different name<br>mangling scheme (of subroutine names) required by differen Fortran<br>compilers. The multiple aliases of common symbols support enables<br>MPICH to equal different common block symbols of the MPI Fortran<br>constant, e.g. MPI_IN_PLACE, MPI_STATUS_IGNORE. So they are understood<br>by different Fortran compilers.</p><p>Since the support of multiple aliases of common symbols is<br>new/experimental, users can disable the feature by using configure<br>option –disable-multi-aliases if it causes any undesirable effect,<br>e.g. linker warnings of different sizes of common symbols, MPIFCMB*<br>(the warning should be harmless).</p><p>We have only tested this support on a limited set of<br>platforms/compilers.  On linux, if the C compiler that builds MPICH is<br>either gcc or icc, the above support will be enabled by configure.  At<br>the time of this writing, pgcc does not seem to have this multiple<br>aliases of common symbols, so configure will detect the deficiency and<br>disable the feature automatically.  The tested Fortran compilers<br>include GNU Fortran compilers (gfortan), Intel Fortran compiler<br>(ifort), Portland Group Fortran compilers (pgfortran), Absoft Fortran<br>compilers (af90), and IBM XL fortran compiler (xlf).  What this means<br>is that if mpich is built by gcc/gfortran, the resulting mpich library<br>can be used to link a Fortran program compiled/linked by another<br>fortran compiler, say pgf90, say through mpifort -fc=pgf90.  As long<br>as the Fortran program is linked without any errors by one of these<br>compilers, the program shall be running fine.</p><hr><h1 id="11-ABI-Compatibility"><a href="#11-ABI-Compatibility" class="headerlink" title="11. ABI Compatibility"></a>11. ABI Compatibility</h1><p>The MPICH ABI compatibility initiative was announced at SC 2014<br>(<a href="http://www.mpich.org/abi" target="_blank" rel="noopener">http://www.mpich.org/abi</a>).  As a part of this initiative, Argonne,<br>Intel, IBM and Cray have committed to maintaining ABI compatibility<br>with each other.</p><p>As a first step in this initiative, starting with version 3.1, MPICH<br>is binary (ABI) compatible with Intel MPI 5.0.  This means you can<br>build your program with one MPI implementation and run with the other.<br>Specifically, binary-only applications that were built and distributed<br>with one of these MPI implementations can now be executed with the<br>other MPI implementation.</p><p>Some setup is required to achieve this.  Suppose you have MPICH<br>installed in /path/to/mpich and Intel MPI installed in /path/to/impi.</p><p>You can run your application with mpich using:</p><p>   % export LD_LIBRARY_PATH=/path/to/mpich/lib:$LD_LIBRARY_PATH<br>   % mpiexec -np 100 ./foo</p><p>or using Intel MPI using:</p><p>   % export LD_LIBRARY_PATH=/path/to/impi/lib:$LD_LIBRARY_PATH<br>   % mpiexec -np 100 ./foo</p><p>This works irrespective of which MPI implementation your application<br>was compiled with, as long as you use one of the MPI implementations<br>in the ABI compatibility initiative.<br>```</p>]]></content>
    
    <summary type="html">
    
      mpich3.2.1-release
    
    </summary>
    
      <category term="高性能计算" scheme="http://blog.zhangchi.xyz/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
      <category term="mpich" scheme="http://blog.zhangchi.xyz/tags/mpich/"/>
    
  </entry>
  
  <entry>
    <title>清理Zabbix历史数据</title>
    <link href="http://blog.zhangchi.xyz/%E6%B8%85%E7%90%86Zabbix%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE.html"/>
    <id>http://blog.zhangchi.xyz/清理Zabbix历史数据.html</id>
    <published>2018-02-08T10:35:41.000Z</published>
    <updated>2018-02-08T10:38:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>统计Zabbix各表的大小<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT table_name AS &quot;Tables&quot;,</span><br><span class="line">round(((data_length + index_length) / 1024 / 1024), 2) &quot;Size in MB&quot; </span><br><span class="line">FROM information_schema.TABLES</span><br><span class="line">WHERE table_schema = &apos;zabbix&apos;</span><br><span class="line">ORDER BY (data_length + index_length) DESC;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;统计Zabbix各表的大小&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/
      
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="Zabbix" scheme="http://blog.zhangchi.xyz/tags/Zabbix/"/>
    
  </entry>
  
  <entry>
    <title>Linux内核源文件网址</title>
    <link href="http://blog.zhangchi.xyz/Linux%E5%86%85%E6%A0%B8%E6%BA%90%E6%96%87%E4%BB%B6%E7%BD%91%E5%9D%80.html"/>
    <id>http://blog.zhangchi.xyz/Linux内核源文件网址.html</id>
    <published>2018-01-26T08:56:46.000Z</published>
    <updated>2018-02-24T03:24:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.kernel.org/pub/linux/kernel/" target="_blank" rel="noopener">click here to download</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.kernel.org/pub/linux/kernel/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;click here to download&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="Linux开发" scheme="http://blog.zhangchi.xyz/categories/Linux%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="Linux Kernel" scheme="http://blog.zhangchi.xyz/tags/Linux-Kernel/"/>
    
  </entry>
  
  <entry>
    <title>Linux网络设备驱动原理文章收藏</title>
    <link href="http://blog.zhangchi.xyz/Linux%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E5%8E%9F%E7%90%86%E6%96%87%E7%AB%A0%E6%94%B6%E8%97%8F.html"/>
    <id>http://blog.zhangchi.xyz/Linux网络设备驱动原理文章收藏.html</id>
    <published>2018-01-08T09:31:28.000Z</published>
    <updated>2018-04-22T10:58:18.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://blog.chinaunix.net/uid-20672257-id-3147768.html" target="_blank" rel="noopener">网络设备驱动基本原理和框架</a></li><li><a href="http://blog.csdn.net/xy010902100449/article/details/47157113" target="_blank" rel="noopener">net_device 等数据结构</a></li><li><a href="http://blog.chinaunix.net/uid-29547110-id-5038812.html" target="_blank" rel="noopener">Linux网桥实现分析</a></li><li><a href="http://bbs.chinaunix.net/thread-2005999-1-1.html" target="_blank" rel="noopener">Netfilter之连接跟踪实现机制初步分析</a></li><li><a href="http://blog.csdn.net/jasonchen_gbd/article/details/44874321" target="_blank" rel="noopener">Linux协议栈-netfilter(2)-conntrack</a></li><li><a href="https://www.linuxidc.com/search.aspx?Where=Nkey&amp;Keyword=Linux内核--网络内核实现分析" target="_blank" rel="noopener">Linux网络内核实现分析</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/kernel/l-netbr/" target="_blank" rel="noopener">Linux网桥实现分析</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-20672257-id-3147768.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;网络设备驱动基本原理和框架&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="网络" scheme="http://blog.zhangchi.xyz/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Linux网络" scheme="http://blog.zhangchi.xyz/tags/Linux%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Linux Namespace文章</title>
    <link href="http://blog.zhangchi.xyz/Linux-Namespace%E6%96%87%E7%AB%A0.html"/>
    <id>http://blog.zhangchi.xyz/Linux-Namespace文章.html</id>
    <published>2017-12-19T02:49:56.000Z</published>
    <updated>2018-04-25T03:08:32.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://lwn.net/Articles/531114/" target="_blank" rel="noopener">Namespaces in operation</a><br><a href="http://www.opencloudblog.com/?p=42" target="_blank" rel="noopener">Net Namespace</a><br><a href="https://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespaces" target="_blank" rel="noopener">Linux Namespace</a><br><a href="https://hacpai.com/article/1419219697193?p=1&amp;m=0" target="_blank" rel="noopener">使用Golang操作Linux Namespaces</a><br><a href="http://www.infoq.com/cn/articles/docker-kernel-knowledge-cgroups-resource-isolation#" target="_blank" rel="noopener">cgroups资源限制</a></p>]]></content>
    
    <summary type="html">
    
      Linux namespace
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="Namespace" scheme="http://blog.zhangchi.xyz/tags/Namespace/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结8-未尽事宜</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%938-%E6%9C%AA%E5%B0%BD%E4%BA%8B%E5%AE%9C.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结8-未尽事宜.html</id>
    <published>2017-12-12T12:20:28.000Z</published>
    <updated>2018-02-24T08:37:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="未尽事宜"><a href="#未尽事宜" class="headerlink" title="未尽事宜"></a>未尽事宜</h3><p>由于时间精力和能力有限，很多想做的事情还没有来得及做，或者没能做，在这里也列出来，希望今后有管理员可以将这些工作完成。<br><a id="more"></a></p><h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><ul><li>安全问题</li></ul><p>机房环境监控系统处于瘫痪状态，市电、UPS状态、空调状态、温度、湿度、漏水、消防、监控等信息都不能很好的拿到。有些设备已经处于停止或者故障状态，导致无法准确获取这些信息，一旦出现安全问题无法及时进行处理。</p><p>集群外部只有一个防火墙，服务器上面也只是简单配置iptables来限制用户登录，实际上安全等级不够高，安全控制粒度不够细。</p><p>一个IP若可以登录到集群，该机器可以使用任意的账号登录，也就是说存在账号互相借用的问题，使得有些用户将资源出售或者转借的风险。</p><p>密码安全的问题，有些用户的密码设置超级简单，而且不会更换，如果密码被其他人获取，就会导致资源浪费问题。</p><p>服务安全问题，集群未对在集群上运行的任意服务进行监控和安全配置，如果收到恶意攻击，会导致集群崩溃。<br>最典型，也最重要的服务是文件系统，文件系统本身故障频繁，另外如果收到而已破坏，将会直接导致软件环境破坏、用户数据丢失、集群崩溃等。</p><p>长期不登录账号，及时锁定。<br>根据用户登录IP地址统计情况，出现异常IP登录时，发送验证码或者提醒邮件。</p><ul><li>root权限的管理问题</li><li>登录跳转控制问题</li><li>流量管理和带宽控制</li><li>磁盘配额管理</li></ul><h4 id="集群分区管理"><a href="#集群分区管理" class="headerlink" title="集群分区管理"></a>集群分区管理</h4><p>由于集群现在是面向全校提供服务，主要的用户分为两种类型，一种是实验室内部同学进行试验，另外一种是实验室外部的用户进行高性能计算，这两类用户的需求差异很大，实验室内部用户一般是进行了某些系统的优化，需要一批机器部署自己的实验系统，然后比较，一般周期不会特别长，但是对系统的权限要求比较高，对系统的环境破坏比较大，使用完成后，需要做的工作也非常多。而第二种用户只需要集群提供了相应的软件，需要一批节点，提交作业，对系统环境破坏小，但是对机器数量要求或者资源的需求比较大。</p><p>目前集群的分配策略是，来了一个用户，给他分配几个节点，用完之后回收，但是实际上，除了实验室内部短期做实验的用户，外部用户一般长期需要计算资源，基本上资源分配出去之后，就很难回收，长期占有，甚至不用了都不会主动归还，导致下次有用户进行资源申请时，很难找到空闲的资源进行使用。</p><p>根据集群用户的需求和特点以及资源的使用情况，目前的想法是，将集群分为A和B两个区域，其中A区提供给实验室内部人员使用，B区专门用于进行高性能计算，这样的好处是，B区可以使用队列调度系统，用户通过调度系统提交作业，然后就可以充分利用B区的所有机器，大家的作业按照顺序进行调度，提高了资源利用率，同时也加快了计算效率。</p><p>由于时间有限，以上想法暂时未执行，希望下一届管理员能够争取做到使用队列管理系统进行作业提交，保证用户计算公平性、资源充分利用。</p><p>队列系统已经于2017年下半年开启使用了。效果挺好，资源利用率得到明显提高，同时降低了节点分配的成本。</p><h4 id="maui-d作业调度器安装"><a href="#maui-d作业调度器安装" class="headerlink" title="maui.d作业调度器安装"></a>maui.d作业调度器安装</h4><p>Torque自带的调度器非常简单，无法实现复杂的任务调度算法。maui.d相对而言更加高级，性能更加出色，希望后期能配置该调度器，提升Torque的灵活性和稳定性。</p><h4 id="LDAP或NIS服务配置"><a href="#LDAP或NIS服务配置" class="headerlink" title="LDAP或NIS服务配置"></a>LDAP或NIS服务配置</h4><p>每次分配账户，都需要将/etc/passwd，/etc/shadow，/etc/group等文件复制到计算节点，工作量还是挺大的，尽管有pssh、pdsh等工具，但是每次复制也会带来问题。如果说所有机器的系统一样，软件环境一样，直接复制这些文件不会有其他问题，但是如果系统不一样，直接覆盖，会带来系统或者软件故障，甚至系统无法启动的问题。</p><p>如果配置了LDAP或者NIS，可以只在一台机器维护所有的账户和共享文件信息，其他机器通过访问相关服务完成用户认证和基本配置文件访问。</p><h4 id="集群资源管理系统"><a href="#集群资源管理系统" class="headerlink" title="集群资源管理系统"></a>集群资源管理系统</h4><p>集群目前管理相对比较零散，用户多，管理管理起来比较困难。例如说现在采用的是一个数据库记录节点分配和用户信息，操作效率低，使用极其不方便，找个信息要翻半天。</p><p>期望下一届管理员能够开发一个简单的Web系统，用于集群节点分配和用户信息的管理。例如说，设计一个页面，查询用户历史使用节点信息，目前使用信息，违法操作行为等。同时可以查询某个节点的使用情况，有多少个用户使用，维修情况等。</p><h4 id="Kadeploy部署软件的安装"><a href="#Kadeploy部署软件的安装" class="headerlink" title="Kadeploy部署软件的安装"></a>Kadeploy部署软件的安装</h4><p>这个软件非常好，了解和安装可以减轻很多工作。</p><h4 id="GridView管理软件安装"><a href="#GridView管理软件安装" class="headerlink" title="GridView管理软件安装"></a>GridView管理软件安装</h4><p>集群建设初期，厂家提供了一些用户文档给用户，但是那些文档都是基于厂家的Gridview集群管理系统建设的，现在Gridview被破坏了，无法正常运行，加上Gridview本身比较老了，厂家也没有提供新的版本给我们，另外安装文档又不是很详细，导致有些配置无法正常进行，导致无法使用。</p><p>GridView是厂家自己开发的集群管理软件，该软件功能还是挺强大的，界面也比较友好，其实挺适合作为集群管理软件来使用，但是由于之前集群零散分配给用户使用，将GridView的节点信息等都破坏了，整个软件无法控制所有的集群。</p><p>如果可以，寻求厂家的帮助，将GridView集群管理软件装好，主要是该软件提供了界面化提交作业的界面，可以非常方便使用，降低用户的使用难度。</p><h4 id="集群主页或FAQ开发"><a href="#集群主页或FAQ开发" class="headerlink" title="集群主页或FAQ开发"></a>集群主页或FAQ开发</h4><p>同时还能够使用开源的博客系统例如Wordpress等构建一个集群首页或者社区，用于用户之间的信息交流，特别是一些集群使用的方法、软件安装方法、集群公告等信息，这样不仅用户学习起来非常方便，同时也减少了管理员的工作量，另外提升了集群管理的科学有效性。</p><p>当然最重要的是能够制定一些规章制度，规范用户的使用行为，规范资源的分配和管理工作，这样能够保证一切能够高效进行。</p><h3 id="期待更好"><a href="#期待更好" class="headerlink" title="期待更好"></a>期待更好</h3><p>期待集群的管理更加科学化，同时也期待集群的管理和运维工作得到实验室的重视，集群建设资金不菲，价值也不菲，但是由于缺少管理人员，缺少资金支持，缺少重视，还没有完全发挥它的作用，因此期待在集群报废之前，能够充分利用集群的价值，为更多的科研项目服务，期待实验室的集群管理制度更上一层楼。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;未尽事宜&quot;&gt;&lt;a href=&quot;#未尽事宜&quot; class=&quot;headerlink&quot; title=&quot;未尽事宜&quot;&gt;&lt;/a&gt;未尽事宜&lt;/h3&gt;&lt;p&gt;由于时间精力和能力有限，很多想做的事情还没有来得及做，或者没能做，在这里也列出来，希望今后有管理员可以将这些工作完成。&lt;br&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结7-故障处理</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%937-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结7-故障处理.html</id>
    <published>2017-12-12T12:20:27.000Z</published>
    <updated>2018-02-24T08:37:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="常见故障"><a href="#常见故障" class="headerlink" title="常见故障"></a>常见故障</h3><h4 id="用户无法登录到集群"><a href="#用户无法登录到集群" class="headerlink" title="用户无法登录到集群"></a>用户无法登录到集群</h4><p>有如下几种原因会导致用户无法登录到集群：</p><ul><li>用户使用的是校园无线网络，校园无线网络不允许登录到集群。</li><li>用户在校园网外部登录集群，集群IP地址未暴露给外网，校园网外部无法直接访问。</li><li>用户使用校园内部有线网络，无法登录到集群，需要在集群管理服务器添加IP地址许可。</li><li>用户不存在，则无法登录集群。</li><li>用户长期不登录或者已归还集群，账户密码被锁定，则无法登录集群。<a id="more"></a>解决方法：</li><li>检查IP地址是否允许登录，若不允许登录，则添加IP地址到iptables。</li><li>检查是否存在该用户，若不存在，则使用clusconf命令添加用户。</li><li>若用户存在，则执行<code>passwd -u username</code>解锁用户密码。</li></ul><h4 id="用户无法登录节点"><a href="#用户无法登录节点" class="headerlink" title="用户无法登录节点"></a>用户无法登录节点</h4><p>这种情况可能的原因如下：</p><ul><li>1.节点死机或者被关机。</li><li>2.节点负载过高。</li><li>3.该机器限制了用户从某些主机的登录行为。</li><li>4.SSH服务默认端口被恶意更改。</li></ul><p>解决方法依次如下：</p><ul><li>1.使用IPMI重启机器。</li><li>2.待负载降下来之后登录。</li><li>3.从其他机器跳转到节点后，编辑/etc/hosts.deny解除登录节点限制。</li><li>4.询问上一批次使用的用户具体端口，或者直接去机房本地登录后进行修改。</li></ul><h4 id="用户登录节点时需要密码"><a href="#用户登录节点时需要密码" class="headerlink" title="用户登录节点时需要密码"></a>用户登录节点时需要密码</h4><p>出现需要输入密码，可能原因如下：</p><ul><li>Lustre文件系统没有正确挂载，导致用户主目录没有，需要输入密码。</li><li>该节点没有分配给该用户。</li><li>该用户主目录中的密钥文件不存在或者权限不正确。</li></ul><p>解决方法依次如下：</p><ul><li>1.登录到节点解决Lustre文件系统挂载的问题，参考<a href="#Lustre">集群分布式文件系统</a>章节进行解决。</li><li>2.考虑分配节点给用户或禁止用户登录该节点。</li><li>3.提示用户生成密钥文件或修改密钥文件的权限。</li></ul><h4 id="Lustre文件系统无法挂载"><a href="#Lustre文件系统无法挂载" class="headerlink" title="Lustre文件系统无法挂载"></a>Lustre文件系统无法挂载</h4><p>导致无法挂载的原因较多，较常见的如下：</p><ul><li>Infiniband网路故障。</li><li>Lustre相关模块未加载。</li><li>计算节点内核被升级或者更换。</li><li>Lustre文件系统本身故障。</li></ul><p>解决方法依次如下：</p><ul><li>1.参考<a href="#Infiniband">Infiniband网络故障解决</a>部分解决Infiniband网络故障。</li><li>2.加载Lustre相关模块，若是由于Infiniband网络故障导致的模块无法加载，请先解决该网络故障，或者屏蔽Infiniband网络模块。</li><li>3.将内核修改为编译过Lustre的内核。</li><li>4.寻到具体故障，并进行解决，如果无法解决，首先在所有节点将/public或/home目录卸载，然后重启Lustre文件系统。</li></ul><h4 id="无法获取zabbix监控信息"><a href="#无法获取zabbix监控信息" class="headerlink" title="无法获取zabbix监控信息"></a>无法获取zabbix监控信息</h4><p>可能原因如下：</p><ul><li>zabbix配置信息不正确。</li><li>zabbix相关文件夹权限问题。</li><li>zabbix配置文件与服务端不一致。</li><li>防火墙未开放相关端口。</li></ul><p>解决方法依次如下：</p><ul><li>1.重新配置zabbix配置信息，编辑/etc/zabbix/zabbix_agent.conf文件中的Hostname，Server，ServerActive信息。</li><li>2.将/var/log/zabbix和/var/run/zabbix文件夹的权限分配给zabbix用户，并检查用户是否存在，若不存在，则不添加。</li><li>3.查看服务器配置信息，并修改配置文件。这个一般是Server或ServerActive信息不一致导致。</li><li>4.关闭防火墙或者添加端口允许。</li></ul><h4 id="IPMI无法获取服务器信息"><a href="#IPMI无法获取服务器信息" class="headerlink" title="IPMI无法获取服务器信息"></a>IPMI无法获取服务器信息</h4><p>可能有如下原因导致：</p><ul><li>IPMI相关模块未加载。</li><li>IPMI软件被恶意卸载，特别是有root权限的用户。</li><li>集群长时间运行导致IPMI模块无法正常工作，可能需要卸下并插上刀片。</li></ul><p>解决方法依次如下：</p><ul><li><p>1.执行如下命令加载IPMI模块，然后再次执行IPMI命令，查看是否正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modprobe ipmi_watchdog</span><br><span class="line">modprobe ipmi_poweroff</span><br><span class="line">modprobe ipmi_devintf</span><br><span class="line">modprobe ipmi_si</span><br><span class="line">modprobe ipmi_msghandler</span><br></pre></td></tr></table></figure></li><li><p>2.重新安装freeipmi软件包，具体网上有，集群的公共软件目录中也有。</p></li><li>3.刀片关机，然后拔掉后面的Infiniband线缆，拔出刀片，等待2分钟，插入刀片，插上Infiniband线缆，启动刀片，一般而言会好。如果实在不行，可以考虑关闭刀箱，并切断电源，但是这种做法不太推荐，除非这个刀箱的大多数刀片都无法进行IPMI控制，这些节点又经常需要进行重启，且无法使用操作系统进行开关机命令重启的情况下，才能使用。</li></ul><h3 id="售后报修"><a href="#售后报修" class="headerlink" title="售后报修"></a>售后报修</h3><p>集群建设日期是2012年12月，合同约定保修日期是5年，报修到期日期是2017年12月，在此期间，任何硬件故障、软件故障都可以打电话给厂家全国客服，报修之前一定要把服务器的序列号记下来，然后进行报修。报修之后，厂家湖北分公司会有技术支持联系，之后会进行故障处理。</p><h3 id="故障历史"><a href="#故障历史" class="headerlink" title="故障历史"></a>故障历史</h3><h4 id="挖矿病毒"><a href="#挖矿病毒" class="headerlink" title="挖矿病毒"></a>挖矿病毒</h4><p>在每个节点上运行有很多的[watchdog]进程，这些进程原本是操作系统上进行不同CPU之间进程迁移的进程，挖矿病毒伪装为[watchdog]进程，占用全部的CPU进行计算，每天晚上22:00开始自动执行/etc/systemc命令，然后产生多个[watchdog]进程，在次日的7:00自动执行killall -9 [watchdog]杀死该进程。这些自动操作是添加到crontab中自动执行的。</p><p>另外一个特点就是，ssh登录到感染该病毒的节点之后，[watchdog]进程就不见了，但是使用top命令查看系统过去1、5、15分钟的系统负载都是很高的，原因是在/etc/bashrc中包含如下命令killall -9 [watchdog]，每次用户登录时自动执行该脚本文件杀死进程。</p><p>病毒感染规模很大，大约有1/3的节点感染了该病毒。每天22:00至7:00运行，导致机房温度曲线在这个时段偏高，这个可以通过空调控制面板中的温度曲线看到。</p><p>根据文件的修改日期，大概是2014年12月发生的。如今严格控制了节点的网络访问，也限制了登录到集群的外网IP地址，现象比较少出现了。</p><h4 id="RAID磁盘阵列崩溃"><a href="#RAID磁盘阵列崩溃" class="headerlink" title="RAID磁盘阵列崩溃"></a>RAID磁盘阵列崩溃</h4><p>某一台Lustre文件系统的对象存储服务器后面的磁盘阵列上面的所有磁盘亮红灯，/home目录无法正常读取或写入文件，由于/home目录提供的是用户文件，还有用户的软件，用户科研数据，为了防止情况恶化，直接停止了集群的使用，集群准备维修。</p><p>售后过来之后，读取了该服务器的RAID配置信息，发现是在一块磁盘出现问题，RAID进行Rebuild的过程中，又坏掉了一块磁盘，导致RAID直接崩溃了。他们尝试修复阵列信息，但是修复后，阵列一直处于Rebuild的过程中，Rebuid完成之后，又开始Rebuild过程，里面的文件被一次又一次的被破坏了，最后Lustre文件系统的/home目录压根就无法挂载了。因此该阵列上面的所有用户信息都丢失了。</p><p>由于这个阵列出现故障，直接将该服务器从Lustre的对象管理服务器中踢出，Lustre文件系统还是无法工作。售后没有办法解决这个问题了，而直接采用外部公司来恢复数据，价格估计很贵，加上集群本身是免费提供给用户使用的，因此也没有义务修复数据，因此集群就开始暂停服务，等待解决方案了。</p><p>最后经过几个月的等待，解决方案是重新安装Lustre文件系统。</p><h4 id="Infiniband网络在Initializing和Active间切换"><a href="#Infiniband网络在Initializing和Active间切换" class="headerlink" title="Infiniband网络在Initializing和Active间切换"></a>Infiniband网络在Initializing和Active间切换</h4><p>在集群重启时，集群所有服务器、以太网交换机、Infiniband网络交换机、BMC模块下电后，重新上电，当所有机器启动后，计算节点的IB网络状态正常（通过ibstat中的State字段查看），但是Lustre文件系统所在服务器，node309、node310、node317-node323的IB网络状态一直在切换。导致Lustre文件系统不能正常服务。即，计算节点无法正常挂载Lustre文件系统，即使挂载成功，也会由于IB网络不断在Initializing和Active之间切换，导致非常卡顿，无法正常使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">发现问题</span><br><span class="line">紧急处理措施</span><br><span class="line">恢复挂载</span><br></pre></td></tr></table></figure><h5 id="终极解决方案"><a href="#终极解决方案" class="headerlink" title="终极解决方案"></a>终极解决方案</h5><p><a href="http://blog.zhangchi.xyz/opensmd服务导致IB网络状态不断切换.html">opensmd服务导致IB网络状态不断切换</a><br>或者参考上面的内容<a href="#ibpro">IB网络故障</a><br><strong>教训：一定要每周检查磁盘的状态，特别是Lustre文件系统的存储服务器的磁盘，及时找到有故障的磁盘信息，并将磁盘报修，如果出现亮红灯的情况，一定要及时更换，请参考<a href="#disk">服务器磁盘故障发现与处理</a>部分进行处理</strong></p><h4 id="异常停电"><a href="#异常停电" class="headerlink" title="异常停电"></a>异常停电</h4><p>说到停电，一般指的是市电。集群的配电房后面有一个箱变，这个变压器的交流电是从<strong>其他大楼</strong>连接过来，不是从集群所在楼栋连接的线路。因此需要密切关注<strong>其他大楼</strong>的停电信息。<br>如果有相关停电通知，最好提前一天禁止用户提交作业和使用集群，停电之前的3-5小时按照<a href="#poweroff">集群关机流程</a>来进行集群关机。然后断开配电柜和UPS相关的输出开关，避免突然来电导致的其他故障。<br>如果没有任何通知就突然停电了，事情就非常麻烦了。市电断开以后，UPS会给服务器继续供电，服务器会继续工作，而主机房空调，照明，新风等所有非IT设备都会断电，也就是说，机房的温度会陡然升高，直至UPS电量耗尽。因此在夏天，温度可能会达到60-80摄氏度，冬天可能达到50-70度左右，很可能导致火灾。不敢想象。所以这种情况一旦出现，必须第一时间感到配电房，将UPS输出的总开关断掉，停止服务器，防止继续产热。</p><p>在本人任职期间，机房出现过一次异常停电的情况，当时未接到任何通知，当时机房的火灾报警器响起，刚开始以为是误报，因为夏天下大雨时出现过报警器湿度过大导致误报的情况（不止一次），而且出现报警时，也第一时间登录集群，发现没啥问题，所以更加坚信应该没问题。但是报警器不会无缘无故响起，于是还是第一时间赶到机房，来到门口的那一刻，还是被吓到了，配电房和主机房的门都因为门禁没电而自动打开了，而且配电柜和UPS柜的输出电压显示都是熄灭的，所有开关都没有亮，感觉是出问题了，机房都没电，除了UPS主机和控制一直在显示之外。然后迅速跑到主机房，一打开，一阵火气扑面而来，还夹杂着一股塑料的味道，被吓到了，不敢进去，害怕里面已经烧坏了。于是赶紧把门打开，散热，但是过了好久，门口还是能够感到一大股热量。后来借了一把手电筒还是进去看了下，所有机柜后面后非常热，机柜门也是非常热，很浓的塑料味，不是烧焦的味道，但是第一次看到这样的场景，还是吓到了。刚开始以为机房烧了，价值不菲的机器废掉了，心里还是很担心的。搞不懂到底是啥问题，刚开始以为空调设备老化，短路，导致机房配电柜或者是箱变烧毁或者跳闸了。但是后来又仔细想了下，如果空调出现故障，服务器一直运行，那么会导致产热一直不会停止，导致温度达到近100度，机房会直接起火烧毁，那么就不会是这样子了。机房目前的状况来看，感觉应该没有损坏太多东西，应该不会有啥大问题。计算维修，也不需要太多费用。由于是周五，而且很晚了，一时间难以解决问题，于是将市电配电柜和UPS输出的所有开关都断开了，等待第二天的处理。</p><p>第二天一大早上来到机房，发现配电柜的三相电指示灯都亮了，我瞬间明白了，昨天的问题可能是机房停电了。这是，主机房的空调维保公司已经到位，并且检查了精密空调的状况，发现没有故障。于是经过确认后，我将UPS电池开关、主输出、旁路输出、主输入开关都打开了，然后把照明、门禁的空气开关也打开了，照明和门禁都正常工作了。于是我把新风、ADU地板也打开了，目前打开的设备一切OK。空调维修师傅确认后，我把空调的空气开关也打开了，空调开始正常工作了。经过这个过程，我大致可以确定，服务器应该也没有故障，因此抱着尝试的态度打开了2个机柜的UPS输出开关，进行供电。到主机房后，发现，机器可以正常工作，于是将所有机柜的UPS输出打开，PDM和服务器风扇和电源模块基本上都正常上电了。OK，说明机房基本没啥问题。于是按照高<a href="#poweron">集群开机流程</a>进行了集群开机操作。经过30分钟的耐心等待，集群基本上正常工作。</p><p>通过排查服务器系统日志、精密空调日志、火灾报警器日志等发现，均是在前一天的下午18:40左右出现的断电，服务器断开时间稍晚一些。<br>这次故障出现真的是听折腾人的，担心了一晚上，没休息好，居然是停电导致的。之后经过确认，确实是<strong>其他大楼</strong>停电能导致的。希望后来的管理员遇到这种情况也可以从容面对了。哈哈。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;常见故障&quot;&gt;&lt;a href=&quot;#常见故障&quot; class=&quot;headerlink&quot; title=&quot;常见故障&quot;&gt;&lt;/a&gt;常见故障&lt;/h3&gt;&lt;h4 id=&quot;用户无法登录到集群&quot;&gt;&lt;a href=&quot;#用户无法登录到集群&quot; class=&quot;headerlink&quot; title=&quot;用户无法登录到集群&quot;&gt;&lt;/a&gt;用户无法登录到集群&lt;/h4&gt;&lt;p&gt;有如下几种原因会导致用户无法登录到集群：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户使用的是校园无线网络，校园无线网络不允许登录到集群。&lt;/li&gt;
&lt;li&gt;用户在校园网外部登录集群，集群IP地址未暴露给外网，校园网外部无法直接访问。&lt;/li&gt;
&lt;li&gt;用户使用校园内部有线网络，无法登录到集群，需要在集群管理服务器添加IP地址许可。&lt;/li&gt;
&lt;li&gt;用户不存在，则无法登录集群。&lt;/li&gt;
&lt;li&gt;用户长期不登录或者已归还集群，账户密码被锁定，则无法登录集群。
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结6-监控与开关机</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%936-%E7%9B%91%E6%8E%A7%E4%B8%8E%E5%BC%80%E5%85%B3%E6%9C%BA.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结6-监控与开关机.html</id>
    <published>2017-12-12T12:20:26.000Z</published>
    <updated>2018-02-24T08:37:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="集群软件环境"><a href="#集群软件环境" class="headerlink" title="集群软件环境"></a>集群软件环境</h3><p>集群目前所有软件都配置在/public/software目录下面，目前配置的有Intel编译器、OpenMPI、Mathlab2009、Matlab2016、lammps、FDTD等，还有很多需要的软件可能还没有安装。</p><p>很多集群的用户都喜欢在自己的目录下面安装计算软件，这就导致了同一个非常占用空间的软件在集群上面有多个副本，<strong>导致集群存储空间被大大浪费掉了。</strong></p><p>因此最好是能够及时统计用户需要使用的软件的信息，并和软件用户一起将软件安装到/public/software目录下面，并告知其他用户直接使用该目录下面的软件即可。</p><p>集群还安装了Torque5作业调度管理系统，目前主节点为node307，后期考虑将B区的所有节点都安装作业调度管理系统，并采用调度系统提交作业。</p><p><strong>总而言之，集群软件尽量安装到/public/software目录下面，并编写一定的说明文档，告知用户使用方法，可以找一些已经有过安装和使用经验的用户进行交流，让他们提供这些资料。</strong><br><a id="more"></a></p><h3 id="集群监控"><a href="#集群监控" class="headerlink" title="集群监控"></a>集群监控</h3><p>集群的监控工作的作用和意义就不再强调了，为了更好掌握集群每个节点的运行情况、资源使用情况，监控是必须的。本人之前在集群的所有节点上面都安装了zabbix2.14监控管理软件，zabbix有个非常大的好处就是报警处理做的比较好，目前主要针对机器网络不通、重启会进行报警，当前情况下设置了Email报警以及短信报警。zabbix监控管理软件的URL地址是^v^，这个保密，只对管理员开放咯。</p><p>由于个人精力有限，没有深入一部配置zabbix的报警规则。另外本人重新编写了一个脚本，该脚本的作用是在白天没过2个小时扫描一下集群所有计算节点的温度，发现CPU温度超过90摄氏度的，则向终端用户发出警告，CPU温度超过95摄氏度的，直接进行关机处理。该脚本所在目录地址是/vpublic01/zhangchi/clustermgmt/temp-monitor/monitor.sh，使用操作系统的crontab进行定时执行。该脚本能够正确获取到温度信息的前提是，刀片的BMC管理模块是好的，而且在BIOS中设置了该刀片BMC模块的IP地址信息，而且网络是连通的。</p><h3 id="集群关机与开机流程"><a href="#集群关机与开机流程" class="headerlink" title="集群关机与开机流程"></a>集群关机与开机流程</h3><p>一般而言，集群不需要进行关机处理，但是实验室暑假、春节期间由于长期没有人照看，需要进行关机处理，特别是暑假温度高、用户如果计算量上去，加上空调质量不是很好，很可能会导致难以预料的后果。</p><p><span id="poweroff">集群关机流程如下：</span></p><ul><li>关闭所有的计算节点和GPU节点，编号是node1-node306，node327-node348</li><li>关闭集群管理节点，node307-node308</li><li>关闭对象存储服务器，node311-node326节点</li><li>关闭元数据服务器，node309-node310节点</li></ul><p><span id="poweron">集群开机流程如下：</span></p><ul><li>启动元数据服务器，node309-node310节点</li><li>等待5-10分钟</li><li>启动对象存储服务器，node311-node326节点</li><li>等待10-15分钟</li><li>启动集群管理节点，node307-node308节点</li><li>查看node307和node308上面的/public是否正常挂载，如果正常则执行下面的步骤</li><li>如果不正常，则按照关机流程，重启元数据和对象存储服务器</li><li>启动所有计算节点，node1-node306，node327-node348</li></ul><h3 id="集群断电与通电流程"><a href="#集群断电与通电流程" class="headerlink" title="集群断电与通电流程"></a>集群断电与通电流程</h3><p>集群如果关机只是暂时关机，而不是断电，那么执行普通的关机的命令关闭所有的服务器即可。<br>如果由于实验室长期无人看管，导致需要停止集群，那么需要进行断电处理。</p><h4 id="机房电路简介"><a href="#机房电路简介" class="headerlink" title="机房电路简介"></a>机房电路简介</h4><p><strong>进行带电设备操作之前，请务必要主要安全，务必注意安全，务必注意安全！</strong><br><strong>请阅读资料确保安全后操作，请阅读资料确保安全后操作，请阅读资料确保安全后操作！</strong><br><em>管理员曾经因为不谨慎就被电过，幸好没出事，请一定要谨慎！</em></p><p>本部分内容仅供参考，不保准准确性，请勿直接接触供电设备。进行有关电气设备操作之前，请务必与相关负责老师联系，做好防触电准备，请务必谨记，安全第一。</p><p>下面介绍主机房电源的接入情况。<br>主机房有14个机柜，一般情况下，2个机柜共用一个PDM，少数机柜单独使用一个PDM，因此主机房有7个PDM，这些PDM上面有一个开关，关闭开关，就可以将其负责供电的机柜的断电。</p><p>PDM主要分配在机柜2、机柜3、机柜6、机柜7、机柜9、机柜11、机柜13的最下面。</p><p>这些PDM由配电房的UPS电源模块来进行供电。在配电房的配电柜里面有对应的开关，可以直接断掉这些PDM的供电。</p><p>另外，主机房还有2个大型工业空调用于集群的制冷工作，这两个空调不是通过UPS进行供电的，走的是市电线路。</p><p>主机房还有地板PEU，即从地上往上进行吹风的风扇，用于辅助空气流动，帮助集群进行扇热的，也是接的市电线路。</p><p>主机房还有一个设备叫做新风，简要理解为一个电风扇，主要负责集群内部的空气和室外的空气交换的，这个设备也是走的市电线路。</p><p>主机房还要照明设备、消防报警器、门禁设备等，这些也都是走的市电通道。</p><p>总而言之，除了集群的机柜是使用UPS进行供电外，其他的所有设备都是市电进行供电。</p><h4 id="配电房设备介绍"><a href="#配电房设备介绍" class="headerlink" title="配电房设备介绍"></a>配电房设备介绍</h4><p>配电房主要有UPS设备和配电柜、空调。<br>简要介绍一个各设备的摆布，走进配电房。</p><p>最西边的两个乳白色的大箱子里面安装的是UPS铅酸电池组，平时巡检的时候，需要稍微闻一下是否存在异味，由于设备老化，电池已经处于危险期了。</p><p>背面有4个柜子并列，左侧两个是配电柜，右侧两个是UPS的主机和控制箱。</p><p>左侧第一个配电柜是市电配电柜，上面有一个市电总开关，控制整个机房的市电接入，一般情况不要动此开关。下面还有一排空气开关，这些开关的最左侧是UPS旁路开关，从左往右依次是配电室5P柜式空调开关、备用开关、主机房新风、ADU地板、主机房照明、电源室照明、主机房插座、电源室插座、电源室空调（壁挂式关空调，已经没有使用）、主机房精密空调一、主机房精密空调二。这些开关直接控制了相应设备的市电输入，如果需要断电，可以直接从这里进行控制。</p><p>左侧第二个配电柜是UPS输出配电柜，上面有一个UPS输出总开关，该开关控制着集群所有机柜的供电，而且该供电是通过UPS进行输出的，所以一般情况不要断电。</p><p>右侧从左往右第一个机柜是UPS的控制和监控柜，显示了一些UPS的运行信息以及UPS的几个主要开关。</p><p>右侧最后一个机柜是UPS的控制电路柜，这里需要注意的是过滤网要定期的清洗。</p><h4 id="集群断电流程"><a href="#集群断电流程" class="headerlink" title="集群断电流程"></a>集群断电流程</h4><ul><li>集群关机，按照<a href="#poweroff">集群关机流程</a>部分描述的方法，将集群所有服务器按照正确顺序进行关机处理。</li><li>关闭机柜2、机柜3、机柜6、机柜7、机柜9、机柜11、机柜13最下面的PDM的开关，从所有机柜的后面查看是不是所有服务器电源模块的指示灯都断电了。最明显的断电特征是，机房的声音变得很小了，基本上没有了，因为刀箱的散热模块断电了。</li><li>关闭主机房两台精密空调，精密空调的右侧有个显示面板，显示面板的左侧有一个可以旋转开关，将开关旋转至Off即可关闭空调。在集群关机之后，可以关闭精密空调。</li><li>到配电房左侧市电柜中关闭精密空调开关、主机房新风、主机房ADU地板的开关。</li><li>回到主机房检查服务器、空调、地板、以及换气设备是否都已经停止工作了，确认好之后，关好主机房的门。</li></ul><p>经过以上步骤，主机房就基本上进行了断电，即服务器设备和空调设备都进行了断电，其他东西可以不用断电，因为功率比较小。</p><p>配电房由于存在UPS电源设备以及配电柜，因此配电房里面的设备不能直接断电，而且配电房空调也不能断电。</p><p>最后总结一下断电的流程：集群服务器设备断电-&gt;集群精密空调断电-&gt;其他辅助设备断电。</p><h4 id="集群通电流程"><a href="#集群通电流程" class="headerlink" title="集群通电流程"></a>集群通电流程</h4><ul><li>到配电房左侧市电柜中打开精密空调的开关。</li><li>进入主机房，打开机柜2、机柜3、机柜6、机柜7、机柜9、机柜11、机柜13最下面的PDM的开关，从所有机柜的后面查看是不是所有服务器电源模块的指示灯都通电了。需要依次检查每个刀箱背后的4个电源模块是否正常运行，即，电源模块的指示灯是否全部是绿色，如果是绿色，表示正常。若有一个是黄色，则很有可能导致刀箱交换模块无法正常被供电，导致无法工作，这种情况导致的后果就是整个刀箱无法连接，也无法相互通信，所以通电后，第一件事请就是要保证所有刀箱的所有电源模块正常工作，如果无法正常工作，请参考<a href="#power_module_error">刀箱电源模块故障处理</a>进行处理。</li><li>中间还有一点需要说明的就是，每个刀片服务器的前置面板的上方有一个白色的纸质标签，标签上面写着nodeXXX，即节点的主机名或者编号信息，这个标签是通过刀箱散热模块产生的向后的空气流动形成的吸引力吸在前置面板上面的，如果集群的刀箱断电后，散热模块停止运行，将会导致有些不是很稳的标签飘落下来，因此，集群断电之后，如果看到了标签落地，请将标签集中捡起来，并放到一个没有风的地方，最好用大一点的螺丝压住，然后等集群通电的时候，散热模块运行，标签可以稳定吸住之后，再将落下的标签贴上。</li><li>开启主机房两台精密空调，精密空调的右侧有个显示面板，显示面板的左侧有一个可以旋转开关，将开关旋转至On即可启动空调。集群开机之前，请首先打开精密空调。</li><li>到配电房左侧市电柜中打开主机房新风、主机房ADU地板的开关。</li><li>集群开机，按照<a href="#poweron">集群开机流程</a>部分描述的方法，将集群所有服务器按照正确顺序进行开机处理。集群开机过程中，请务必注意服务器</li></ul><p>经过以上步骤，集群基本上已经通电并且开机了。</p><p>最后总结一下通电的流程：集群机柜PDM通电-&gt;集群空调通电并开机-&gt;其他辅助设备通电运行-&gt;集群服务器通电并开机。</p><h4 id="紧急情况处理"><a href="#紧急情况处理" class="headerlink" title="紧急情况处理"></a>紧急情况处理</h4><h5 id="停电"><a href="#停电" class="headerlink" title="停电"></a>停电</h5><p>由于机房的市电是从学校其他大楼接入过来的，所以可能存在这样的问题，E5楼停电了，校内会通知，但是机房基本上不受影响。其他大楼停电了，但是学校没有正常通知到E5楼，导致集群以外断电。</p><p>集群具有UPS电源，理论上，最多可支持集群非IT设备运行30分钟左右时间。注意，空调，照明灯非IT设备的电源都未接入到UPS中，所以一旦市电断开，只有服务器，网络设备会正常运行，所以这个时候，机房的温度会陡然升高，可想而知，几百台机器疯狂散热，就算没有负载，机房温度也会升很高，如果是夏天，那可能直接导致机房起火了。</p><p>所以一旦出现断电，温度过高，机房的火灾报警器会响起来，这时候，应该尽早进行关机处理。有时候来不及正常关机，可以直接跑到配电房里面，将UPS电源输出柜的电源开关全部断开，这样机器会全部断电，速度上更快，更早防止损失。</p><p>目前这个问题还没有很好的解决办法，目前能够做的最好方式就是，及时关注学校的停电通知，及早做好相关的处理。另外，还有一个办法，就是需要添加特殊设备，能够实现市电断开报警或直接将UPS的的电池输出断开，就是把UPS当作一个大的配电箱来用，市电断开，集群立即全部都停电。这样就不会导致空调开着，集群还在运行，温度过高的情况出现了。</p><p>近期看到阿里巴巴平台上面有一种断电告警设备，能够检测机房等市电断电情况，一旦断电，即将通过短信，电话等报警的方式通知接收人，这样可以在第一时间知道机房停电的事情，从而进行及时处理。</p><p>个人想法，UPS电源的确是一个非常好的方式，不仅可以帮助机房IT设备稳定电压，而且还能在停电时采取正确关机方式，保证用户数据的安全和应用的数据一致性问题，整体来说是非常有效一种措施和设施，但是，如果停电后，UPS工作，但是管理员没有收到通知，最后电池耗尽了，导致关机，这种情况，有没有UPS都一样，甚至有了UPS还会出事情，严重情况会导致火灾。如果停电后，UPS供电，空调断电，机房继续工作，温度过高，特别是在夏天，机器工作20分钟，足以将机房温度升到70-100度直接导致火灾，所以机房一定要有断电通知的机制，否则，UPS不仅是摆设，还是危害。硬件设施重要，但是管理机制更加重要。</p><h3 id="集群管理软件和技能"><a href="#集群管理软件和技能" class="headerlink" title="集群管理软件和技能"></a>集群管理软件和技能</h3><h4 id="管理软件推荐"><a href="#管理软件推荐" class="headerlink" title="管理软件推荐"></a>管理软件推荐</h4><p>pssh<br>shell<br>Linux<br>LSI RAID</p><h4 id="需要的技能"><a href="#需要的技能" class="headerlink" title="需要的技能"></a>需要的技能</h4><p>Linux OS基本架构和原理的理解<br>Linux Shell编程，最好是熟练掌握<br>计算机网络知识</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;集群软件环境&quot;&gt;&lt;a href=&quot;#集群软件环境&quot; class=&quot;headerlink&quot; title=&quot;集群软件环境&quot;&gt;&lt;/a&gt;集群软件环境&lt;/h3&gt;&lt;p&gt;集群目前所有软件都配置在/public/software目录下面，目前配置的有Intel编译器、OpenMPI、Mathlab2009、Matlab2016、lammps、FDTD等，还有很多需要的软件可能还没有安装。&lt;/p&gt;
&lt;p&gt;很多集群的用户都喜欢在自己的目录下面安装计算软件，这就导致了同一个非常占用空间的软件在集群上面有多个副本，&lt;strong&gt;导致集群存储空间被大大浪费掉了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因此最好是能够及时统计用户需要使用的软件的信息，并和软件用户一起将软件安装到/public/software目录下面，并告知其他用户直接使用该目录下面的软件即可。&lt;/p&gt;
&lt;p&gt;集群还安装了Torque5作业调度管理系统，目前主节点为node307，后期考虑将B区的所有节点都安装作业调度管理系统，并采用调度系统提交作业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总而言之，集群软件尽量安装到/public/software目录下面，并编写一定的说明文档，告知用户使用方法，可以找一些已经有过安装和使用经验的用户进行交流，让他们提供这些资料。&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结5-Lustre维护</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%935-Lustre%E7%BB%B4%E6%8A%A4.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结5-Lustre维护.html</id>
    <published>2017-12-12T12:20:25.000Z</published>
    <updated>2018-02-24T08:37:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="Lustre维护"><a href="#Lustre维护" class="headerlink" title="Lustre维护"></a>Lustre维护</h3><p><span id="Lustre"></span><br>科研教育网格集群采用了高性能分布式计算文件系统Lustre，Top500超级计算机中有超过50%都是用了该文件系统。该文件系统的命运特别坎坷，先后转手了好几家公司，现在是Intel公司所属。Lustre原先是一套开源的产品，最初是美国能源部提出开发的，后来成立CFS公司，2007年转卖给Sun公司，2010年Sun公司被Oracle公司收购，2010年又卖给了Whamcloud公司，最后于2012年被Intel收购。<br><a id="more"></a><br>Lustre是一种基于对象存储的分布式文件系统，主要包含MDS(Metadata Server)和OSS(Object Storage Server)，实验室集群的node309和node310是MDS,311-323是OSS，其中309和310互为备份，310提供了/public目录的元数据，并备份node309中的元数据，309之前提供了/home目录，后来因为一些故障，导致309没有再次提供服务了，目前309仅仅作为/public元数据的一个备份，但是309和310必须要同时启动才能正常工作。之前311-318提供/home目录的数据文件存储，319-323提供/public目录的数据文件存储，但是由于之前出现过阵列故障，所以目前只有319-323提供了数据存储服务。总结，node309和node310提供元数据存储服务，node329-323提供数据存储服务，这几个服务器不能随意允许个人登录，也不允许任意关机，否则导致整个集群无法正常使用。</p><p>所有计算节点都预先安装了Lustre客户端软件，使用Lustre文件系统，需要保证内核中加载了Lustre文件系统网络模块，配置文件在/etc/modprobe.d/lustre.conf，其中的内容如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=&quot;o2ib0(ib0),tcp0(eth0)&quot;</span><br></pre></td></tr></table></figure></p><p>Lustre文件系统的网络模块，其中配置了两种网络通道，第一种是Infiniband网络通道，另外一种是以太网通道。如果节点的InfiniBand网络通道出现问题，可以使用以太网，后面会介绍Infiniband网络出现问题时使用以太网挂载Lustre文件系统的方法。</p><p>Lustre文件系统的挂载脚本是/etc/client_mount.local内部，内容如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sleep 20</span><br><span class="line">mount -t lustre &quot;inode310@o2ib0:inode309@o2ib0:node310@tcp0:node309@tcp0:/p100fs02&quot; /public</span><br><span class="line">#mount -t lustre &quot;inode309@o2ib0:inode310@o2ib0:node309@tcp0:node310@tcp0:/p100fs01&quot; /home</span><br><span class="line">mount --rbind /public/home /home</span><br></pre></td></tr></table></figure></p><p>由于Lustre网络模块或者Infiniband网络模块还没有初始化，因此需要等待20秒之后再次挂载，有些节点可能由于InfiniBand网络加载速度更慢，因此需要等待更长时间才能执行挂载工作。由于集群之前的/home目录出现问题，因此现在将/public/home目录挂载为/home，也就是说/home目录实际上也是由/public目录存储的。</p><p>如果某个节点无法挂载Lustre文件系统，登录到节点，首先检查节点的Infiniband网络是否正常，执行如下命令中的一种，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node236 ~]# ibstat</span><br><span class="line">CA &apos;mlx4_0&apos;</span><br><span class="line">CA type: MT4099</span><br><span class="line">Number of ports: 1</span><br><span class="line">Firmware version: 2.30.3000</span><br><span class="line">Hardware version: 0</span><br><span class="line">Node GUID: 0x46d2c92000003890</span><br><span class="line">System image GUID: 0x46d2c92000003893</span><br><span class="line">Port 1:</span><br><span class="line">State: Active</span><br><span class="line">Physical state: LinkUp</span><br><span class="line">Rate: 56</span><br><span class="line">Base lid: 151</span><br><span class="line">LMC: 0</span><br><span class="line">SM lid: 317</span><br><span class="line">Capability mask: 0x02514868</span><br><span class="line">Port GUID: 0x46d2c92000003891</span><br><span class="line">Link layer: InfiniBand</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node236 ~]# ibstatus</span><br><span class="line">Infiniband device &apos;mlx4_0&apos; port 1 status:</span><br><span class="line">default gid: fe80:0000:0000:0000:46d2:c920:0000:3891</span><br><span class="line">base lid: 0x97</span><br><span class="line">sm lid: 0x13d</span><br><span class="line">state: 4: ACTIVE</span><br><span class="line">phys state: 5: LinkUp</span><br><span class="line">rate: 56 Gb/sec (4X FDR)</span><br><span class="line">link_layer: InfiniBand</span><br></pre></td></tr></table></figure><p>如果state属性显示ACTIVE，那么表示Infiniband网络是好的，可以手动进行挂载，如果挂载还是出现问题，很有可能是Lustre文件系统模块没有加载完成，另外还需要执行uname-a命令查看内核的版本是否被用户升级或者更换了。如果内核不是2.6.32-220，那么不能使用Lustre文件系统。因为Lustre客户端没有在其他内核版本进行安装。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node236 ~]# uname -a</span><br><span class="line">Linux node236 2.6.32-220.el6.x86_64 #1 SMP Wed Nov 9 08:03:13 EST 2011 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure></p><p>如果Infiniband网络的Physical State属性显示是PortConfigurationTraining同时State是Down，这种情况一般是Infiniband网络的子网管理服务器没有启动的原因，可以使用执行如下命令重启Infiniband网络服务，然后再查看Infiniband网络是否正常。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service openibd restart</span><br><span class="line">service opensmd restart</span><br></pre></td></tr></table></figure></p><p>如果phys state是Down，那么很有可能机器的Infiniband没有安装或者存在问题，那么试试重新安装驱动，如果还是没有效果，那么很有可能Infiniband线缆已经损坏，可以从刀箱后面拔掉坏掉的线缆，插上其他的线缆试试，一般线缆是好的，Infiniband网络的网口会有两个灯亮着，如果灯不亮，一般有可能是驱动没有安装，也有可能是线缆坏掉了，或者是Infiniband线缆坏掉了。</p><p>备注：用户重新安装操作系统后，无法使用Lustre文件系统。</p><h4 id="使用以太网挂载Lustre文件系统"><a href="#使用以太网挂载Lustre文件系统" class="headerlink" title="使用以太网挂载Lustre文件系统"></a>使用以太网挂载Lustre文件系统</h4><p>如果确定Infiniband网络线缆有问题，那么可以不用Infiniband网络，而是直接使用以太网络来进行Lustre通信。方法如下，将/etc/modprobe.d/lustre.conf修改为如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=&quot;tcp0(eth0)&quot;</span><br></pre></td></tr></table></figure><p>然后将挂载脚本/etc/client_mount.local文件修改为如下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sleep 20</span><br><span class="line">mount -t lustre &quot;node310@tcp0:node309@tcp0:/p100fs02&quot; /public</span><br><span class="line">mount --rbind /public/home /home</span><br></pre></td></tr></table></figure></p><p>最后重新启动计算节点，不出意外情况，应该可以正常使用Lustre文件系统了，如果还是有问题，只有两种可能，一种是Lustre模块出现问题，需要重新配置，另外一种可能是，内核中没有安装Lustre文件系统。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;Lustre维护&quot;&gt;&lt;a href=&quot;#Lustre维护&quot; class=&quot;headerlink&quot; title=&quot;Lustre维护&quot;&gt;&lt;/a&gt;Lustre维护&lt;/h3&gt;&lt;p&gt;&lt;span id=&quot;Lustre&quot;&gt;&lt;/span&gt;&lt;br&gt;科研教育网格集群采用了高性能分布式计算文件系统Lustre，Top500超级计算机中有超过50%都是用了该文件系统。该文件系统的命运特别坎坷，先后转手了好几家公司，现在是Intel公司所属。Lustre原先是一套开源的产品，最初是美国能源部提出开发的，后来成立CFS公司，2007年转卖给Sun公司，2010年Sun公司被Oracle公司收购，2010年又卖给了Whamcloud公司，最后于2012年被Intel收购。&lt;br&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结4-网络访问控制</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%934-%E7%BD%91%E7%BB%9C%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结4-网络访问控制.html</id>
    <published>2017-12-12T12:20:24.000Z</published>
    <updated>2018-02-24T08:38:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="网络访问控制"><a href="#网络访问控制" class="headerlink" title="网络访问控制"></a>网络访问控制</h3><p>集群主要通过安全网关硬件和iptables进行访问控制。</p><p>安全网关配置了允许登录进入集群的IP类型以及开放的端口信息。<br>具体的IP地址限制由iptables来进行限制。</p><h4 id="允许某一IP地址访问集群"><a href="#允许某一IP地址访问集群" class="headerlink" title="允许某一IP地址访问集群"></a>允许某一IP地址访问集群</h4><p>编辑/etc/sysconfig/iptables文件，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A INPUT -s 99.99.1.1 -p tcp -m tcp -j ACCEPT</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>然后执行如下命令，重启iptables服务，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables restart</span><br></pre></td></tr></table></figure></p><p>即可允许99.99.1.1这个IP地址登录到集群。</p><h4 id="允许网段的IP地址访问集群"><a href="#允许网段的IP地址访问集群" class="headerlink" title="允许网段的IP地址访问集群"></a>允许网段的IP地址访问集群</h4><p>编辑/etc/sysconfig/iptables文件，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A INPUT -s 99.99.1.1/24 -p tcp -m tcp -j ACCEPT</span><br></pre></td></tr></table></figure></p><p>然后执行如下命令，重启iptables服务，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables restart</span><br></pre></td></tr></table></figure></p><p>即可允许99.99.1.1-99.99.1.255这个网段内部的所有IP地址登录集群。</p><h4 id="允许所有节点访问特定的IP地址"><a href="#允许所有节点访问特定的IP地址" class="headerlink" title="允许所有节点访问特定的IP地址"></a>允许所有节点访问特定的IP地址</h4><p>编辑/etc/sysconfig/iptables文件，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -d 202.114.0.242/32 -o eth1 -j MASQUERADE</span><br></pre></td></tr></table></figure></p><p>然后执行如下命令，重启iptables服务，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables restart</span><br></pre></td></tr></table></figure></p><p>执行完命令后，所有集群都可以访问202.114.0.242这个IP地址所在的服务器。<br>此时，需要在需要访问202.114.0.242这个IP的计算节点上面执行下面的操作。<br>首先登录到需要访问外网的节点，执行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">route</span><br></pre></td></tr></table></figure></p><p>查看结果中是否有以下记录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">default         node307         0.0.0.0         UG    0      0        0 eth0</span><br></pre></td></tr></table></figure></p><p>如果存在上述内容，则直接可以连接外网，如果ping外网还是不同，一般有可能是DNS信息没有填写正确，可以编辑/etc/resolv.conf文件，在该文件头部加入一下信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nameserver 202.114.0.242</span><br><span class="line">nameserver 202.112.0.131</span><br></pre></td></tr></table></figure></p><p>保存后，就可以正常访问外部网络了。</p><h4 id="允许某一节点访问特定的IP地址"><a href="#允许某一节点访问特定的IP地址" class="headerlink" title="允许某一节点访问特定的IP地址"></a>允许某一节点访问特定的IP地址</h4><p>编辑/etc/sysconfig/iptables文件，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -s 11.11.0.1/32 -d 192.30.252.128/32 -o eth1 -j MASQUERADE</span><br></pre></td></tr></table></figure></p><p>然后执行如下命令，重启iptables服务，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables restart</span><br></pre></td></tr></table></figure></p><p>执行完命令后，允许node1（IP地址为11.11.0.1）访问github网站（IP地址为192.30.252.128）。</p><h4 id="允许某一节点访问所有网站"><a href="#允许某一节点访问所有网站" class="headerlink" title="允许某一节点访问所有网站"></a>允许某一节点访问所有网站</h4><p>编辑/etc/sysconfig/iptables文件，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-A POSTROUTING -s 11.11.0.1/32 -o eth1 -j MASQUERADE</span><br></pre></td></tr></table></figure></p><p>然后执行如下命令，重启iptables服务，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service iptables restart</span><br></pre></td></tr></table></figure></p><p>执行完命令后，node1（IP地址为11.11.0.1）具有全部外网访问权限。</p><h4 id="配置端口转发规则"><a href="#配置端口转发规则" class="headerlink" title="配置端口转发规则"></a>配置端口转发规则</h4><p>允许集群外部访问内部节点的某一或多个端口，实际原理是SNAT和DNAT配置。<br>例如说。我希望访问集群node1(IP地址为11.11.0.1)的http服务(端口号为80)，此时管理节点的IP为99.99.1.1(虚构的一个外网IP地址)，则配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-A PREROUTING --dst 99.99.1.1 -p tcp --dport 8888 -j DNAT --to-destination 11.11.0.1:80</span><br><span class="line">-A POSTROUTING -d 11.11.0.1 -p tcp -m tcp --dport 80 -j SNAT --to-source 99.99.1.1</span><br></pre></td></tr></table></figure><p>以上语句的意思是，将访问99.99.1.1:8888的数据包都转发到11.11.0.1:80，</p><h4 id="集群DNS服务器信息"><a href="#集群DNS服务器信息" class="headerlink" title="集群DNS服务器信息"></a>集群DNS服务器信息</h4><p>以下是校内DNS服务器的信息，可以编辑/etc/resolv.conf文件进行设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nameserver 202.114.0.242</span><br><span class="line">nameserver 202.112.0.131</span><br></pre></td></tr></table></figure></p><h3 id="网络故障处理"><a href="#网络故障处理" class="headerlink" title="网络故障处理"></a>网络故障处理</h3><h4 id="Infiniband网络故障"><a href="#Infiniband网络故障" class="headerlink" title="Infiniband网络故障"></a>Infiniband网络故障</h4><p><span id="Infiniband"></span><br><a href="http://www.mellanox.com/page/software_overview_ib" target="_blank" rel="noopener">Mellanox Infiniband驱动下载地址</a><br>上面是Infiniband驱动的官方下载地址，根据系统版本进行下载。</p><p>Infiniband网络最多的故障就是状态不是Active，导致状态一直是PortConfigurationTraining，这个时候可以稍等2分钟，如果状态一直是这样，可以考虑重启一下openibd服务，如果重启后还是不行，可能是子网管理服务没有启动，这种情况下，应该停止集群所有节点的子网管理服务，opensmd服务，<strong>请务必记住，一个子网内部只能启动一个opensmd服务，启动多个会导致Infiniband网络出现问题，接着导致文件系统无法使用，直接导致集群无法正常计算</strong>，因此需要登录到运行opensmd服务的节点，<span id="ibpro">本集群默认在node310上开启opensmd服务，</span>命令如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service opensmd restart</span><br></pre></td></tr></table></figure></p><p>如果提示服务不存在，那么问题的原因就是网络驱动没有正常安装，或者是出现了故障，这个时候可以采用重新安装驱动的方式来进行修复。</p><p>Infiniband网络配置文件为/etc/sysconfig/network-scripts/ifcfg-ib0</p><h4 id="以太网故障"><a href="#以太网故障" class="headerlink" title="以太网故障"></a>以太网故障</h4><p>以太网故障一般比较少，常见的故障可能是刀片机箱的交换模块断电了。交换模块断电的一个明显的特征是网络指示灯一个都不亮，这种情况在集群断电后首次启动时经常遇到。</p><p><span id="power_module_error"></span><br>造成刀箱的网络交换模块无法正常工作的原因是，刀箱的电源模块在长时间断电之后没有正常启动或者由于电源模块功率过高而风扇转速太低导致电源过热保护，特点是指示灯为黄色，如果一个刀箱有2个电源模块亮黄灯，那么很有可能导致刀箱的交换模块无法正常启动。这个时候可以尝试一下办法来解决这个问题。</p><p>将亮黄灯电源模块的电源线拔下，将电源模块拔下，等待1-2分钟之后，黄灯熄灭，然后把电源模块重新插入，并插上电源模块的电源线，如果一会之后，电源模块指示灯亮绿色，表示该电源模块可以正常工作。将所有的电源模块都进行这样的处理。直到所有的电源模块指示灯都是绿色的。</p><p>然后将刀箱内部的所有刀片节点进行关机处理，关机后，在刀箱背面的左下角的有一个黑色的开关，这个开关是整个刀箱的电源开关，它控制刀箱所有模块的供电。找到后关闭它，等待5分钟后，重启打开该开关，如果不出意外情况，也就是所有的电源模块指示灯为绿色的情况下，一会刀箱的电源交换模块的网络指示灯就会正常亮起来了。如果还是不行，有可能就是交换模块出现故障了。</p><p><strong>如果不能正常解决，直接打电话给官方客服进行报修啦。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;网络访问控制&quot;&gt;&lt;a href=&quot;#网络访问控制&quot; class=&quot;headerlink&quot; title=&quot;网络访问控制&quot;&gt;&lt;/a&gt;网络访问控制&lt;/h3&gt;&lt;p&gt;集群主要通过安全网关硬件和iptables进行访问控制。&lt;/p&gt;
&lt;p&gt;安全网关配置了允许登录进入集群的IP类型以及开放的端口信息。&lt;br&gt;具体的IP地址限制由iptables来进行限制。&lt;/p&gt;
&lt;h4 id=&quot;允许某一IP地址访问集群&quot;&gt;&lt;a href=&quot;#允许某一IP地址访问集群&quot; class=&quot;headerlink&quot; title=&quot;允许某一IP地址访问集群&quot;&gt;&lt;/a&gt;允许某一IP地址访问集群&lt;/h4&gt;&lt;p&gt;编辑/etc/sysconfig/iptables文件，&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;-A INPUT -s 99.99.1.1 -p tcp -m tcp -j ACCEPT&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结3-用户与权限管理</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%933-%E7%94%A8%E6%88%B7%E4%B8%8E%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结3-用户与权限管理.html</id>
    <published>2017-12-12T12:20:23.000Z</published>
    <updated>2018-02-24T08:38:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="用户与权限管理"><a href="#用户与权限管理" class="headerlink" title="用户与权限管理"></a>用户与权限管理</h3><h4 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h4><h5 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h5><p>集群默认安装了clusconf软件，厂家的马少杰经理之前写的，主要用于集群的用户添加、开关机管理、批量执行命令、批量同步命令等。还是挺不错的，但是速度比较慢。特别是同步文件的时候，如果有几个节点存在问题，可能就会卡在那里半天，而且同步过程是顺序执行的，导致速度很慢。说白了，实际上是由于在进行远程执行命令的时候，没有考虑到失败的情况，也就是说没有添加失败时延，导致操作失败的时延差不多为30s，所以如果一批节点中，有几个节点拓机或者负载太高，会导致整个操作流程延长，所以，文件同步这一点来说，还是不太灵活。而pssh这个软件就比较好，如果连接失败，马上就返回，并且所有的节点的同步操作时并行执行的，所以速度非常快。因此我们结合了clusconf的方便和pssh的速度，来进行用户的添加。<br><a id="more"></a><br>用户的添加实际上就是在管理节点添加了某一个用户，用户名存储在/etc/passwd文件，用户组信息存储在/etc/group文件，用户密码存储在/etc/shadown文件里面。实际上，添加集群用户的过程就是将这些文件复制到希望用户登录的节点上面。聪明的你应该很快就会发现，这个方式有几个弊端。</p><ul><li>每次添加用户，都需要进行文件的同步。</li><li>一旦添加新用户，原有用户也可以登录新用户分配的节点。</li><li>某些节点更换操作系统后，直接复制这些文件会导致该节点不可用。</li><li>某个用户在管理节点或者部分节点更新密码，其他节点的密码也需要同步更新。</li><li>其他未提及的弊端。</li></ul><p>其实有不少集群统一用户管理的方式。比如说NIS，LDAP的形式都可以很好的解决这种问题。时间关系和经历有限，所以没有部署这样的系统。</p><p>好吧，感觉有点啰嗦了。切中正题，如何添加用户。有如下两个步骤：</p><ul><li>在管理节点添加用户。</li><li>同步文件到计算节点。</li></ul><h5 id="管理节点用户添加"><a href="#管理节点用户添加" class="headerlink" title="管理节点用户添加"></a>管理节点用户添加</h5><p>集群的管理节点是node307和node308，Lustre文件系统的MDS服务器是node309和node310，NFS文件系统所在服务器是node324，所以不管用户需要使用多少节点，必须在这几个节点添加同一个用户，而且ID也必须是一样的，这样用户才可以在计算节点访问分布式文件系统和NFS文件系统。具体添加命令如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clusconf -p node -n `seq 307 310` 324 -ua tom</span><br><span class="line">#假设用户名是tom</span><br></pre></td></tr></table></figure></p><h5 id="同步用户信息文件"><a href="#同步用户信息文件" class="headerlink" title="同步用户信息文件"></a>同步用户信息文件</h5><p>使用pssh软件来同步用户信息文件，主要是/etc/passwd和/etc/shadown文件。<br>前提条件是管理节点安装了基于Python的PSSH软件。执行如下命令同步这几个文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pscp -h node-list /etc/passwd /etc/</span><br><span class="line">pscp -h node-list /etc/shadow /etc/</span><br><span class="line">pscp -h node-list /etc/group /etc/</span><br></pre></td></tr></table></figure></p><p>简要介绍一下pscp命令的使用吧。<br>-h选项后面跟着的就是包含了计算节点或者分配给用户的节点主机名或者IP地址的文件。文件的内容可能是如下形式。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure></p><p>也可以是IP地址的形式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.17.0.1</span><br><span class="line">172.17.0.2</span><br><span class="line">172.17.0.3</span><br></pre></td></tr></table></figure></p><p>当然，也可以是主机和IP地址混合的形式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node1</span><br><span class="line">172.17.0.2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure></p><p>根据具体情况来定吧。我一般是使用一个脚本来生成这样的主机名文件。参考代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># 生成节点列表，根绝给定的起止号以及保存的文件名。</span><br><span class="line"># $1起始编号</span><br><span class="line"># $2终止编号</span><br><span class="line"># $3保存文件名</span><br><span class="line"></span><br><span class="line">start=0;</span><br><span class="line">end=1;</span><br><span class="line">if [ $1 ]</span><br><span class="line">then</span><br><span class="line">start=$1;</span><br><span class="line">fi;</span><br><span class="line"></span><br><span class="line">if [ $2 ]</span><br><span class="line">then</span><br><span class="line">end=$2;</span><br><span class="line">fi;</span><br><span class="line"></span><br><span class="line">filename=$start-$end-node-list;</span><br><span class="line">if [ $3 ]</span><br><span class="line">then</span><br><span class="line">filename=$3;</span><br><span class="line">fi;</span><br><span class="line"></span><br><span class="line">for i in `seq $start $end`</span><br><span class="line">do</span><br><span class="line">echo node$i &gt;&gt; $filename;</span><br><span class="line">done;</span><br><span class="line"></span><br><span class="line">echo &quot;nodelist from $start to $end is saved in $filename&quot;;</span><br></pre></td></tr></table></figure></p><p>pscp后面跟着两个文件或者目录，第一个是源目录或者源文件，第二个是目标目录和目标文件。<br>pscp命令的help信息如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pscp  [-vAr]  [-h  hosts_file]  [-H [user@]host[:port]] [-l user] [-p par] [-o outdir] [-e errdir] [-t timeout] [-O options] [-x args] [-X arg] local remote</span><br></pre></td></tr></table></figure></p><p>默认情况下，这个同步的过程是非常快的，基本上是1-2s就可以搞定了。</p><h5 id="一步到位的添加方法"><a href="#一步到位的添加方法" class="headerlink" title="一步到位的添加方法"></a>一步到位的添加方法</h5><p>如果分配给用户的节点不是很多的话， 使用上面的方式可能比较麻烦，这个时候，可使用集群默认的clusconf软件，可以用于用户的管理工作，常用的命令如下，例如说来了一个新的用户tom，需要给tom分配几个节点，比如说是100，101，105，106，107，下面简单介绍使用clusconf命令在以上节点添加用户tom。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clusconf -p node -n `seq 307 310` 100 101 `seq 105 107` -ua tom</span><br></pre></td></tr></table></figure></p><p>-p node是指节点主机名的前缀，例如说node1的前缀是node，这个是固定写法。<br>-n 选项指定要进行操作的节点编号，无论用户分配哪些节点给用户，以下节点必须包含在里面，307和308节点是集群的入口节点，只有这两个节点才能登陆到集群，309和310是Lustre文件系统的元数据#服务器，如果没有在这两个节点添加用户，那么用户将无法登录到集群使用文件系统，很可能导致卡住。因此在进行添加用户时，一定要添加<code>seq 307 310</code>这句。其他节点的编号，可以使用<code>seq startindex endindex</code>方法进行生成，前提是节点编号连续。当节点编号不连续时，可以依次输入节点编号，空格分开。-ua tom的意思是添加用户tom，ua即useradd的意思</p><p>由于目前仅仅使用clusconf的用户添加功能，所以只能介绍到这里，下面附上一个clusconf的使用教程，需要了解可以详细阅读。<a href="http://7xsnoh.com1.z0.glb.clouddn.com//clustermgmt/pdf/clusconf-1.4%E7%94%A8%E6%88%B7%E6%89%8B%E5%86%8C.pdf" target="_blank" rel="noopener">clusconf1.4使用手册</a></p><h4 id="权限管理"><a href="#权限管理" class="headerlink" title="权限管理"></a>权限管理</h4><h5 id="root权限管理"><a href="#root权限管理" class="headerlink" title="root权限管理"></a>root权限管理</h5><p>遵循最小化权限配置原则。尽量不给root权限。如果是普通的刀片节点，可以给root权限。<br>GPU节点，尽量不给，因为环境的恢复过程非常复杂，少则半天到一天，多则2-3天。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;用户与权限管理&quot;&gt;&lt;a href=&quot;#用户与权限管理&quot; class=&quot;headerlink&quot; title=&quot;用户与权限管理&quot;&gt;&lt;/a&gt;用户与权限管理&lt;/h3&gt;&lt;h4 id=&quot;添加用户&quot;&gt;&lt;a href=&quot;#添加用户&quot; class=&quot;headerlink&quot; title=&quot;添加用户&quot;&gt;&lt;/a&gt;添加用户&lt;/h4&gt;&lt;h5 id=&quot;概览&quot;&gt;&lt;a href=&quot;#概览&quot; class=&quot;headerlink&quot; title=&quot;概览&quot;&gt;&lt;/a&gt;概览&lt;/h5&gt;&lt;p&gt;集群默认安装了clusconf软件，厂家的马少杰经理之前写的，主要用于集群的用户添加、开关机管理、批量执行命令、批量同步命令等。还是挺不错的，但是速度比较慢。特别是同步文件的时候，如果有几个节点存在问题，可能就会卡在那里半天，而且同步过程是顺序执行的，导致速度很慢。说白了，实际上是由于在进行远程执行命令的时候，没有考虑到失败的情况，也就是说没有添加失败时延，导致操作失败的时延差不多为30s，所以如果一批节点中，有几个节点拓机或者负载太高，会导致整个操作流程延长，所以，文件同步这一点来说，还是不太灵活。而pssh这个软件就比较好，如果连接失败，马上就返回，并且所有的节点的同步操作时并行执行的，所以速度非常快。因此我们结合了clusconf的方便和pssh的速度，来进行用户的添加。&lt;br&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结2-服务器维护</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%932-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%B4%E6%8A%A4.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结2-服务器维护.html</id>
    <published>2017-12-12T12:20:22.000Z</published>
    <updated>2018-02-24T08:38:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="服务器硬件维护"><a href="#服务器硬件维护" class="headerlink" title="服务器硬件维护"></a>服务器硬件维护</h3><p>集群配置的服务器主要有三种，刀片服务器、存储服务器、GPU服务器，其中刀片服务器最多，存储服务器硬盘最多，GPU服务器配置的GPGPU卡最多。</p><p>集群硬件主要包括了上述的三种服务器、H3C万兆以太网交换机、Mellanox InfniBand交换机、安全网关、PDU、PDM等设备。</p><p>而其中最容易出现故障的就是硬盘和刀片服务器了。存储服务器硬盘故障最多，其次是计算刀片服务器容易出现主板烧坏的情况。下面将简要介绍这两种故障的发现和处理。</p><p><span id="disk"></span></p><a id="more"></a><h4 id="硬盘故障发现"><a href="#硬盘故障发现" class="headerlink" title="硬盘故障发现"></a>硬盘故障发现</h4><p>最常见的硬盘故障是存储服务器的故障，存储服务器型号是Sugon I640-G15，该存储服务器的存储配置信息为，有两块RAID卡，机箱前置面板和后置面板都有硬盘插槽，其中前面可以放24块SAS硬盘，后面可以放12块SAS硬盘，前置硬盘插槽由RAID0卡管理，后置硬盘插槽由RAID1卡管理，一定要记住顺序了，其实顺序非常好记忆的。</p><p>每周需要有时间进行机房巡视，查看存储服务器硬盘的健康状况，这是一种最直接的发现问题的方式。</p><p>另外一种发现故障的方式是采用软件的方式，而且软件方式更加方便、直观，最主要就是能够在磁盘亮红灯之前就发现故障盘，可以及早发现问题，因此推荐采用这种方式来进行故障盘的发现。</p><p>集群使用的RAID卡品牌好像是Mega，厂家提供了两个工具来进行RAID信息配置和管理，MegaCLI和Mega RAID Storage Manager两个软件，顾名思义，MegaCLI是命令行工具，Mega RAID Storage Manager是图形化管理工具，推荐使用后者，因为配置方便、更加直观、操作简单。但是需要使用VNC连接到集群入口节点node307或者node308，然后通过Mega RAID Storage Manager连接到具体的存储服务器查看RAID阵列信息和磁盘信息，具体使用方法确实挺简单的，百度上面很多文章，这里就不再重复描述了。MegaCLI的优势就是，可以通过Shell脚本来提取磁盘信息，发现磁盘故障，因此可以使用MegaCLI编写磁盘健康监控脚本，由于时间有限，脚本没编写，不过很简单的。上面两种软件，已经被安装到了所有存储服务器，直接使用即可，如果存在问题，可以重新安装。</p><h4 id="硬盘故障处理"><a href="#硬盘故障处理" class="headerlink" title="硬盘故障处理"></a>硬盘故障处理</h4><p>发现硬盘故障后，首先使用手机拍下存储服务器上面的序列号，直接拨打全国客服热线400-810-0466进行报修，我们是非专项客户，存储服务器产品报修，然后报序列号，他们会查产品是否在保修期内，如果在保，他们会发一块硬盘过来，大约3-5天过来更换。</p><p>如果同一个服务器的同一个阵列，如前置阵列或者后置阵列上面出现两块硬盘闪红灯的情况，那么，不能等售后发货了，如果下一块磁盘出现故障，整个集群的数据都有可能丢失，这个时候的处理方式是，从最后的2台没有闲置的存储服务器上面找到一块硬盘，并且从RAID卡管理界面将该硬盘的RAID信息删除（具体做法后面可能会提到，也可以参考网上的MegaRAID的配置教程），然后拔掉这块硬盘，把硬盘从硬盘托上拆下来，换到坏掉的硬盘的所在的硬盘托架上面，并将好的硬盘插入到故障存储服务器上面，刚刚插上服务器时，硬盘指示灯还是会一直亮红色，大概5-10秒过后，RAID卡检测到新的硬盘插入后，如果硬盘上面没有之前的RAID信息，那么RAID卡会进行回写工作。</p><p>这里稍微讲解一下RAID卡的工作原理，一般的RAID阵列都会配置1-2块热备盘，热备盘一般会选择ID号靠近最后的1-2块作为热备盘，具体情况可以通过RAID卡配置界面查看到。当阵列中的某一块磁盘（非热备盘）出现故障时，RAID卡根据RIAD级别以及阵列其他磁盘的信息，计算出坏掉的磁盘信息，并将信息写入到热备盘中，即RAID卡将坏掉的磁盘信息还原并写入到热备盘，这个过程可能会比较慢，通常由数据大小和磁盘大小来决定，一般也需要2-3个小时，热备盘写入完成后，阵列又可以正常工作了，其实在写热备盘的过程中，阵列还是可以工作，只是速度慢一些，因为缺少了一块磁盘信息，需要通过计算才能得到，因此在写热备盘的过程中，不能出现频繁I/O请求，否则可能导致RAID阵列崩溃，阵列数据完全丢失的情况，这一点可以通过关闭集群入口节点的登录功能或者限制用户计算来达到。</p><p>需要注意的是，如果磁盘出现了故障，最好等到热备盘写完成之后，再插入新的硬盘，新的硬盘插上后，就开始回写过程，回写就是把热备盘的信息复制到新换上的硬盘当中，相当于原来的磁盘信息复制到新的磁盘上，当回写完成后，磁盘阵列又恢复了原来的状态，即原来的数据盘还是数据盘，热备盘还是热备盘，热备盘就不提供数据的读写服务了，只有当其他的磁盘坏掉是，热备盘才会开始工作，用于存储RAID计算出来的坏掉的磁盘的数据。</p><p>这其中存在一个情况就是，如果新插入的磁盘上面含有其他RAID卡的信息，很可能就是我们集群的情况，因为最后两台存储服务器node325和node326节点上面本身就有RAID信息，所以将这两块RAID卡上面的硬盘拔下，插入到其他存储服务器时，其他服务器RAID卡会认为他们是外部存储设备，无法正常使用。因此插上带有RAID信息的磁盘时，红灯会一直亮起，无法进行回写工作。那么我们可以通过在好硬盘所在的存储服务器上面，打开RAID卡配置界面，将需要取出来的硬盘从原有的RAID阵列中删除，这样磁盘上面就不包含RAID信息了，插入到存在坏盘的存储服务器上面，就可以马上自动开始回写工作了。</p><h4 id="计算刀片故障发现与处理"><a href="#计算刀片故障发现与处理" class="headerlink" title="计算刀片故障发现与处理"></a>计算刀片故障发现与处理</h4><p>首先简单介绍一下刀片服务器的构造。实验室的刀片类型是CB60-G15类型的，刀片服务器不能够独立存在，很多刀片同时排列在一起，统一有一个刀片机箱blade来进行管理。刀片机箱型号为TC4600，每个刀箱中可以放置14块CB60-G15刀片。</p><p>刀箱一般包含有4个冗余电源模块、散热模块、交换模块、直通模块、控制模块等，电源模块每个最大功率是2000w，支持热插拔，散热模块提供刀箱内部热量的排出，交换模块负责刀箱内部多个刀片的网络互通以及整个刀箱与外部的网络连通，直通模块提供了每个刀片直接与外部网络连通的功能，控制模块提供了刀箱的页面化配置管理、刀片物理状态的获取等、电源、散热模块的功率、转速配置等。</p><p>本集群的刀片机箱值安装了交换模块，没有安装直通模块，也就是只有一个以太网卡可以使用。</p><p>计算刀片的故障发现非常容易，但是需要一定的耐心。计算刀片出现故障的前兆是，刀片经常出现死机的情况，有时候负载不是很高就开卡住或者反映特别慢，出现这种现象时，可以首先重新启动计算刀片的操作系统，然后再观察一段时间，如果情况依旧，很有可能是主板出现故障，老化或者有电容坏掉了。这个时候，最后进一步做判断，将计算刀片关机，并把另外一块好的计算刀片的硬盘拔下来，插入到怀疑有故障的刀片上面，启动，如果发现故障依旧，那么基本可以断定，该计算刀片的主板出问题了。</p><p>计算刀片出现问题的时候，首先在前置面板几下刀片上面的一个数字，这个数字是刀片插槽的编号，然后到刀片所在刀箱的机柜后面，将对应编号的InfiniBand网线拔出来，直接拉那个蓝色的塑料圈圈就可以拔出来。然后返回到刀片正面，有个蓝色的小铁杆，直接往外拨，刀片就出来了一些，然后就可以把刀片整个拔出来了。但是这里不需要把刀片全部拔出来，只需要拔出一些，看到刀片前面的一个序列号标签，拿出手机拍照，记下序列号，然后拨打售后全国客服热线400-810-0466进行报修。大概3-5天之后，会有售后过来进行维修，一般而言就是更换刀片的主板，CPU、内存什么的的还是不会更换的。由于集群保修期大概到2017年12月份，因此需要早点发现故障刀片，尽快更换一批有故障的刀片服务器。</p><h3 id="计算节点OS维护"><a href="#计算节点OS维护" class="headerlink" title="计算节点OS维护"></a>计算节点OS维护</h3><p>集群所有节点，管理节点、存储节点、计算节点、GPU节点上面默认安装的操作系统是Redhat Enterprise Linux6.2，RHEL6.2 for short，内核版本2.6.32，有非常少部分节点由于用户的需求安装了CentOS7.0或者CentOS7.3版本的操作系统。</p><p>由于集群文件系统Lustre是与操作系统相关联的，导致升级计算节点操作系统是一件非常苦难的事情。尽管作为管理员，已经多次提过要升级全部节点的操作系统版本，以满足用户实验需要。但是由于此事真的是难上加上，包括需要一笔费用以及其中牵扯到的利益纠纷问题，因此一直没有升级。</p><p>由于很多同学的实验可能需要Linux3.0以上版本的内核，甚至有些同学可能需要Linux4.0以上版本的内核，因此可以考虑给这些用户root权限，让用户自己升级内核，这样用户可以在升级后的内核里面进行自己的科研实验，但是升级完内核之后，用户将无法使用Lustre文件系统。可以考虑挂载NFS文件系统来进行文件的读取，NFS文件系统也是每台机器都进行了挂载，用户可以通过集群入口服务器上传文件到NFS文件系统，然后升级后的内核中可以读取到相应的文件。</p><p>当集群的操作系统被破坏后，其他用户使用时存在问题，因此需要进行还原工作，鉴于集群经常存在这样的用户，他们会需要root权限进行一些操作，之后操作系统做了很多修改，再次分配给其他用户使用时会存在很多问题，因此需要进行系统重装。直接安装系统不仅要安装软件还要编译Lustre内核，非常复杂，一个比较好的办法是直接拷贝其他磁盘的系统，修改几个简单的配置文件，系统就搞定了，因此下面介绍两种方法进行系统快速还原。</p><h4 id="使用再生龙进行系统备份和还原工作"><a href="#使用再生龙进行系统备份和还原工作" class="headerlink" title="使用再生龙进行系统备份和还原工作"></a>使用再生龙进行系统备份和还原工作</h4><p>再生龙工具非常的强大，如果集群需要恢复的系统特别多，那么可以采用这种方式，下面附上再生龙使用教程，百度上面也可以搜索到，参考文档进行使用。</p><p><a href="http://clonezilla.nchc.org.tw/news/" target="_blank" rel="noopener">再生龙官网</a><br><a href="http://7xsnoh.com1.z0.glb.clouddn.com//clustermgmt/pdfClonezilla%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E.pdf" target="_blank" rel="noopener">再生龙详细使用文档</a></p><p>我们需要将再生龙服务器版本刻录到U盘或者光盘上面，然后启动到再生龙进行操作。</p><p>系统包括克隆和还原两个步骤，首先需要将一个计算节点的系统使用再生龙克隆到NFS文件系统或者另外一个大的磁盘上面，然后切换到再生龙的还原模式，启动需要还原的计算节点，从网络启动，然后进行还原工作。</p><p>需要说明的是设置网卡的时候，只需要设置一个网卡eth0即可，因为eth1是直通模块。eth0的IP地址建议设置为11.11.4.100，子网掩码设置为255.255.0.0，NFS文件系统的IP地址是11.11.2.18，路径是/public/vpublic02，这个目录里面已经备份过几个计算节点的磁盘了，可以暂时使用，或者自己选择一个重新备份。</p><h4 id="使用硬盘对拷进行重装系统"><a href="#使用硬盘对拷进行重装系统" class="headerlink" title="使用硬盘对拷进行重装系统"></a>使用硬盘对拷进行重装系统</h4><p><strong>数据无价，拷贝之前一定要确定磁盘的用户数据备份了。</strong></p><p>首先将需要重装的节点上面的硬盘卸载下来，插入到一个负载不是很大的节点的备用硬盘仓中，然后使用fdisk -l来查看新插入的硬盘的编号，由于本集群的计算节点只有两个硬盘仓，所以后插入的硬盘的编号就是/dev/sdb了。</p><h5 id="卸载Lustre和NFS挂载点"><a href="#卸载Lustre和NFS挂载点" class="headerlink" title="卸载Lustre和NFS挂载点"></a>卸载Lustre和NFS挂载点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in &#123;&apos;/public&apos;,&apos;/home&apos;,&apos;/vpublic01&apos;,&apos;/vpublic02&apos;&#125;;</span><br><span class="line">do</span><br><span class="line">  umount $i;</span><br><span class="line">done;</span><br></pre></td></tr></table></figure><h5 id="执行dd命令"><a href="#执行dd命令" class="headerlink" title="执行dd命令"></a>执行dd命令</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if=/dev/sda of=/dev/sdb bs=32256 conv=notrunc,noerror &amp;</span><br></pre></td></tr></table></figure><p>后面的&amp;表示以后台形式运行，运行之后会返回一个编号，就是这个进程的PID。</p><p>执行如下命令来查看拷贝的进度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while kill -USR1 2643;</span><br><span class="line">do </span><br><span class="line">  echo;</span><br><span class="line">  sleep 10;</span><br><span class="line">done</span><br></pre></td></tr></table></figure></p><p>把2643替换为你自己的进程PID就可以了。<br>这里的300GB的硬盘，大概拷贝需要40分钟左右，因为是Serial SCSI硬盘，速度稍微快一点。</p><h5 id="修改IP地址"><a href="#修改IP地址" class="headerlink" title="修改IP地址"></a>修改IP地址</h5><p>修改Ethernet和IB网IP地址，通过修改/etc/sysconfig/network-scripts/中的ifcfg-ib0和ifcfg-br0两个文件中的IP地址即可。</p><h5 id="删除rules文件"><a href="#删除rules文件" class="headerlink" title="删除rules文件"></a>删除rules文件</h5><p>查看/etc/udev/rules.d/70-persistent-net.rules文件，修改对应参数，我一般比较懒，直接删除，重启会重建。</p><h5 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h5><p>修改/etc/sysconfig/network文件，在RHEL6中该文件存储的是系统主机名。RHEL7系统中，请使用<code>hostnamectl sethostname hostname</code>命令设置主机名，请将最后的hostname替换为你想设置的主机名。</p><h5 id="磁盘检查"><a href="#磁盘检查" class="headerlink" title="磁盘检查"></a>磁盘检查</h5><p>很多时候，母机器上面有一些inode信息不一致或者坏掉的磁盘块，因此需要进行一下简单的修复工作。请依次执行一下命令，检查和修复磁盘。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fsck -y /dev/sdb1</span><br><span class="line">fsck -y /dev/sdb2</span><br></pre></td></tr></table></figure></p><p>其中dev/sdb1和/dev/sdb2为需要还原系统的磁盘的分区编号，根据具体情况进行替换即可。</p><h5 id="放回磁盘，启动系统"><a href="#放回磁盘，启动系统" class="headerlink" title="放回磁盘，启动系统"></a>放回磁盘，启动系统</h5><p>将硬盘取回，放到原节点，启动节点，则会进入一个初始化的过程，有时会由于存在磁盘中的块与索引信息不一致的情况，尽管之前做过检查，但是在插入到新机器上后，还会出现，它会提示你输入root密码进行修复或者按Ctrl-D取消，这时需要输入root密码，因为是拷贝的其他节点的数据，因此root密码也是被拷贝节点的密码。如果root密码忘记，可以将磁盘重新插入到一台机器，将那台机器的/etc/shadow文件拷贝到磁盘中，这样就相当于重置了root密码。输入root密码之后，有可能提示你磁盘有问题。这个时候需要进行修复。此时一般会提示你哪个分区有问题，例如/dev/sda1有问题或者是/dev/sda2有问题，那么需要我们进行修复，输入一下命令，建议将所有分区一次性修复。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#修复第一个分区，这个分区一般是引导分区</span><br><span class="line">fsck -y /dev/sda1</span><br><span class="line"></span><br><span class="line">#修复第二个分区，这个分区一般是主文件分区</span><br><span class="line">fsck -y /dev/sda2</span><br></pre></td></tr></table></figure></p><p>有时会提示你是/dev/sda2有问题，但是建议也修复一下/dev/sda1，这样重启之后不会因为文件系统有小故障导致重新修复再次重启。</p><p>输入reboot重启即可。</p><h5 id="被拷贝机还原"><a href="#被拷贝机还原" class="headerlink" title="被拷贝机还原"></a>被拷贝机还原</h5><p>将被拷贝的机器的文件系统挂载还原。或者直接重启也可以。</p><p><strong>再次强调，数据无价，拷贝之前一定要确定磁盘的用户数据已经备份了。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;服务器硬件维护&quot;&gt;&lt;a href=&quot;#服务器硬件维护&quot; class=&quot;headerlink&quot; title=&quot;服务器硬件维护&quot;&gt;&lt;/a&gt;服务器硬件维护&lt;/h3&gt;&lt;p&gt;集群配置的服务器主要有三种，刀片服务器、存储服务器、GPU服务器，其中刀片服务器最多，存储服务器硬盘最多，GPU服务器配置的GPGPU卡最多。&lt;/p&gt;
&lt;p&gt;集群硬件主要包括了上述的三种服务器、H3C万兆以太网交换机、Mellanox InfniBand交换机、安全网关、PDU、PDM等设备。&lt;/p&gt;
&lt;p&gt;而其中最容易出现故障的就是硬盘和刀片服务器了。存储服务器硬盘故障最多，其次是计算刀片服务器容易出现主板烧坏的情况。下面将简要介绍这两种故障的发现和处理。&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;disk&quot;&gt;&lt;/span&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>HPC集群运维总结1-配置与职责</title>
    <link href="http://blog.zhangchi.xyz/HPC%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%80%BB%E7%BB%931-%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%81%8C%E8%B4%A3.html"/>
    <id>http://blog.zhangchi.xyz/HPC集群运维总结1-配置与职责.html</id>
    <published>2017-12-12T12:20:01.000Z</published>
    <updated>2018-02-24T07:26:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><h3 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h3><p>2016年5月至2018年3月之间，我与另外一位同学担任了某集群的运行维护工作。期间主要负责了机房维护、空调维护、配电房维护、安全防护、分布式系统管理、高性能软件管理、用户管理、计算资源管理、服务器系统管理、服务器硬件维护等工作，内容繁多，工作复杂，但是总体来说，在过去一年的集群管理工作中还是有不少收获的。</p><p>下面将自己在管理过程中遇到的问题和解决方法分享给大家，尽管里面涉及到的技术和理论还不够深入，甚至不足以解决一些很棘手的问题，但是可以方便后来的管理人员快速入手，同时也是一个交流的机会，非常欢迎大家批评指正和指导。<br><a id="more"></a></p><h3 id="集群配置概况"><a href="#集群配置概况" class="headerlink" title="集群配置概况"></a>集群配置概况</h3><h4 id="服务器配置信息"><a href="#服务器配置信息" class="headerlink" title="服务器配置信息"></a>服务器配置信息</h4><p>集群包含4种类型的服务器</p><ul><li>TC4600刀箱中的CB60-G15计算刀片</li><li>I640-G10普通2U机架式服务器</li><li>I640-G15存储服务器</li><li>W580I-G10GPU节点</li></ul><p>各机型配置如下：</p><ul><li><p>TC4600刀箱</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">标准19英寸5U机架式刀片机箱、可以支持14个计算刀片；</span><br><span class="line">1*管理模块，集成远程KVM和远程虚拟媒体；</span><br><span class="line">1*万兆上联模块，对外2个sfp+万兆，对内14个千兆端口；</span><br><span class="line">4*冗余热插拔散热模块；</span><br><span class="line">4*2000W电源（3+1冗余热拔插）；</span><br></pre></td></tr></table></figure></li><li><p>刀片CB60-G15</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2*Intel Xeon E5-2670 八核处器 (2.6GHz)；</span><br><span class="line">8*8GB DDR3 1600MHz；</span><br><span class="line">1*300G 2.5寸10000转SAS硬盘；</span><br><span class="line">1*56Gb Infiniband 接口；</span><br><span class="line">2*1000M以太网接口；</span><br></pre></td></tr></table></figure></li><li><p>管理I620-G10</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2U机架式</span><br><span class="line">2*Intel Xeon E5-2670 八核处器 (2.6GHz)；</span><br><span class="line">8*16GB DDR3 1600MHz；</span><br><span class="line">1*300G 2.5寸10000转SAS硬盘；</span><br><span class="line">2*1000M以太网接口；     </span><br><span class="line">1+1冗余电源；</span><br></pre></td></tr></table></figure></li><li><p>存储I640-G15</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">4U机架式</span><br><span class="line">1*Intel Xeon E5-2603 CPU 四核处器 (1.8GHz)；</span><br><span class="line">4*4GB DDR3 ECC；</span><br><span class="line">1*300G 2.5寸10000转SAS硬盘；</span><br><span class="line">36* 900G SAS硬盘；</span><br><span class="line">1*56Gb Infiniband 接口；</span><br><span class="line">2*1000M以太网接口；</span><br><span class="line">冗余电源；</span><br></pre></td></tr></table></figure></li><li><p>存储I620-G10</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2U机架式</span><br><span class="line">2*Intel Xeon E5-2603 CPU 四核处器 (1.8GHz)；</span><br><span class="line">8*4GB DDR3 ECC；</span><br><span class="line">1*300G 2.5寸10000转SAS硬盘；</span><br><span class="line">1*56Gb Infiniband 接口；</span><br><span class="line">2*1000M以太网接口；     </span><br><span class="line">1+1冗余电源；</span><br></pre></td></tr></table></figure></li></ul><p>其中node1-node306是CB60-G15计算刀片，每14个刀片位于同一个刀箱，最后一个刀箱只有10个刀片。<br>node307-node308是I640-G10普通2U机架服务器，主要用来登录集群和集群的管理工作，其中node307主要用于用户登录和集群管理工作，node308部署了zabbix和集群温度监控脚本，以及Crane主页等内容。</p><p>node309-node310是Lustre的MDS服务器，即分布式文件系统的索引服务器。<br>node311-node323是Lustre的对象存储服务器。<br>node324目前是一台NFS服务器，主要用于某些不能使用Lustre的节点进行文件传输。<br>node325-node326是两台空闲的存储服务器，可以用来做备用机，一般情况不使用，主要用于应急，当然后期也可以考虑将该节点加入到Lustre的对象存储服务器中。</p><p>node327-node336是10台GPU节点<br>node337-node348共12台刀片，在北面从西至东的第三个机柜的最下面一个刀箱，也是计算刀片。 </p><h4 id="网络配置信息"><a href="#网络配置信息" class="headerlink" title="网络配置信息"></a>网络配置信息</h4><p>所有计算刀片、存储服务器、存储IO节点通过Mellaonx FDR324口（18个18+18页版）Infiniband交换机连接，16U，模块化交换机。</p><p>集群入口节点没有Infniband网络，只有以太网络，即千兆以太网。</p><p>集群的所有刀箱、机架服务器都通过一个H3C的48口千兆以太网交换机连接在一起了。</p><p>刀箱后面一个H3C的以太网交换模块，这个模块提供了14个内置的交换端口和4个对外的交换端口以及2个万兆光口。</p><p>所有刀箱的交换模块通过一个H3C 24口 SFP+万兆交换机连接，也就是说，跨刀箱的节点通信通过万兆网，刀箱内部通信是通过交换模块内置的网络进行的。</p><p>另外，集群的所有服务器都有一个管理口，即远程控制地址。该套网络也是在H3C 48口以太网交换机、万兆交换机、以及刀箱上面的交换模块结合来构建的，具体方案现在还没有特别确定。具体布局情况最好请教网络方面的同学或工程师。</p><h5 id="通信流程"><a href="#通信流程" class="headerlink" title="通信流程"></a>通信流程</h5><h6 id="同刀箱刀片通信"><a href="#同刀箱刀片通信" class="headerlink" title="同刀箱刀片通信"></a>同刀箱刀片通信</h6><p>同一个刀片机箱的所有刀片节点之间的通信是通过刀箱后面的以太网交换模块进行的，相当与一个交换机。</p><h6 id="跨刀箱刀片通信"><a href="#跨刀箱刀片通信" class="headerlink" title="跨刀箱刀片通信"></a>跨刀箱刀片通信</h6><p>刀片A将数据转发到刀箱的交换模块，交换模块将数据通过万兆光模块转发到万兆交换机，万兆交换机转发到刀片B所在刀箱的交换模块，交换模块将数据转发给刀片B.</p><h6 id="管理节点与刀片通信"><a href="#管理节点与刀片通信" class="headerlink" title="管理节点与刀片通信"></a>管理节点与刀片通信</h6><h6 id="管理节点与存储通信"><a href="#管理节点与存储通信" class="headerlink" title="管理节点与存储通信"></a>管理节点与存储通信</h6><h6 id="刀片节点与存储通信"><a href="#刀片节点与存储通信" class="headerlink" title="刀片节点与存储通信"></a>刀片节点与存储通信</h6><h4 id="文件系统配置信息"><a href="#文件系统配置信息" class="headerlink" title="文件系统配置信息"></a>文件系统配置信息</h4><p>集群采用并行分布式文件系统Lustre来提供/public和/home目录，即所有节点都挂载/public/home作为节点的/home目录，提供统一的用户信息。<br>Lustre文件系统构成如下，node309和node310为MDS服务器，node311-node318为/home对应的OST，node319-node323为/public目录对应的OST，由于之前Lustre文件系统出现过故障，后来node311-node318就不再提供存储服务了，node319-node323提供/public目录，并且提供/public/home为/home目录。<br>其次，在不能使用Lustre的节点，可以挂载NFS文件系统进行文件的传递。</p><h3 id="日常职责"><a href="#日常职责" class="headerlink" title="日常职责"></a>日常职责</h3><h4 id="机房巡检"><a href="#机房巡检" class="headerlink" title="机房巡检"></a>机房巡检</h4><p>该工作原则上是每天至少1次但是考虑到时间有限，可以宽限为每周2-3次，周一、周五都必须检查一次，主要检查空调运行情况、机房温度、机房湿度、是否存在火宅隐患、检查火宅报警器是否正常开机运行等。<br>总结如下：</p><ul><li>主机房空调和配电房空调检查。</li><li>市电和UPS配电柜状态检查。</li><li>UPS系统状态检查。</li><li>火灾报警器工作状态检查，包括主机房和配电房。</li><li>硬盘和服务器指示灯检查。</li><li>未罗列事项。</li></ul><h4 id="集群硬件维护"><a href="#集群硬件维护" class="headerlink" title="集群硬件维护"></a>集群硬件维护</h4><p>该项职责主要包括集群存储服务器硬盘状态查看、计算刀片故障处理、硬盘更换、内存条更换、网络故障处理等。<br>集群软件系统维护，主要包括集群分布式文件系统故障处理、集群计算刀片操作系统管理、集群并行计算软件环境维护等。<br>计算资源分配与管理，主要将空闲的计算资源分配给有需要的用户，为其添加集群账号，分配节点。用户计算完后，主动回收节点，并清理不用的用户资料。<br>总结而言主要包括如下内容：</p><ul><li>计算节点主板更换。</li><li>计算节点内存更换。</li><li>刀片机箱电源模块更换。</li><li>存储服务器硬盘更换。</li><li>Infiniband网络故障解决。</li><li>其他未罗列事项。</li></ul><h4 id="服务器软件维护"><a href="#服务器软件维护" class="headerlink" title="服务器软件维护"></a>服务器软件维护</h4><p>该项职责主要包括如下内容：</p><ul><li>计算节点的操作系统维护，升级内核、还原系统等。</li><li>计算节点内置软件环境的配置和管理。</li><li>计算节点的环境变量配置。</li><li>并行计算环境在各节点的Client端或者代理端配置。</li><li>PBS队列管理系统和Zabbix监控系统的客户端daemon配置与维护。</li><li>其他未罗列的计算节点软件。</li></ul><h4 id="HPC软件环境维护"><a href="#HPC软件环境维护" class="headerlink" title="HPC软件环境维护"></a>HPC软件环境维护</h4><p>HPC软件环境主要包括如下内容：</p><ul><li>Intel编译器、gcc编译器、gcc-c++编译器、gfortran编译器等编译支持软件。</li><li>各类库MKL、acml、fftw2-float、fftw2-double、scalapack等。</li><li>各类MPI版本，openmpi-gpu、openmpi-intel、mvapich2-gnu、mvapich2-intel、mpich3-gnu等。</li><li>各类具体的HPC软件，Matlab、Gaussian、Octopus、Lammps、CellSys、Ansys fluent、BLAST、Ansys CFX、WRF、WAVEWATCH 3、OpenFOAM等具体领域的软件。</li><li>监控平台如Zabbix。</li><li>分布式文件系统Lustre。</li><li>其他未罗列的HPC软件环境。</li></ul><h4 id="集群资源分配与管理"><a href="#集群资源分配与管理" class="headerlink" title="集群资源分配与管理"></a>集群资源分配与管理</h4><ul><li>PBS队列配置与管理</li><li>PBS队列使用权限分配</li><li>HPC集群分区和资源利用最大化</li><li>GPGPU资源的分配和管理</li><li>资源使用的监控与统计</li><li>资源的回收再分配</li></ul><h4 id="用户管理与技术支持"><a href="#用户管理与技术支持" class="headerlink" title="用户管理与技术支持"></a>用户管理与技术支持</h4><ul><li>为用户添加账号和重置密码</li><li>添加IP地址许可和网络转发</li><li>安装公用开源软件</li><li>协助用户解决遇到的问题</li></ul><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>概括主要有如下几件事要做：</p><ul><li>主机房两个空调是否正常运行，主要看报警数据还有温度信息，主机房正常温度在22-24摄氏度，湿度40%-60%范围内，空调控制面板报警选项卡中没有报警信息，并且查看最近一周温度变化情况，是否存在温度异常的情况。</li><li>主机房火灾报警器是否正常运行，主要看是否开机，面板提示是否正常。</li><li>主机房所有机柜绕一圈，检查是否有刀片服务器电源模块亮黄灯，黄灯表示异常，绿灯表示正常运行。同时，检查存储服务器后置硬盘阵列是否存在亮红灯的情况，存在亮红灯的硬盘，请及时拔下硬盘，有手机或者相机拍下服务器上序列号，并进行报修。最后还要检查刀片机箱的交换模块是否正常运行，即网络接口是否有灯在闪。</li><li>配电房UPS电池组是否存在异常，主要看是否有异味，UPS主机柜和配电柜是否有异常显示信息，异常时会有红灯亮，注意不是面板显示的红色信息。</li><li>配电房空调温度是否正常，东北角的柜式空调，正常温度设定在18-20摄氏度，存在异常请及时报修。</li></ul><h3 id="机房巡检-1"><a href="#机房巡检-1" class="headerlink" title="机房巡检"></a>机房巡检</h3><h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p>机房环境的维护是一件非常重要的事情，良好的机房环境是集群正常运行的基础，机房环境出现问题，直接集群瘫痪，无法正常运行，更严重的，还有可能导致火灾，因此机房环境需要重点关注。</p><h4 id="温度"><a href="#温度" class="headerlink" title="温度"></a>温度</h4><p>机房环境主要指房间的温度、湿度，这两点特别重要，如果温度过高、过低或者温度不稳定，集群很容易出现问题，集群本身产热非常大，如果温度过高，很容易出现服务器报警、主板老化的情况。本集群之前空调出现问题，导致主板节点温度过高，服务器过早坏掉的情况。温度过高最容易导致主板电容爆掉的情况，因此需要引起注意，另外用户在进行计算过程中，要特别注意提醒用户不要提交过量的计算任务，避免导致主板温度过高提前老化的情况。任何时候，如果出现机房空调温度不正常，需要及时给实验室相关负责老师打电话并联系空调维保公司技术人员尽快进行空调维护，并关闭一部分机器，避免温度过高。</p><h4 id="湿度"><a href="#湿度" class="headerlink" title="湿度"></a>湿度</h4><p>机房的湿度控制，正常情况下，机房湿度都在标准允许的范围内，即不超过50%。除非是武汉地区长期下雨，可能会存在湿度超标的情况，一般湿度都会在可控的范围内。另外，之前机房顶部靠近北边的地方出现过漏水的情况，具体原因没有查清楚，因此进行机房巡视时，一定要注意看天花板，检查是否存在房顶漏水的情况，如果存在这样的情况，首先及时汇报情况给机房负责老师，并及时关闭集群，并切断电源（具体方法见后面集群断电方式），避免进水导致机器烧毁。</p><h4 id="火灾"><a href="#火灾" class="headerlink" title="火灾"></a>火灾</h4><p>机房火灾报警器检查，机房火灾一般会由于温度过高，机器老化导致，即很可能是因为电路老化导致火灾。因此每次去机房都要检查火灾报警器是否正常运行了。之前机房出现过火灾报警器误报的情况，即实际上不存在火灾，但是报警器一直响，出现过多次，后来发现原因是夏季时机房湿度比较大，报警器上面有水凝结，导致误报。之后稍微修理了一下，暂时没有出现误报的情况。如果出现了报警器报警的情况，检查温度是否过高，是否存在烟雾，是不是误报，然后及时汇报情况给负责老师。如果存在温度过高或者存在烟雾或有异味，赶紧关闭集群，断电方法请参考集群关机或启动流程章节。</p><h4 id="供电"><a href="#供电" class="headerlink" title="供电"></a>供电</h4><p>集群供电是从机械大楼迁过来的，E5楼停电对集群无任何影响。<br>UPS仅仅提供IT设备的供电，断电后，应该在10分钟内关闭整个集群，保证服务器设备的安全和机房的安全。<br>因此巡检需要到配电房检查配电柜、UPS信息等，保证集群电力供应正常。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;h3 id=&quot;序言&quot;&gt;&lt;a href=&quot;#序言&quot; class=&quot;headerlink&quot; title=&quot;序言&quot;&gt;&lt;/a&gt;序言&lt;/h3&gt;&lt;p&gt;2016年5月至2018年3月之间，我与另外一位同学担任了某集群的运行维护工作。期间主要负责了机房维护、空调维护、配电房维护、安全防护、分布式系统管理、高性能软件管理、用户管理、计算资源管理、服务器系统管理、服务器硬件维护等工作，内容繁多，工作复杂，但是总体来说，在过去一年的集群管理工作中还是有不少收获的。&lt;/p&gt;
&lt;p&gt;下面将自己在管理过程中遇到的问题和解决方法分享给大家，尽管里面涉及到的技术和理论还不够深入，甚至不足以解决一些很棘手的问题，但是可以方便后来的管理人员快速入手，同时也是一个交流的机会，非常欢迎大家批评指正和指导。&lt;br&gt;
    
    </summary>
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/categories/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
    
      <category term="集群管理" scheme="http://blog.zhangchi.xyz/tags/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/"/>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="运维" scheme="http://blog.zhangchi.xyz/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="HPC" scheme="http://blog.zhangchi.xyz/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>Linux内核空间分布</title>
    <link href="http://blog.zhangchi.xyz/Linux%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4%E5%88%86%E5%B8%83.html"/>
    <id>http://blog.zhangchi.xyz/Linux内核空间分布.html</id>
    <published>2017-12-07T12:16:47.000Z</published>
    <updated>2017-12-07T12:18:47.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-27177626-id-3436619.html" target="_blank" rel="noopener">内核空间非连续内存区的分配</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-27177626-id-3436619.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;内核空间非连续内存区的分配&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://blog.zhangchi.xyz/tags/Linux/"/>
    
      <category term="Linux内存" scheme="http://blog.zhangchi.xyz/tags/Linux%E5%86%85%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>undefined reference to pthread_create</title>
    <link href="http://blog.zhangchi.xyz/undefined-reference-to-pthread-create.html"/>
    <id>http://blog.zhangchi.xyz/undefined-reference-to-pthread-create.html</id>
    <published>2017-11-26T23:38:38.000Z</published>
    <updated>2018-02-24T03:22:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用pthread库进行线程开发的时候，如果包含了pthread.h头文件后，在编译代码的时候还是出现了如下错误。可能是没有在编译时没有链接相关的函数库。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">undefined reference to `pthread_create&apos;</span><br><span class="line">collect2: ld returned 1 exit status</span><br><span class="line">make: *** [threadid] Error 1</span><br></pre></td></tr></table></figure></p><p>在编译时，加入-l pthread选项即可。<br>如果是用Makefile，也是一样的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CC = gcc</span><br><span class="line">CFLAGS = -I/home/cat/apue/apue.2e/include -Wall -g、</span><br><span class="line">threadid: threadid.o</span><br><span class="line">$(CC) $(CFLAGS) -o $@ $^ -lpthread</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在使用pthread库进行线程开发的时候，如果包含了pthread.h头文件后，在编译代码的时候还是出现了如下错误。可能是没有在编译时没有链接相关的函数库。&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;g
      
    
    </summary>
    
      <category term="Linux开发" scheme="http://blog.zhangchi.xyz/categories/Linux%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="pthread" scheme="http://blog.zhangchi.xyz/tags/pthread/"/>
    
  </entry>
  
</feed>
